# Table of Contents #
1. [A Relational Model of Data for Large Shared Data Banks (1970)](#a-relational-model-of-data-for-large-shared-data-banks-1970)
1. [Scheduling Algorithms for Multiprogramming in a Hard-Real-Time Environment (1973)](#scheduling-algorithms-for-multiprogramming-in-a-hard-real-time-environment-1973)
1. [The Unix Time-Sharing System (1974)](#the-unix-time-sharing-system-1974)
1. [Granularity of Locks and Degrees of Consistency in a Shared Database (1976)](#granularity-of-locks-and-degrees-of-consistency-in-a-shared-database-1976)
1. [On the Duality of Operating System Structures (1979)](#on-the-duality-of-operating-system-structures-1979)
1. [Experience with Processes and Monitors in Mesa (1980)](#experience-with-processes-and-monitors-in-mesa-1980)
1. [A History and Evaluation of System R (1981)](#a-history-and-evaluation-of-system-r-1981)
1. [A Fast File System for UNIX (1984)](#a-fast-file-system-for-unix-1984)
1. [End-to-End Arguments in System Design (1984)](#end-to-end-arguments-in-system-design-1984)
1. [Concurrency Control Performance Modeling: Alternatives and Implications (1987)](#concurrency-control-performance-modeling-alternatives-and-implications-1987)
1. [The Design of the POSTGRES Storage System (1987)](#the-design-of-the-postgres-storage-system-1987)
1. [The POSTGRES Next Generation Database Management System (1991)](#the-postgres-next-generation-database-management-system-1991)
1. [Microkernel Operating System Architecture and Mach (1992)](#microkernel-operating-system-architecture-and-mach-1992)
1. [The Linda alternative to message-passing systems (1994)](#the-linda-alternative-to-message-passing-systems-1994)
1. [Lottery and Stride Scheduling: Flexible Proportional-Share Resource Management (1994)](#lottery-and-stride-scheduling-flexible-proportional-share-resource-management-1994)
1. [A Critique of ANSI SQL Isolation Levels (1995)](#a-critique-of-ansi-sql-isolation-levels-1995)
1. [Exokernel: An Operating System Architecture for Application-Level Resource Management (1995)](#exokernel-an-operating-system-architecture-for-application-level-resource-management-1995)
1. [SPIN -- An Extensible Microkernel for Application-specific Operating System Services (1995)](#spin----an-extensible-microkernel-for-application-specific-operating-system-services-1995)
1. [The HP AutoRAID hierarchical storage system (1996)](#the-hp-autoraid-hierarchical-storage-system-1996)
1. [Disco: Running Commodity Operating Systems on Scalable Multiprocessors (1997)](#disco-running-commodity-operating-systems-on-scalable-multiprocessors-1997)
1. [T Spaces: The Next Wave (1999)](#t-spaces-the-next-wave-1999)
1. [Generalized Isolation Level Definitions (2000)](#generalized-isolation-level-definitions-2000)
1. [The Click Modular Router (2000)](#the-click-modular-router-2000)
1. [Chord: A Scalable Peer-to-peer Lookup Service for Internet Applications (2001)](#chord-a-scalable-peer-to-peer-lookup-service-for-internet-applications-2001)
1. [Inferring Internet Denial-of-Service Activity (2001)](#inferring-internet-denial-of-service-activity-2001)
1. [SEDA: An Architecture for Well-Conditioned, Scalable Internet Services (2001)](#seda-an-architecture-for-well-conditioned-scalable-internet-services-2001)
1. [The Google File System (2003)](#the-google-file-system-2003)
1. [Xen and the Art of Virtualization (2003)](#xen-and-the-art-of-virtualization-2003)
1. [MapReduce: Simplified Data Processing on Large Clusters (2004)](#mapreduce-simplified-data-processing-on-large-clusters-2004)
1. [Analysis and Evolution of Journaling File Systems (2005)](#analysis-and-evolution-of-journaling-file-systems-2005)
1. [Live Migration of Virtual Machines (2005)](#live-migration-of-virtual-machines-2005)
1. [The Chubby lock service for loosely-coupled distributed systems (2006)](#the-chubby-lock-service-for-loosely-coupled-distributed-systems-2006)
1. [Architecture of a Database System (2007)](#architecture-of-a-database-system-2007)
1. [Dryad: Distributed Data-Parallel Programs from Sequential Building Blocks (2007)](#dryad-distributed-data-parallel-programs-from-sequential-building-blocks-2007)
1. [Dynamo: Amazon's Highly Available Key-value Store (2007)](#dynamo-amazons-highly-available-key-value-store-2007)
1. [Bigtable: A Distributed Storage System for Structured Data (2008)](#bigtable-a-distributed-storage-system-for-structured-data-2008)
1. [DryadLINQ: A System for General-Purpose Distributed Data-Parallel Computing Using a High-Level Language (2008)](#dryadlinq-a-system-for-general-purpose-distributed-data-parallel-computing-using-a-high-level-language-2008)
1. [CRDTs: Consistency without concurrency control (2009)](#crdts-consistency-without-concurrency-control-2009)
1. [The Five-Minute Rule 20 Years Later (2009)](#the-five-minute-rule-20-years-later-2009)
1. [SnowFlock: Rapid Virtual Machine Cloning for Cloud Computing (2009)](#snowflock-rapid-virtual-machine-cloning-for-cloud-computing-2009)
1. [BOOM Analytics: Exploring Data-Centric, Declarative Programming for the Cloud (2010)](#boom-analytics-exploring-data-centric-declarative-programming-for-the-cloud-2010)
1. [The Declarative Imperative: Experiences and Conjectures in Distributed Logic (2010)](#the-declarative-imperative-experiences-and-conjectures-in-distributed-logic-2010)
1. [Conflict-free Replicated Data Types (2011)](#conflict-free-replicated-data-types-2011)
1. [Consistency Analysis in Bloom: a CALM and Collected Approach (2011)](#consistency-analysis-in-bloom-a-calm-and-collected-approach-2011)
1. [Dedalus: Datalog in Time and Space (2011)](#dedalus-datalog-in-time-and-space-2011)
1. [Dominant Resource Fairness: Fair Allocation of Multiple Resource Types (2011)](#dominant-resource-fairness-fair-allocation-of-multiple-resource-types-2011)
1. [Mesos: A Platform for Fine-Grained Resource Sharing in the Data Center (2011)](#mesos-a-platform-for-fine-grained-resource-sharing-in-the-data-center-2011)
1. [Don't Settle for Eventual: Scalable Causal Consistency for Wide-Area Storage with COPS (2011)](#dont-settle-for-eventual-scalable-causal-consistency-for-wide-area-storage-with-cops-2011)
1. [Logic and Lattices for Distributed Programming (2012)](#logic-and-lattices-for-distributed-programming-2012)
1. [Making Geo-Replicated Systems Fast as Possible, Consistent when Necessary (2012)](#making-geo-replicated-systems-fast-as-possible-consistent-when-necessary-2012)
1. [Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing (2012)](#resilient-distributed-datasets-a-fault-tolerant-abstraction-for-in-memory-cluster-computing-2012)
1. [Apache Hadoop YARN: Yet Another Resource Negotiator (2013)](#apache-hadoop-yarn-yet-another-resource-negotiator-2013)
1. [Discretized Streams: Fault-Tolerant Streaming Computation at Scale (2013)](#discretized-streams-fault-tolerant-streaming-computation-at-scale-2013)
1. [From L3 to seL4 What Have We Learnt in 20 Years of L4 Microkernels? (2013)](#from-l3-to-sel4-what-have-we-learnt-in-20-years-of-l4-microkernels-2013)
1. [Innovative Instructions and Software Model for Isolated Execution (2013)](#innovative-instructions-and-software-model-for-isolated-execution-2013)
1. [MillWheel: Fault-Tolerant Stream Processing at Internet Scale (2013)](#millwheel-fault-tolerant-stream-processing-at-internet-scale-2013)
1. [Naiad: A Timely Dataflow System (2013)](#naiad-a-timely-dataflow-system-2013)
1. [Replicated Data Consistency Explained Through Baseball (2013)](#replicated-data-consistency-explained-through-baseball-2013)
1. [Automating the Choice of Consistency Levels in Replicated Systems (2014)](#automating-the-choice-of-consistency-levels-in-replicated-systems-2014)
1. [Coordination Avoidance in Database Systems (2014)](#coordination-avoidance-in-database-systems-2014)
1. [Highly Available Transactions: Virtues and Limitations (2014)](#highly-available-transactions-virtues-and-limitations-2014)
1. [In Search of an Understandable Consensus Algorithm (2014)](#in-search-of-an-understandable-consensus-algorithm-2014)
1. [Shielding Applications from an Untrusted Cloud with Haven (2014)](#shielding-applications-from-an-untrusted-cloud-with-haven-2014)
1. [Storm @ Twitter (2014)](#storm--twitter-2014)
1. [The Homeostasis Protocol: Avoiding Transaction Coordination Through Program Analysis (2015)](#the-homeostasis-protocol-avoiding-transaction-coordination-through-program-analysis-2015)
1. [Impala: A Modern, Open-Source SQL Engine for Hadoop (2015)](#impala-a-modern-open-source-sql-engine-for-hadoop-2015)
1. [Large-scale cluster management at Google with Borg (2015)](#large-scale-cluster-management-at-google-with-borg-2015)
1. [Putting Consistency Back into Eventual Consistency (2015)](#putting-consistency-back-into-eventual-consistency-2015)
1. [Spark SQL: Relational Data Processing in Spark (2015)](#spark-sql-relational-data-processing-in-spark-2015)
1. [Twitter Heron: Stream Processing at Scale (2015)](#twitter-heron-stream-processing-at-scale-2015)
1. [Borg, Omega, and Kubernetes (2016)](#borg-omega-and-kubernetes-2016)
1. ['Cause I'm Strong Enough: Reasoning about Consistency Choices in Distributed Systems (2016)](#cause-im-strong-enough-reasoning-about-consistency-choices-in-distributed-systems-2016)
1. [Decibel: The Relational Dataset Branching System (2016)](#decibel-the-relational-dataset-branching-system-2016)
1. [Disciplined Inconsistency with Consistency Types (2016)](#disciplined-inconsistency-with-consistency-types-2016)
1. [Goods: Organizing Google's Datasets (2016)](#goods-organizing-googles-datasets-2016)
1. [Realtime Data Processing at Facebook (2016)](#realtime-data-processing-at-facebook-2016)
1. [TARDiS: A Branch-and-Merge Approach To Weak Consistency (2016)](#tardis-a-branch-and-merge-approach-to-weak-consistency-2016)

## [A Relational Model of Data for Large Shared Data Banks (1970)](TODO) ##
**Summary.**
In this paper, Ed Codd introduces the *relational data model*. Codd begins by
motivating the importance of *data independence*: the independence of the way
data is queried and the way data is stored. He argues that existing database
systems at the time lacked data independence; namely, the ordering of
relations, the indexes on relations, and the way the data was accessed was all
made explicit when the data was queried. This made it impossible for the
database to evolve the way data was stored without breaking existing programs
which queried the data. The relational model, on the other hand, allowed for a
much greater degree of data independence. After Codd introduces the relational
model, he provides an algorithm to convert a relation (which may contain other
relations) into *first normal form* (i.e. relations cannot contain other
relations). He then describes basic relational operators, data redundancy, and
methods to check for database consistency.

**Commentary.**

1. Codd's advocacy for data independence and a declarative query language have
   stood the test of time. I particularly enjoy one excerpt from the paper
   where Codd says, "The universality of the data sublanguage lies in its
   descriptive ability (not its computing ability)".
2. Database systems at the time generally had two types of data: collections
   and links between those collections. The relational model represented both
   as relations. Today, this seems rather mundane, but I can imagine this being
   counterintuitive at the time.  This is also yet another example of a
   *unifying interface* which is demonstrated in both the Unix and System R
   papers.


## [Scheduling Algorithms for Multiprogramming in a Hard-Real-Time Environment (1973)](https://scholar.google.com/scholar?cluster=11972780054098474552&hl=en&as_sdt=0,5)
Consider a hard-real-time environment in which tasks *must* finish within some
time after they are requested. We make the following assumptions.

- (A1) Tasks are periodic with fixed periods.
- (A2) Tasks must finish before they are next requested.
- (A3) Tasks are independent.
- (A4) Tasks have constant runtime.
- (A5) Non-periodic tasks are not realtime.

Thus, we can model each task `t_i` as a period `T_i` and runtime `C_i`. A
scheduling algorithm that immediately preempts tasks to guarantee that the task
with the highest priority is running is called a *preemptive priority
scheduling algorithm*. We consider three preemptive priority scheduling
algorithms: a static/fixed priority scheduler (in which priorities are assigned
ahead of time), a dynamic priority scheduler (in which priorities are assigned
at runtime), and a mixed scheduling algorithm.

**Fixed Priority Scheduling Algorithm.**
First, a few definitions:

- The *deadline* of a task is the time at which the next request is issued.
- An *overflow* occurs at time `t` if `t` is the deadline for an unfulfilled
  task.
- A schedule is *feasible* if there is no overflow.
- The response time of a task is the time between the task's request and the
  task's finish time.
- A *critical instant* for task `t` is the instant where `t` has the highest
  response time.

It can be shown that the critical instant for any task occurs when the task is
requested simultaneously with all higher priority tasks. This result lets us
easily determine if a feasible fixed priority schedule exists by
pessimistically assuming all tasks are scheduled at their critical instant.

It also suggests that given two tasks with periodicities `T1` and `T2` where
`T1 < T2`, we should give higher priority to the shorter task with period `T1`.
This leads to the *rate-monotonic priority scheduling algorithm* where we
assign higher priorities to shorter tasks. A feasible static schedule exists if
and only if a feasible rate-monotonic scheduling algorithm exists.

Define *processor utilization* to be the fraction of time the processor spends
running tasks. We say a set of tasks *fully utilize* the processor if there
exists a feasible schedule for them, but increasing the running time of any of
the tasks implies there is no feasible schedule. The least upper bound on
processor utilization is the minimum processor utilization for tasks that fully
utilize the processor. For `m` tasks, the least upper bound is `m(2^(1/m) - 1)`
which approaches `ln(2)` for large `m`.

**Deadline Driven Scheduling Algorithm.**
The *deadline driven scheduling algorithm* (or earliest deadline first
scheduling algorithm) dynamically assigns the highest priority to the task with
the most imminent deadline. This scheduling algorithm has a least upper bound
of 100% processor utilization. Moreover, if any feasible schedule exists for a
set of tasks, a feasible deadline driven schedule exists.

**Mixed Scheduling Algorithm.**
Scheduling hardware (at the time) resembled a fixed priority scheduler, but a
dynamic scheduler could be implemented for less frequent tasks. A hybrid
scheduling algorithm scheduled the `k` most frequent tasks using the
rate-monotonic scheduling algorithm and scheduled the rest using the deadline
driven algorithm.

## [The Unix Time-Sharing System (1974)](https://scholar.google.com/scholar?cluster=2132419950152599605&hl=en&as_sdt=0,5)
**Summary.**
Unix was an operating system developed by Dennis Ritchie, Ken Thompson, and
others at Bell Labs. It was the successor to Multics and is probably the single
most influential piece of software ever written.

Earlier versions of Unix were written in assembly, but the project was later
ported to C: probably the single most influential programming language ever
developed. This resulted in a 1/3 increase in size, but the code was much more
readable and the system included new features, so it was deemed worth it.

The most important feature of Unix was its file system. Ordinary files were
simple arrays of bytes physically stored as 512-byte blocks: a rather simple
design. Each file was given an inumber: an index into an ilist of inodes. Each
inode contained metadata about the file and pointers to the actual data of the
file in the form of direct and indirect blocks. This representation made it
easy to support (hard) linking. Each file was protected with 9 bits: the same
protection model Linux uses today. Directories were themselves files which
stored mappings from filenames to inumbers. Devices were modeled simply as
files in the `/dev` directory. This unifying abstraction allowed devices to be
accessed with the same API. File systems could be mounted using the `mount`
command. Notably, Unix didn't support user level locking, as it was neither
necessary nor sufficient.

Processes in Unix could be created using a fork followed by an exec, and
processes could communicate with one another using pipes. The shell was nothing
more than an ordinary process. Unix included file redirection, pipes, and the
ability to run programs in the background. All this was implemented using fork,
exec, wait, and pipes.

Unix also supported signals.

## [Granularity of Locks and Degrees of Consistency in a Shared Database (1976)](https://scholar.google.com/scholar?cluster=15730220590995320737&hl=en&as_sdt=0,5)
**Granularity of Locks Summary.**
Locks are needed in a database system to ensure that transactions are isolated
from one another. But what exactly should be locked?

At one extreme, we could lock the entire database using a single lock. This
*coarse-grained* approach has incredibly low locking overhead; only one lock is
ever acquired. However, it limits the amount of concurrency in the system. Even
if two transactions operate on disjoint data, they cannot run concurrently
using a single global lock.

At the other extreme, we could lock individual fields inside of records. This
*fine-grained* approach has incredibly high concurrency. Two transactions could
execute concurrently on the same record, so long as they access disjoint
fields!  However, this approach has very high locking overhead. If the
transaction needs to read a lot of fields from a lot of records, it will spend
a lot of time acquiring a lot of locks.

A compromise between these to extremes is to use *multiple granularity
locking*, where a transaction can choose the granularity of its locks. For
example, one transaction may lock a table, another may lock a page, and another
may lock a record. Note, however, that unlike with single granularity locking,
care must be taken to ensure that locks at different granularities do not
conflict. For example, imagine one transaction has an exclusive lock on a page;
another transaction must be prohibited from acquiring an exclusive lock on the
table that the page belongs to.

In this paper, Gray et al. present an implementation of multiple granularity
locking that exploits the hierarchical nature of databases. Imagine a
database's resources are organized into a hierarchy. For example, a database
has tables, each table has pages, and each page has records. A transaction can
acquire a lock on any node in this hierarchy of one of the following types:

- IS: An *intention shared lock* on a node indicates that a transaction plans
  on acquiring a shared lock on one of the descendants of the node.
- IX: An *intention exclusive lock* on a node indicates that a transaction
  plans on acquiring an exclusive lock on one of the descendants of the node.
- S: A *shared lock* on a node implicitly grants the transaction shared read
  access to the subtree rooted at the node.
- SIX: A *SIX lock* on a node implicitly grants the transaction shared read
  access to the subtree rooted at the node and simultaneously indicates that
  the same transaction may acquire an exclusive lock on one of the descendants
  of the node.
- X: An *exclusive lock* on a node implicitly grants the transaction exclusive
  read and write access to the subtree rooted at the node.

Transactions acquire locks starting at the root and obey the following
compatibility matrix:

|     | IS  | IX  | S   | SIX | X   |
| --- | --- | --- | --- | --- | --- |
| IS  | ✓   | ✓   | ✓   | ✓   |     |
| IX  | ✓   | ✓   |     |     |     |
| S   | ✓   |     | ✓   |     |     |
| SIX | ✓   |     |     |     |     |
| X   |     |     |     |     |     |

More specifically, these are the rules for acquiring locks:

1. If a transaction wants an S or IS lock on a node, it must acquire an IX or
   IS lock on its parent.
2. If a transaction wants an X, SIX, or IX lock on a node, it must acquire a
   SIX, or IX lock on its parent.
3. Locks are either released in any order all at once after the transaction or
   released from leaf to root.

This locking protocol can easily be extended to directed acyclic graphs (DAGs)
as well. Now, a node is implicitly shared locked if one of its parents is
implicitly or explicitly shared locked. A node is implicitly exclusive locked
if all of its parents are implicitly or exclusive exclusive locked. Thus when a
shared lock is acquired on a node, it implicitly locks all nodes reachable from
it. When an exclusive lock is acquired on a node, it implicitly locks all nodes
[dominated](https://en.wikipedia.org/wiki/Dominator_(graph_theory)) by it.

The paper proves that if two lock graphs are compatible, then the implicit
locks on the leaves are compatible. Intuitively this means that the locking
protocol is equivalent to the naive scheme of explicitly locking the leaves,
but it does so without the locking overhead.

The protocol can again be extended to *dyamic lock graphs* where the set of
resources changes over time. For example, we can introduce *index interval
locks* that lock an interval of the index. To migrate a node between parents,
we simply acquire X locks on the old and new location.

**Degrees of Consistency Summary.**
Ensuring serializability is expensive, and some applications can get away with
weaker consistency models. In this paper, Grey et al. present three definitions
of four degrees of consistency.

First, we can informal define what it means for a transaction to observe degree
i consistency.

- Degree 0: no dirty writes.
- Degree 1: Degree 0 plus no writes are committed until the end of the
  transaction.
- Degree 2: Degree 1 plus no dirty reads.
- Degree 3: Degree 2 plus repeatable reads.

Second, we can provide definitions based on locking protocols.

- Degree 0: Short X locks.
- Degree 1: Long X locks
- Degree 2: Long X locks and short read locks.
- Degree 3: Long X locks and long read locks.

Finally, we can define what it means for schedule to have degree i consistency.
A transaction is a sequence of begin, end, S, X, unlock, read, and write
actions beginning with a begin and ending with an end. A schedule is a
shuffling of multiple transactions. A schedule is serial if every transaction
is run one after another. A schedule is legal if obeys a locking protocol. A
schedule is degree i consistent if every transaction observes degree i
consistency according to the first definition.

- *Assertion 1*. Definition 2 implies definition 3. That is, using the locking
  protocol for degree i ensures degree i consistent schedules.
- *Assertion 2*. Transactions can pick their consistency.

Define the following relations on transactions:

- `T1 < T2` if there is a write-write dependency between T1 and T2.
- `T1 << T2` if `T1 < T2` or there is a write-read dependency between T1 and
  T2.
- `T1 <<< T2` if `T1 << T2` or there is a read-write dependency between T1 and
  T2.

Let `<*`, `<<*`, and `<<<*` be the transitive closure of `<`, `<<`, and `<<<`.
If `<*`, `<<*`, `<<<*` is a partial order for a schedule, then the schedule is
degree 1, 2, 3 consistent.

## [On the Duality of Operating System Structures (1979)](https://scholar.google.com/scholar?cluster=12379045883699292297&hl=en&as_sdt=0,5)
**Summary.**
Lauer and Needham explain the duality in expressiveness and performance between

- *message-oriented* concurrency models in which there are a small number of
  fixed tasks that communicate explicitly, and
- *process-oriented* concurrency models in which there are a larger number of
  dynamic processes that share memory.

Message-oriented systems can be characterized by the following hallmarks,
consequences, and provided facilities.

| Hallmark                                                                                 | Consequences                                                                                            | Facilities                                                                                                      |
|------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------|
| Long standing communication channels are typically created during program initialization | Synchronization is implicitly performed in the queues connecting processes                              | Messages and message ids                                                                                        |
| There are a fewer number of long lasting processes                                       | Shared structures are passed by reference; only processes with a reference to a structure can act on it | Message channels and ports that provide the ability to `Send`, `WaitForReply`, `WaitForMessage`, or `SendReply` |
| Processes don't share memory                                                             | Peripheral devices are treated like processes and communicated with                                     | Process creation (but no deletion)                                                                              |
|                                                                                          | Processes read a small number of messages at a time                                                     |                                                                                                                 |

Process-oriented systems can be similarly characterized:

| Hallmark                                                 | Consequences                                                         | Facilities                         |
|----------------------------------------------------------|----------------------------------------------------------------------|------------------------------------|
| Global data can be protected and accessed via interfaces | Synchronization is performed in locks                                | Procedures                         |
| Process creation and deletion is a lightweight task      | Data is shared directly, with small portions being locked            | `fork`/`join` procedure invocation |
| Processes typically have a single job                    | Peripheral interaction typically involves locking and sharing memory | Modules and monitors               |
|                                                          |                                                                      | Module instantiation               |
|                                                          |                                                                      | Condition variables                |

There is a duality between the two concurrency models. Any program in one has a
corresponding program written in the other. Lauer and Needham demonstrate the
duality not by simulating  model's primitives using the other, but by drawing
similarities between the two's components:

| Message-oriented                            | Process-oriented                      |
| ------------------------------------------- | ------------------------------------- |
| Processes, `CreateProcess`                  | Monitors, `NEW`/`START`               |
| Message Channels                            | External Procedure identifiers        |
| Message Ports                               | `ENTRY` procedure identifiers         |
| `SendMessage; AwaitReply`                   | simple procedure call                 |
| `SendMessage; ... AwaitReply`               | `FORK; ... JOIN`                      |
| `SendReply`                                 | `RETURN`                              |
| `WaitForMessage` loop with `case` statement | monitor lock, `ENTRY` attribute       |
| arms of `case` statement                    | `ENTRY` procedure declarations        |
| selective waiting for messages              | condition variables, `WAIT`, `SIGNAL` |

This correspondence can be used to directly rewrite a canonical program between
the two models. In fact, the differences between the two models becomes simply
a matter of keyword choice. Moreover, if both models are implemented with
identical blocking and scheduling mechanism, then the two models lead to
identical performance as well. Since the choice of model does not affect the
user or implementor, the decision of which model to use should be based on the
architecture of the underlying hardware.

## [Experience with Processes and Monitors in Mesa (1980)](https://goo.gl/1TqQQM) ##
**Summary.**
In 1980, synchronization primitives like semaphores, monitors, and condition
variables had been well studied in the literature, but there weren't any large
systems implementing them. Mesa was a programming language that was being
developed to write the Pilot operating system at Xerox. Due to the inherent
concurrency of an operating system, Mesa was designed to ease the development
of concurrent programs. The Mesa designers considered implementing a message
passing interface, but deemed it too complex. They considered semaphores, but
found them too undisciplined. They considered cooperative multi-threading but
came upon a number of serious disadvantages:

- Cooperative multithreading cannot take advantage of multiple cores.
- Preemption is already required to service time-sensitive I/O devices.
- Cooperation is at odds with modularity. Critical sections have no principled
  way of knowing if they are calling a function which yields control.

Eventually, Mesa settled on implementing monitors and condition variables and
exposed a number of previously undiscussed issues:

- What is the interface for dynamically spawning threads and waiting for them
  to terminate?
- What is the interface for dynamically constructing monitors?
- How are threads scheduled when waiting and notifying each other?
- What are the semantics of `wait` when one monitor calls into another monitor
  which also calls `wait`?
- How are exceptions and I/O devices handled?

Mesa allowed an arbitrary function call to be forked and run by a separate
thread and eventually joined:

    p <- fork ReadLine[terminal]
    ...
    buffer <- join p

Moreover, if a forked thread was not intended to be joined, it could instead be
detached via `detach[p]`. This fork/join style process management had a number
of advantages---(i) processes were first class values, (ii) thread forking was
type checked, and (iii) any procedure could be forked---but also introduced
lots of opportunities for dangling references.

Monitors are objects that only allow a single thread to be executing one of its
functions at any given time. They unify data, synchronization of the data, and
access of the data into one lexical bundle. Mesa monitors included public
*entry preocedures* and private *internal procedures* that operated with the
monitor locked as well as public *external procedures* that operated without
locking the monitor. Monitors, in conjunction with condition variables, were
used to maintain an invariant about an object that was true upon entering and
exiting any of the monitor's methods. Monitors also lead to potential deadlocks:

- Two monitor methods could wait for one another.
- Two different monitors could enter each other.
- A monitor `M` could enter a monitor `N`, then wait on a condition that could
  only be enabled by another thread entering `M` through `N`.

Special care also had to be taken to avoid priority inversion.

Mesa also introduced Mesa semantics, best explained with this code snippet:

```python
"""
Condition variables typically obey one of two semantics:

1. Under *Hoare semantics* [1], when a thread calls `notify` on a condition
   variable, the execution of one of the threads waiting on the condition
   variable is immediately resumed. Thus, a thread that calls `wait` can assume
   very strong invariants are held when it is awoken.
2. Under *Mesa semantics* [2], a call to `notify` is nothing more than a hint.
   Threads calling `wait` can be woken up at any time for any reason.

Understanding the distinction between Hoare and Mesa semantics can be
solidified by way of example. This program implements a concurrent queue (aka
pipe) to which data can be written and from which data can be read. It spawns a
single thread which iteratively writes data into the pipe, and it spawns
`NUM_CONSUMERS` threads that read from the pipe. The producer produces the same
number of items that the consumers consume, so if all goes well, the program
will run and terminate successfully.

Run the program; not all goes well:

    Exception in thread Thread-3:
    Traceback (most recent call last):
      File "/usr/lib/python2.7/threading.py", line 810, in __bootstrap_inner
        self.run()
      File "/usr/lib/python2.7/threading.py", line 763, in run
        self.__target(*self.__args, **self.__kwargs)
      File "hoare_mesa.py", line 66, in consume
        pipe.pop()
      File "hoare_mesa.py", line 52, in pop
        return self.xs.pop(0)
    IndexError: pop from empty list

Why? The pipe is implemented assuming Python condition variables obey Hoare
semantics. They do not. Modify the pipe's implementation assuming Mesa
semantics and re-run the program. Everything should run smoothly!

[1]: https://scholar.google.com/scholar?cluster=16665458100449755173&hl=en&as_sdt=0,5
[2]: https://scholar.google.com/scholar?cluster=492255216248422903&hl=en&as_sdt=0,5
"""

import threading

# The number of objects read from and written to the pipe.
NUM_OBJECTS = 10000

# The number of threads consuming from the pipe.
NUM_CONSUMERS = 2

# An asynchronous queue (a.k.a. pipe) that assumes (erroneously) that Python
# condition variables follow Hoare semantics.
class HoarePipe(object):
    def __init__(self):
        self.xs = []
        self.lock = threading.Lock()
        self.data_available = threading.Condition(self.lock)

    # Pop the first element from the pipe, blocking if necessary until data is
    # available.
    def pop(self):
        with self.lock:
            # This code is incorrect beacuse Python condition variables follows
            # Mesa, not Hoare, semantics. To correct the code, simply replace
            # the `if` with a `while`.
            if len(self.xs) == 0:
                self.data_available.wait()
            return self.xs.pop(0)

    # Push a value to the pipe.
    def push(self, x):
        with self.lock:
            self.xs.append(x)
            self.data_available.notify()

def produce(pipe):
    for i in range(NUM_OBJECTS):
        pipe.push(i)

def consume(pipe):
    assert NUM_OBJECTS % NUM_CONSUMERS == 0
    for i in range(NUM_OBJECTS / NUM_CONSUMERS):
        pipe.pop()

def main():
    pipe = HoarePipe()
    producer = threading.Thread(target=produce, args=(pipe,))
    consumers = [threading.Thread(target=consume, args=(pipe,))
                 for i in range(NUM_CONSUMERS)]

    producer.start()
    for consumer in consumers:
        consumer.start()

    producer.join()
    for consumer in consumers:
        consumer.join()

if __name__ == "__main__":
    main()
```

Threads waiting on a condition variable could also be awoken by a timeout, an
abort, or a broadcast (e.g. `notify_all`).

Mesa's implementation was divided between the processor, a runtime, and the
compiler. The processor was responsible for process management and scheduling.
Each process was on a ready queue, monitor lock queue, condition variable
queue, or fault queue. The runtime was responsible for providing the fork/join
interface. The compiler performed code generation and a few static sanity
checks.

Mesa was evaluated by Pilot (an OS), Violet (a distributed calendar), and
Gateway (a router).

## [A History and Evaluation of System R (1981)](https://scholar.google.com/scholar?cluster=9472628621431764243&hl=en&as_sdt=0,5)
**Summary.**
Ed Codd proposed the relational model in 1970. As opposed to the navigational
data models that came before it, the relational model boasted *data
independence*: the ability for data storage and access methods to change
independently of applications. Some worried that data independence necessitated
poor performance. System R was one of the first relation databases and proved
that the relational model could be implemented efficiently.

System R development proceeded in three phases. *Phase 0* (1974-1975) was a
single-user PL/I interpreter prototype that processed a subset of SQL (e.g. no
joins) using the XRM access method. The Phase 0 prototype was always intended
to be thrown away. Instead, the focus was on tuning the user interface SQL.
User studies and interviews were performed to increase the usability and
understandability of SQL. Every tuple in the database was labelled with a TID
which contained a page number. Each tuple contained pointers into separate
domains, and *inversions* existed to map domain values to TIDs. The Phase 0
query optimizer aimed to minimize the number of fetched tuples and would
perform tricks like TID intersection to evaluate conjunctions. The prototype
also introduced the design that the system catalog should be stored as
relations. Phase 0 brought about the following ideas:

1. The optimizer should consider more than the cost of fetching tuples. It
   should also take into account the costs of TID manipulation, data fetching,
   etc.
2. Number of I/Os would have been a better metric than the number of tuples
   fetched. This would have also exposed the deficiency of the XRM access
   method.
3. The Phase 0 optimizer was CPU bound! This encouraged the later optimizer to
   be a weighted cost of CPU and I/O.
4. SQL joins are very important.
5. The query optimizer was complicated; more care should be given towards
   simpler and more common queries.

*Phase 1* ranged from 1976 to 1977 and included the implementation of a full
blown multi-user relational database. Phase 1 was divided into two pieces:

1. The *Relational Data System* (RDS) was an optimizing SQL processor
   responsible for query optimization.
2. The *Research Storage System* (RSS) was the access method that replaced XRM
   and was responsible for things like locking and logging.

Users could query System R using interactive queries or by embedding SQL
queries in PL/I or Cobol. A preprocessor would compile the embedded SQL queries
into an access module using a repository of hand-compiled fragments. Of course,
the compiled query plan could be invalidated over time. For example, the query
plan could use an index which is later dropped. Thus, each query's dependencies
were put in the system catalog and queries were recompiled when their
dependencies were invalidated.

Unlike the XRM, the RSS stored data directly in the tuples. This meant that
certain column values were stored redundantly, but an entire row could be read
in a single I/O. RSS also supported B+ tree indexes, tuple links, index scans,
full table scans, link scans, tuple nested loop joins, index nested loop joins,
and sort merge joins.

The query optimizer minimized a weighted sum of RSS calls and I/Os using a
dynamic programming approach. It avoided using some of the TID list
intersection tricks that the Phase 0 optimizer used.

Views were stored as parse trees and merged back into the SQL queries used to
query them. Updates were only allowed on single-table views. Views were the
atomic unit of authorization using a grant/revoke mechanism.

System R used a combination of logging and shadow pages to implement recovery.
During recovery, pages were restored to their old shadow pages, and the log was
processed backwards.

Since Phase 1 was a multi-user database, it introduced multiple granularity
locking in the form of intension locks. Originally, it had predicate locking,
but this was abandoned because it was (1) difficult to check for predicate
disjointness, (2) predicates were sometimes falsely marked as overlapping, and
(3) predicate locking broke the abstraction barrier of the RSS.

*Phase 2* was a two-year period in which System R was evaluated. Users
generally enjoyed the uniformity of SQL, and their recommendations led to the
introduction of EXISTS, LIKE, prepared statements, and outer joins. The query
optimizer was evaluated assuming that data was uniformly distributed and that
all columns were independent. Shadow pages led to poor locality, extra
bookkeeping, and semi-expensive page swapping. System R provided read
uncommitted, read committed, and full serializable transactions. Read
uncommitted wasn't implemented as fast as it should have been. Read committed
had more overhead than expected. Serializable transactions ended up being the
most commonly used.

**Commentary.**
System R introduced a bevy of influential and perennial ideas in the field of
databases. Unix introduced a bevy of influential and perennial ideas in the
field of operating systems. It's no coincidence that there are a striking
number of system design principles that System R and Unix---as presented in
*The Unix Time-Sharing System*---share:

1. *Unified Abstractions.* Unix unified the file and I/O device abstraction
   into a single interface. System R unified the table and catalog/metadata API
   into a single interface (i.e. everything is a relation). System R also
   unifed SQL as the query language used for ad-hoc queries, program-embeded
   queries, and view definitions. System R's decision to use relations to
   represent the catalog can also be seen as a form of dogfooding.
2. *Simple is Better.* Unix started as Ken Thompson's pet project as an effort
   to make development simpler and more enjoyable. Unix's simplicity stuck and
   was one of its selling points. Similarly, System R spent a considerable
   amount of effort simplifying the SQL interface to make it as easy to use as
   possible. If a system's interface is too complicated, nobody will use it.
3. *Performance isn't Everything.* Thompson and Ritchie implemented Unix in C
   instead of assembly despite the fact that the kernel size increased by one
   third because C greatly improved the readability and maintainability of the
   system. Similarly, the System R paper comments that the relational model may
   never exceed the performance of a hand-optimized navigational database, but
   the abstraction it provides is worth the cost. Somewhat comically, today's
   compilers and query optimizers are so good, compiled C is likely smaller
   than hand-written assembly and optimized queries are likely faster than
   hand-optimized ones. This is an example of another systems principle of
   favoring higher-level declarative APIs which leave room for optimization.

## [A Fast File System for UNIX (1984)](TODO) ##
**Summary.**
The *Fast Filesystem* (FFS) improved the read and write throughput of the
original Unix file system by 10x by

1. increasing the block size,
2. dividing blocks into fragments, and
3. performing smarter allocation.

The original Unix file system, dubbed "the old file system", divided disk
drives into partitions and loaded a file system on to each partition. The
filesystem included a superblock containing metadata, a linked list of free
data blocks known as the *free list*, and an *inode* for every file. Notably,
the file system was composed of **512 byte** blocks; no more than 512 bytes
could be transfered from the disk at once. Moreover, the file system had poor
data locality. Files were often sprayed across the disk requiring lots of
random disk accesses.

The "new file system" improved performance by increasing the block size to any
power of two at least as big as **4096 bytes**. In order to handle small files
efficiently and avoid high internal fragmentation and wasted space, blocks were
further divided into *fragments* at least as large as the disk sector size.

          +------------+------------+------------+------------+
    block | fragment 1 | fragment 2 | fragment 3 | fragment 4 |
          +------------+------------+------------+------------+

Files would occupy as many complete blocks as possible before populating at
most one fragmented block.

Data was also divided into *cylinder groups* where each cylinder group included
a copy of the superblock, a list of inodes, a bitmap of available blocks (as
opposed to a free list), some usage statistics, and finally data blocks. The
file system took advantage of hardware specific information to place data at
rotational offsets specific to the hardware so that files could be read with as
little delay as possible. Care was also taken to allocate files contiguously,
similar files in the same cylinder group, and all the inodes in a directory
together. Moreover, if the amount of available space gets too low, then it
becomes more and more difficult to allocate blocks efficiently. For example, it
becomes hard to allocate the files of a block contiguously. Thus, the system
always tries to keep ~10% of the disk free.

Allocation is also improved in the FFS. A top level global policy uses file
system wide information to decide where to put new files. Then, a local policy
places the blocks. Care must be taken to colocate blocks that are accessed
together, but crowding a single cyclinder group can exhaust its resources.

In addition to performance improvements, FFS also introduced

1. longer filenames,
2. advisory file locks,
3. soft links,
4. atomic file renaming, and
5. disk quota enforcement.

## [End-to-End Arguments in System Design (1984)](TODO) ##
**Summary.**
This paper presents the *end-to-end argument*:

> The function in question can completely and correctly be implemented only
> with the knowledge and help of the application standing at the end points of
> the communication system. Therefore, providing that questioned function as a
> feature of the communication system itself is not possible. (Sometimes an
> incomplete version of the function provided by the communication system may
> be useful as a performance enhancement.)

which says that in a layered system, functionality should, nay must be
implemented as close to the application as possible to ensure correctness (and
usually also performance).

The end-to-end argument is motivated by an example file transfer scenario in
which host A transfers a file to host B. Every step of the file transfer
presents an opportunity for failure. For example, the disk may silently corrupt
data or the network may reorder or drop packets. Any attempt by one of these
subsystems to ensure reliable delivery is wasted effort since the delivery may
still fail in another subsystem. The only way to guarantee correctness is to
have the file transfer application check for correct delivery itself. For
example, once it receives the entire file, it can send the file's checksum back
to host A to confirm correct delivery.

In addition to being necessary for correctness, applying the end-to-end
argument also usually leads to improved performance. When a functionality is
implemented in a lower level subsystem, every application built on it must pay
the cost, even if it does not require the functionality.

There are numerous other examples of the end-to-end argument:

- Guaranteed packet delivery.
- Secure data transmission.
- Duplicate message suppression.
- FIFO delivery.
- Transaction management.
- RISC.

The end-to-end argument is not a hard and fast rule. In particular, it may be
eschewed when implementing a functionality in a lower level can lead to
performance improvements. Consider again the file transfer protocol above and
assume the network drops one in every 100 packets. As the file becomes longer,
the odds of a successful delivery become increasingly small making it
prohibitively expensive for the application alone to ensure reliable delivery.
The network may be able to perform a small amount of work to help guarantee
reliable delivery making the file transfer more efficient.

## [Concurrency Control Performance Modeling: Alternatives and Implications (1987)](https://scholar.google.com/scholar?cluster=9784855600346107276&hl=en&as_sdt=0,5)
**Overview.**
There are three types of concurrency control algorithms: locking algorithms,
timestamp based algorithms, optimistic algorithms. There have been a large
number of performance analyses aimed at deciding which type of concurrency
algorithm is best, however despite the abundance of analyses, there is no
definitive winner. Different analyses have contradictory results, largely
because there is no standard performance model or set of assumptions. This
paper presents a complete database model for evaluating the performance of
concurrency control algorithms and discusses how varying assumptions affect the
performance of various algorithms.

**Performance Model.**
This paper analyzes three specific concurrency control mechanisms,

- *Blocking.* Transactions acquire locks before they access a data item.
  Whenever a transaction acquires a lock, deadlock detection is run. If a
  deadlock is detected, the youngest transaction is aborted.
- *Immediate-restart.* Again, transactions acquire locks, but instead of
  blocking if a lock cannot be immediately acquired, the transaction is instead
  aborted and restarted with delay. This delay is adjusted dynamically to be
  roughly equal to the average transaction duration.
- *Optimistic.* Transactions do not acquire locks. A transaction is aborted
  when it goes to commit if it read any objects that had been written and
  committed since the transaction began.

using a closed queueing model of a single-site database. Essentially,
transactions come in, sit in some queues, and are controlled by a concurrency
control algorithm. The model has a number of parameters, some of which are held
constant for all the experiments and some of which are varied from experiment
to experiment. Some of the parameters had to be tuned to get interesting
result. For example, it was found that with a large database and few conflicts,
all concurrency control algorithms performed roughly the same and scaled with
the degree of parallelism.

**Resource-Related Assumptions.**
Some analyses assume infinite resources. How do these assumptions affect
concurrency control performance?

- *Experiment 1: Infinite Resources.* Given infinite resources, higher degrees
  of parallelism lead to higher likelihoods of transaction conflict which in
  turn leads to higher likelihoods of transaction abort and restart. The
  blocking algorithm thrashes because of these increased conflicts. The
  immediate-restart algorithm plateaus because the dynamic delay effectively
  limits the amount of parallelism. The optimistic algorithm does well because
  aborted transactions are immediately replaced with other transactions.
- *Experiment 2: Limited Resources.* With a limited number of resources, all
  three algorithms thrash, but blocking performs best.
- *Experiment 3: Multiple Resources.* The blocking algorithm performs best up
  to about 25 resource units (i.e. 25 CPUs and 50 disks); after that, the
  optimistic algorithm performs best.
- *Experiment 4: Interactive Workloads.* When transactions spend more time
  "thinking", the system begins to behave more like it has infinite resources
  and the optimistic algorithm performs best.

**Transaction Behavior Assumptions.**

- *Experiment 6: Modeling Restarts.* Some analyses model a transaction restart
  as the spawning of a completely new transaction. These fake restarts lead to
  higher throughput because they avoid repeated transaction conflict.
- *Experiment 7: Write-Lock Acquisition.* Some analyses have transactions
  acquire read-locks and then upgrade them to write-locks. Others have
  transactions immediately acquire write-locks if the object will ever be
  written to. Upgrading locks can lead to deadlock if two transactions
  concurrently write to the same object. The effect of lock upgrading varies
  with the amount of available resources.

## [The Design of the POSTGRES Storage System (1987)](TODO) ##
**Summary.**
POSTGRES, the ancestor of PostgreSQL, employed a storage system with three
interesting characteristics:

1. No write-ahead logging (WAL) was used. In fact, there was no recovery code
   at all.
2. The entire database history was recorded and archived. Updates were
   converted to updates, and data could be queried arbitrarily far in the past.
3. The system was designed as a collection of asynchronous processes, rather
   than a monolithic piece of code.

Transactions were sequentially assigned 40-bit transaction identifiers (XID)
starting from 0. Each operation in a transaction was sequentially assigned a
command identifiers (CID). Together the XID and CID formed a 48 bit interaction
identifier (IID). Each IID was also assigned a two-bit transaction status and
all IIDs were stored in a transaction log with a most recent *tail* of
uncommitted transactions and a *body* of completed transactions.

Every tuple in a relation was annotated with

- a record id,
- a min XID, CID, and timestamp,
- a max XID, CID and timestamp, and
- a forward pointer.

The min values were associated with the transaction that created the record,
and the max values were associated with the transaction that updated the
record. When a record was updated, a new tuple was allocated with the same
record id but updated min values, max values, and forward pointers. The new
tuples were stored as diffs; the original tuple was the *anchor point*; and the
forward pointers chained together the anchor point with its diffs.

Data could be queried at a particular timestamp or in a range of timestamps.
Moreover, the min and max values of the records could be extracted allowing for
queries like this:

    SELECT Employee.min_timestamp, Eployee.max_timestamp, Employee.id
    FROM Employee[1 day ago, now]
    WHERE Employee.Salary > 10,000

The timestamp of a transaction was not assigned when the transaction began.
Instead, the timestamps were maintained in a TIME relation, and the timestamps
in the records were left empty and asynchronously filled in. Upon creation,
relations could be annotated as

- *no archive* in which case timestamps were never filled in,
- *light archive* in which timestamps were read from a TIME relation, or
- *heavy archive* in which timestamps were lazily copied from the TIME relation
  into the records.

POSTGRES allowed for any number of indexes. The type of index (e.g. B-tree) and
the operations that the index efficiently supported were explicitly set by the
user.

A *vacuum cleaner* process would, by instruction of the user, vacuum records
stored on disk to an archival storage (e.g. WORM device). The archived data was
allowed to have a different set of indexes. The vacuum cleaning proceeded in
three steps:

1. Data was archived and archive indexes were formed.
2. Anchor points were updated in the database.
3. Archived data space was reclaimed.

The system could crash during this process which could lead to duplicate
entries, but nothing more nefarious. The consistency guarantees were a bit weak
compared to today's standards. Some crashes could lead to slowly accumulating
un-reclaimed space.

Archived data could be indexed by values and by time ranges efficiently using
R-trees. Multi-media indexes which spanned the disk and archive were also
supported.

## [The POSTGRES Next Generation Database Management System (1991)](https://scholar.google.com/scholar?cluster=6521586065605065941&hl=en&as_sdt=0,5)
POSTGRES is a relational database which supports features along three
dimensions:

1. *Data management.* Like all relational databases, POSTGRES processes
   transactions and queries.
2. *Object management.* POSTGRES can efficiently store and manipulate
   non-traditional data types like bitmaps, text, and polygons.
3. *Knowledge management.* POSTGRES provides rules to specify integrity
   constraints and derived data.

**POSTGRES Data Model and Query Language.**
Traditional databases support a very basic set of types: float, int, char,
string, money, date, etc. POSTGRES includes a richer data model. The design of
the data model and query language was governed by three principles:

1. *Orientation towards database access from a query language.* Most people
   will access POSTGRES using a set-oriented, SQL-like query language called
   POSTQUEL. POSTGRES also supports a navigational interface where tuples can
   be navigated using their unique OID. POSTGRES also supports user defined
   functions which include statements, queries, and direct calls into POSTGRES
   internal interfaces. These functions can be called from within POSTQUEL or
   run directly by a program. The ability for programs to call directly into
   POSTGRES internals is known as the *fast path*.
2. *Orientation towards multilingual access.* POSTGRES could have tightly
   integrated into a specific programming language. For example, certain
   variables in a program could be persisted into the database, or perhaps
   queries could be embedded in the control flow of the program. However, the
   authors believe that databases are accessed by multiple programming
   languages. Still, one can still integrate a programming language with
   POSTGRES easily using the fast path.
3. *Small number of concepts.* The POSTGRES data model and query language was
   designed to be simple. They revolve around four basic concepts: classes,
   inheritance, types, and functions.

**POSTGRES Data Model.**
A POSTGRES database is a collection of *classes*. Each class is a collection of
*instances*. Each instance is a collection of named, typed *attributes*. Each
instance is assigned a unique OID. Classes can inherit from other classes, and
multiple inheritance is allowed so long as no ambiguities arise.

| Relational Data Model | POSTGRES Data Model |
| --------------------- | ------------------- |
| relation              | class               |
| tuple                 | instance            |
| field                 | attribute           |

There are three types of *classes*, three types of *types*, and three types of
*functions*. First, the classes:

1. *Real (base) classes* are standard classes which are stored by the database.
2. *Derived (virtual, view) classes* are classes which are derived from other
   classes in the database. These are essentially views.
3. *Versions.* A version is a branched copy of a parent class which is stored
   as a set of diffs from the parent.

Next, the types:

1. *Base types* are the standard set of simple types like float, int, char,
   etc. POSTGRES also allows programmers to define their own base types defined
   by functions which serialize instances of the type to and from character
   strings.
2. *Array types* are arrays of other types.
3. *Composite types* are like records. Instances can hold other instances. For
   example, an employee instance can include an employee field. Moreover,
   POSTGRES supports a `set` type which is a heterogeneously typed set; that
   is, its a set of things where each thing can be of any type.

Finally, the functions:

1. *C Functions.* Users can write arbitrary C functions which operate over base
   and composite types. These functions cannot be optimized by POSTGRES.
2. *Operators.* Operators are like unary or binary functions with optimizer
   hints. For example, a user could provide an "area greater than" operator
   `AGT` to compare the area of two polygons and tell the optimizer that its
   complement is the `ALT` operator. Moreover, programmers can write their own
   custom access methods to efficiently implement operators. For example,
   programmers could implement a special indexing data structure to efficiently
   support polygon overlap queries.
3. *POSTQUEL functions.* POSTQUEL functions wrap up a sequence of POSTQUEL
   queries.

**POSTGRES Query Language.**
POSTQUEL is a superset of a relational query language which supports nested
queries, transitive closures, inheritance, and time travel. Nested queries are
self-explanatory. Transitive closures are like recursive SQL queries.
Inheritance support means that POSTQUEL can query either a class or the class
and all subclasses. Time travel is a fancy term for historical queries.

**Fast Path.**
There are two reasons to include a fast path:

1. Some applications use their own query language. For these applications, they
   construct a query AST and it is difficult to convert the AST to a textual
   query. It is easier to directly call into the POSTGRES internals to execute
   the query.
2. Integrating POSTGRES into programming language sometimes requires some
   low-level tricks. For example, the authors integrated POSTGRES into Lisp
   which required them to reserve sets of OIDs: something which could only be
   done using the fast path.

Note that the fast path is essentially an RPC mechanism.

**POSTGRES Rules.**
POSTGRES wanted *one* rule system which supported *all* of view management,
triggers, integrity constraints, referential integrity, protection, and version
control. POSTGRES rules take the form "if some event happens which satisfies
some properties, then run some sequence of commands."

For example, imagine we want to maintain an invariant that Joe's salary and
Fred's salary are th same.  We can install a rule which updates Joe's salary
whenever Fred's salary is updated. Note that when the body of the rule is
executed, it may trigger other rules to fire. This is known as *forward
chaining*.

Alternatively, imagine we install a rule which rewrites reads of Joe's salary
to reads of Fred's salary. In this case, reading a piece of data may trigger
other rules to generate the read. This is known as *backwards chaining*.

**Implementaton of Rules**
There are two implementations of POSTGRES rules: a record-level implementation
and a query rewrite implementation.

1. *Record-level implementation.* In this implementation, instances or instance
   fields are annotated with markers which point to the rules which must be
   evaluated when the instance or instance field change. For example, Fred's
   salary may be annotated with a marker which points to the rule to update
   Joe's salary. When the query evaluator encounters one of these markers, it
   executes it. Care must be taken to avoid certain corner cases. For example,
   if Fred changes his name, then the marker should be removed.
2. *Query rewrite implementation.* Sometimes, a bulk query would cause a lot of
   individual markers to be executed again and again. These types of queries
   can more efficiently be implemented using a query rewrite mechanism in which
   all the rules are fired at once.

**Rules Semantics.**
The record-level and query rewrite rule implementations provide different
semantics. Moreover, there are other semantic choices to make. Should rules be
run immediately or should their execution be deferred? Should the rule be run
in the same transactions which caused it or in a separate transaction. All
combinations of these choices can be useful. POSTGRES currently only implements
one.

**Rules System Applications.**
Rules can be used to manage *views* and *versions*.

- *Views*. View creation statements are compiled to set of rules which define
  the view. For example, queries against the view can be rewritten as queries
  against the base tables over which the view is defined. By default, views can
  be updated only when doing so is unambiguous. However, POSTGRES allows
  programmers to write custom rules to allow for more complex updates.
- *Versions*. A version of a base table is like a branched copy of the table.
  The version can be updated without affecting the base table. Versions can be
  implemented trivially by copying the base table, but they can be implemented
  more efficiently using diffs. When a user creates a version, POSTGRES creates
  a positive and negative delta table and uses rules to maintain them.

**Storage System.**
POSTGRES uses a no-overwrite storage system. This storage system makes crash
recovery practically instantaneous and allows for historical, time travelling
queries. The implementation of the no-overwrite storage engine requires that
when a transaction commits, all of the pages it modified be written to disk.
Thus, an efficient implementation of the no-overwrite storage engine depends on
something like non-volatile memory.

**POSTGRES implementation.**
POSTGRES has four notable implementation details:

1. POSTGRES uses a process per user. This was done because it was simple.
2. The parser, optimizer, and execution engine are table driven and read
   configuration from the catalog. This makes the database extendable.
3. Types, operators, and functions can be loaded dynamically.
4. The rule system implementations are novel.

## [Microkernel Operating System Architecture and Mach (1992)](https://scholar.google.com/scholar?cluster=1074648542567860981&hl=en&as_sdt=0,5)
**Summary.**
A *microkernel* is a very minimal, very low-level piece of code that interfaces
with hardware to implement the functionality needed for an operating system.
Operating systems implemented using a microkernel architecture, rather than a
monolithic kernel architecture, implement most of the operating system in user
space on top of the microkernel.  This architecture affords many advantages
including:

- *tailorability*: many operating systems can be run on the same microkernel
- *portability*: most hardware-specific code is in the microkernel
- *network accessibility*: operating system services can be provided over the
  network
- *extensibility*: new operating system environments can be tested along side
  existing ones
- *real-time*: the kernel does not hold locks for very long
- *multiprocessor support*: microkernel operations can be parallelized across
  processors
- *multicomputer support*: microkernel operations can be parallelized across
  computers
- *security*: a microkernel is a small trusted computing base

This paper describes various ways in which operating systems can be implemented
on top of the Mach microkernel. Mach's key features include:

- *task and thread management*: Mach supports tasks (i.e. processes) and
  threads. Mach implements a thread scheduler, but privileged user level
  programs can alter the thread scheduling algorithms.
- *interprocess communication*: Mach implements a capabilities based IPC
  mechanism known as ports. Every object in Mach (e.g. tasks, threads, memory)
  is managed by sending message to its corresponding port.
- *memory object management*: Memory is represented as memory objects managed
  via ports.
- *system call redirection*: Mach allows a number of system calls to be caught
  and handled by user level code.
- *device support*: Devices are represented as ports.
- *user multiprocessing*: Tasks can use a user-level threading library.
- *multicomputer support*: Mach abstractions can be transparently implemented
  on distributed hardware.
- *Continuations*: In a typical operating system, when a thread blocks, all of
  its registers are stored somewhere before another piece of code starts to
  run. Its stack is also left intact. When the blocking thread is resumed, its
  stored registers are put back in place and the thread starts running again.
  This can be wasteful if the thread doesn't need all of the registers or its
  stack. In Mach, threads can block with a continuation: an address and a bunch
  of state. This can be more efficient since the thread only saves what it
  needs to keep executing.

Many different operating systems can be built on top of Mach. It's ideal that
applications built for these operating systems can continue to run unmodified
even when the underlying OS is implemented on top of Mach. A key part of this
virtualization is something called an *emulation library*. An emulation library
is a piece of code inserted into an application's address space. When a program
issues system call, Mach immediately redirects control flow to the emulation
library to process it. The emulation library can then handle the system call
by, for example, issuing an RPC to an operating system server.

Operating systems built on Mach can be architected in one of three ways:

1. The entire operating system can be baked into the emulation library.
2. The operating system can be shoved into a single multithreaded Mach task.
   This architecture can be very memory efficient, and is easy to implement
   since the guest operating system can be ported with very little code
   modification.
3. The operating system can be decomposed into a larger number of smaller
   processes that communicate with one another using IPC. This architecture
   encourages modularity, and allows certain operating system components to be
   reused between operating systems. This approach can lead to inefficiency,
   especially if IPC is not lighting fast!

## [The Linda alternative to message-passing systems (1994)](https://scholar.google.com/scholar?cluster=2449406388273902590&hl=en&as_sdt=0,5) ##
**Summary.**
*Distributed shared memory* is a powerful abstraction for implementing
distributed programs, but implementing the abstraction efficiently is very
challenging. This led to the popularity of message passing abstractions for
parallel and distributed programming which were easier to implement. Linda is a
programming model and family of programming languages that does implement
distributed shared memory as efficiently (or almost as efficiently) as message
passing.

Linda's memory model revolves around a *tuple space*: a collection of tuples
(pretty much a table). Users interact with the tuple space by writing programs
in one of the Linda languages. For example, C-Linda programs are traditional C
programs that can additionally use one of a few Linda constructs:

- `out(...)` synchronously writes tuples into a tuple space.
- `eval(...)` concurrently evaluates its arguments and writes tuples into a
  tuple space asynchronously.
- `in(...)` reads and removes a single tuple from the tuple space using a tuple
  template: a partial tuple filled in with wildcards.
- `rd(...)` is a nondestructive version of `in`.

The paper argues for the flexibility and expressiveness of Linda's memory
model. It makes it easy to implement master/slave architectures where all
workers can access a shared data structure. The data-centric approach allows
processes to communicate by reading and writing data rather than bothering with
message passing details. Tuple spaces make it easy to implement *static load
balancing* in which some static domain is divided evenly between workers and
*dynamic load balancing* in which the tuple space acts as a queue of requests
which are read and processed by workers.

Linda's implementation comprises three parts:

1. *Language-dependent compilers* compile out the Linda specific constructs.
   For example, the C-Linda compiler compiles a C-Linda compiler into pure C;
   the Linda constructs are compiled to function calls which eventually call
   into the Linda runtime.
2. The *link-time optimizer* chooses the best tuple space accessor
   implementations to use for the specific program.
3. The *machine-dependent run-time* partitions the tuple space so that each
   participating machine serves as both a *computation server* and a *tuple
   space server*. Every tuple of a particular tuple type (determined by the
   number of types of its constituents) is assigned to a single Linda server
   known as the *rendezvous point*. The runtime performs a few notable
   optimizations:

    - Certain tuple types are partitioned across machines, rather than all
      being assigned to a single rendezvous point.
    - Certain tuple fields can be quite large in which case, they are not
      transfered from a machine to the rendezvous point. Instead, they are kept
      locally on the machine that produced them. The rendezvous point is then
      used as a metadata service to locate the field's location. This is
      similar to GFS's design.
    - Some reads are broadcasted to all nodes unsolicitedly as a form of
      prefetching.
    - Rendezvous nodes can be reassigned dynamically depending on the workload
      to place data as close as possible to the accessor.

**Commentary.**
The paper argues that Linda's memory model is expressive, and they support
their claim by implementing a particular scientific computing style
application. I disagree. I think the memory model is primitive and missing some
very useful features. In particular,

- Only one tuple can ever be returned at a time.
- Tuple queries are based solely on equality of individual fields. Queries such
  as "select all tuples whose first column is greater than its third column"
  are not supported.
- There are no transactions. The paper also does not describe the consistency
  of the system.

Essentially, the system is more or less a key-value store which arguably is not
expressive enough for many applications. Moreover, I believe there are some
implementation oversights.

- Data is not replicated.
- Only certain tuple types are sharded, and skewed data distributions are not
  discussed.

## [Lottery and Stride Scheduling: Flexible Proportional-Share Resource Management (1994)](https://scholar.google.com/scholar?cluster=15431493885859350148&hl=en&as_sdt=0,5)
Schedulers multiplex scarce resources and can greatly impact the throughput and
response times of a system. Ideally, schedulers service clients with *relative
computation rates*. Existing priority schedulers do this to some degree but are
ad-hoc. Similarly, fair-share and microeconomic schedulers are too coarse. This
paper introduces *lottery scheduling* which implements proportional-share
resource management: consumption rates are proportional to relative shares
allocated.

**Lottery Scheduling.**
Lottery schedulers allocate resource rights to clients in the form of
*tickets*. Lotteries are held, and the client with the winning ticket is
granted the resource.

Tickets are

- *abstract*: tickets are a logical entity independent of underlying machine
  details.
- *relative*: the value of a ticket varies with the total number of tickets.
- *uniform*: heterogeneous resources can be managed homogeneously with tickets.

Lottery scheduling is a probabilistic scheduling algorithm, but when quanta are
small, fairness can be achieved rapidly. Moreover, there is no fear of
starvation. As long as a client has tickets, it will eventually be scheduled.

**Modular Resource Management.**
- *Ticket transfers.* Clients can transfer tickets to others. For example, a
  client blocking on a server can transfer tickets to the server. This
  effectively avoids priority inversion.
- *Ticket inflation.* Mutually trusting applications can inflate or deflate
  tickets in order to change resource allocation without communication.
- *Currencies.* Logically distinct groups of mutually trusting applications can
  establish their own currencies backed by the same base currency. This allows
  for finer grained ticket allocation.
- *Compensation tickets.* If a client runs for only `f` of its quanta, its
  number of tickets is boosted by `1/f` to ensure fairness.

**Implementation.**
The authors implemented a lottery scheduling prototype in the Mach 3.0
microkernel with all of the features listed above and with a 100 ms quantum.

- *Random numbers.* Random numbers are generated using a 10 instruction pseudo-random number generator.
- *Lotteries.* Clients can be stored in a list where each list entry includes
  the number of tickets assigned to the client. Given a ticket number drawn for
  a lottery, the client with that ticket can be found by linearly searching
  through the clients maintaining a partial sum along the way. Sorting clients
  in descending order starting with those with the most tickets decreases the
  number of clients that need to be searched. This can be implemented also with
  a move-to-front policy. Moreover, one could also use a tree or a binary
  search.
- *Mach kernel interface.* Mach provides a minimal interface to create and
  destroy tickets ad currencies, fund currencies, unfund currencies, etc. A
  ticket includes its name and amount. A currency includes a list of backing
  tickets, its name, the amount of tickets allocated, and a list of the
  allocated tickets.
- *Ticket currencies.* Currencies are subdivided into other currencies forming
  a currency graph which makes it easy to translate all currency amounts into a
  base currency amount. The prototype uses a move-to-front linear search with
  partial sums computed in the base currency.
- *Compensation tickets.* Compensation tickets ensure that two clients with the
  same number of tickets execute for the same amount of time even if one of
  them does not use its full quantum. This is good for I/O bound tasks.
- *Ticket transfer.* The prototype instrumented synchronous IPC calls to
  automatically transfer tickets from clients to servers.
- *User interface.* Users can manage tickets through the command line.

**Managing Diverse Resources.**
Lottery scheduling can be used to manage more than CPU quanta.

- *Synchronization resources.* The prototype implements lottery scheduled
  mutexes in which all threads blocked on the mutex transfer tickets to the
  thread who holds the mutex. When the mutex is released, a lottery is held to
  hand it off to one of the waiting threads. This type of mutex does not suffer
  from priority inversion.
- *Space-shared resources.* Lottery scheduling can also be used to multiplex
  space. We run an inverse-lottery in which a loser, as opposed to a winner, is
  chosen. For example, if we need to allocate a physical page to back a virtual
  page, but all the virtual pages are taken, we run an inverse lottery to
  select a loser and flush the page to disk. Each transaction is chosen with
  probability `(1/(n-1))(1 - t/T)` where `n` is the number of clients, `t` is
  the number of tickets for a client, and `T` is the total number of tickets.
  The multiplier is a normalizing factor and the multiplicand is the inverse
  probability.

## [A Critique of ANSI SQL Isolation Levels (1995)](https://scholar.google.com/scholar?cluster=2396911751922252868&hl=en&as_sdt=0,5)
**Overview**
The ANSI SQL standard defines Dirty Read, Non-repeatable Read, and Phantom
anomalies. Unfortunately, these definitions are written in English and are a
bit ambiguous. Each definition can be formalized in one of two ways. Dirty
Reads can be characterized by P1 or A1, Non-repeatable Reads by P2 or A2, and
Phantom by P3 or A3. Using these ambiguous definitions, the ANSI standard
defines a set of isolation levels tabularized below.

Alternatively, we can define an isolation level as the set of histories allowed
by a particular lock-based concurrency control mechanism. This is also
tabularized below.

**What's Wrong With ANSI?**
There are a couple things weird about the ANSI definitions besides their
ambiguity. First, they do not prohibit P0: Dirty Writes. Dirty writes can
violate constraints between objects (e.g. `w1[x] w2[x] w2[y] w1[y]`) and also
makes recovery challenging (e.g. `w1[x] w2[x] a1`).

Second, prohibiting A1, A2, and A3 does not provide serializability. That is,
ANOMALY SERIALIZABLE is not SERIALIZABLE. Each of A1, A2, and A3 should be
replaced with P1, P2, and P3. Failure to do so leads to different anomalous
histories: H1, H2, and H3, each of which exploits constraints between multiple
objects.

There is also a natural correspondence between the lock-based concurrency
control mechanisms and the P1, P2, P3 phenomena.

**What's New?**
We can also introduce more isolation levels. *Cursor Stability* is designed to
avoid the lost update phenomenon (P4, P4C) and falls between READ COMMITTED and
REPEATABLE READ. In Cursor Stability, read locks are held on an object until
the cursor pointing at the object advances to the next object. Note that these
read locks are still not long, but they're longer than short.

In *Snapshot Isolation*, each transaction is assigned a begin timestamp when it
first begins executing. Subsequent reads read from the database at this
timestamp. When the transaction commits, it is given a commit timestamp. If the
transaction's write set doesn't overlap with any other transactions that have
committed in [begin timestamp, commit timestamp], then the transaction commits.
Snapshot Isolation prevents A5A but not A5B. It is stronger than READ
COMMITTED, incomparable to REPEATABLE READ, and weaker than SERIALIZABLE.

**Hasse Diagrams**
Let `NSS(L)` be the set of all non-serializable histories satisfying isolation
level `L`. We can define a partial order on isolation levels by ordering their
non-serializable histories by subset. For example, `L1 <= L2` if `NSS(L1)
\subseteq NSS(L2)`. Using this partial order, we can draw the Hasse diagram
below.

**Tables and Figures**

| Code | Name                         | History                              |
| ---- | ---------------------------- | ------------------------------------ |
| P0   | Dirty Write                  | `w1[x] w2[x] {c1/a1}x{c2/a2}`        |
| P1   | Dirty Read                   | `w1[x] r2[x] {c1/a1}x{c2/a2}`        |
| P2   | Non-repeatable or Fuzzy Read | `r1[x] w2[x] {c1/a1}x{c2/a2}`        |
| P3   | Phantom                      | `r1[P] w2[y in P] {c1/a1}x{c2/a2}`   |
| P4   | Lost Update                  | `r1[x] w2[x] w1[x] c1`               |
| P4C  | Cursor Lost Update           | `rc1[x] w2[x] w1[x] c1`              |
| A1   | Dirty Read                   | `w1[x] r2[x] {a1}x{c2}`              |
| A2   | Non-repeatable or Fuzzy Read | `r1[x] w2[x] c2 r1[x] c1`            |
| A3   | Phantom                      | `r1[P] w2[y in P] c2 r1[P] c1`       |
| A5A  | Read Skew                    | `r1[x] w2[x] w2[y] c2 r1[y] {c1/a1}` |
| A5B  | Write Skew                   | `r1[x] r2[y] w1[y] w2[x] {c1}x{c2}`  |

| Name     | History                                                             |
| -------- | ------------------------------------------------------------------- |
| H1       | `r1[x=50] w1[x=10] r2[x=10] r2[y=50] c2 r1[y=50] w1[y=90] c1`       |
| H1.SI    | `r1[x0=50] w1[x1=10] r2[x0=50] r2[y0=50] c2 r1[y0=50] w1[y1=90] c1` |
| H1.SI.SV | `r1[x=50] r1[y=50] r2[x=50] r2[y=50] c2 w1[x=10] w1[y=90] c1`       |
| H2       | `r1[x=50] r2[x=50] w2[x=10] r2[y=50] w2[y=90] c2 r1[y=90] c1`       |
| H3       | `r1[P] w2[insert y to P] r2[z] w2[z] c2 r1[z] c1`                   |
| H4       | `r1[x=100] r2[x=100] w2[x=120] c2 w1[x=130] c1`                     |
| H5       | `r1[x=50] r1[y=50] r2[x=50] r2[y=50] w1[y=40] w2[x=40] c1 c2`       |

| Isolation Level       | P1/A1 Dirty Read | P2/A2 Fuzzy Read | P3/A3 Phantom |
| --------------------- | ---------------- | ---------------- | ------------- |
| ANSI READ UNCOMMITTED | ✓                | ✓                | ✓             |
| ANSI READ COMMITTED   |                  | ✓                | ✓             |
| ANSI REPEATABLE READ  |                  |                  | ✓             |
| ANOMALY SERIALIZABLE  |                  |                  |               |

| Consistency Level                   | Read Locks                 | Write Locks |
| ----------------------------------- | -------------------------- | ----------- |
| Degree 0                            | none                       | short       |
| Degree 1 = Locking READ UNCOMMITTED | none                       | long        |
| Degree 2 = Locking READ COMMITTED   | short                      | long        |
| Cursor Stability                    | cursor lock                | long        |
| Locking REPEATABLE READ             | long item; short predicate | long        |
| Degree 3 = Locking SERIALIZABLE     | long                       | long        |

| Isolation Level  | P0 Dirty Write | P1 Dirty Read | P2 Fuzzy Read | P3 Phantom |
| ---------------  | -------------- | ------------- | ------------- | ---------- |
| READ UNCOMMITTED |                | ✓             | ✓             | ✓          |
| READ COMMITTED   |                |               | ✓             | ✓          |
| REPEATABLE READ  |                |               |               | ✓          |
| SERIALIZABLE     |                |               |               |            |


```
                            Serializable = Degree 3
                             /                 \
                    Repeatable Read             |
                           |               Snapshot Isolation
                    Cursor Stability            |
                             \                 /
                           Read Committed = Degree 2
                                       |
                          Read Uncommitted = Degree 1
                                       |
                                    Degree 0
```

## [Exokernel: An Operating System Architecture for Application-Level Resource Management (1995)](https://scholar.google.com/scholar?cluster=4636448334605780007&hl=en&as_sdt=0,5)
**Summary.**
Monolithic kernels provide a large number of abstractions (e.g. processes,
files, virtual memory, interprocess communication) to applications.
Microkernels push some of this functionality into user space but still provide
a fixed set of abstractions and services. Providing these inextensible fixed
abstractions is detrimental to applications.

- An application cannot be best for all applications. Tradeoffs must be made
  which can impact performance for some applications.
- A rigid set of abstractions can make it difficult for an application to layer
  on its own set of abstractions. For example, a user level threads package may
  encounter some difficulties of not having access to page faults.
- Having a rigid set of abstractions means the abstractions are rarely updated.
  Innovative OS features are seldom integrated into real world OSes.

The exokernel operating system architecture takes a different approach. It
provides protected access to hardware and nothing else. All abstractions are
implemented by library operating systems. The exokernel purely provides
protected access to the unabstracted underlying hardware.

The exokernel interface governs how library operating systems get, use, and
release resources. Exokernels follow the following guidelines.

- *Securely expose hardware.* All the details of the hardware (e.g. privileged
  instructions, DMA) should be exposed to libOSes.
- *Expose allocation.* LibOSes should be able to request physical resources.
- *Expose physical names.* The physical names of resources (e.g. physical page
  5) should be exposed.
- *Expose revocation.* LibOSes should be notified when resources are revoked.

Exokernels use three main techniques to ensure protected access to the
underlying hardware.

1. *Secure bindings.* A secure binding decouples authorization from use and is
   best explained through an example. A libOS can request that a certain entry
   be inserted into a TLB. The exokernel can check that the entry is valid.
   This is the authorization. Later, the CPU can use the TLB without any
   checking. This is use. The TLB entry can be used multiple times after being
   authorized only once.

   There are three forms of secure bindings. First are hardware mechanism like
   the TLB entries or screens in which each pixel is tagged with a process.
   Second are software mechanisms like TLB caching or packet filters. Third is
   downloading and executing code using type-safe languages, interpretation, or
   sandboxing. Exokernels can download Application-Specific Safe Handlers
   (ASHes).
2. *Visible resource revocation.* In traditional operating systems, resource
   revocation is made invisible to applications. For example, when an
   application's page is swapped to disk, it is not notified. The exokernel
   makes resource revocation visible by notifying the libOS. For example, each
   libOS is notified when its quantum is over. This allows it do things like
   only store the registers it needs.
3. *Abort protocol.* If a libOS is misbehaving and not responding to revocation
   requests, the exokernel can forcibly remove allocations. Naively, it could
   kill the libOS. Less naively, ti can simply remove all secure bindings.

The paper also presents the Aegis exokernel and the ExOS library operating
system.

## [SPIN -- An Extensible Microkernel for Application-specific Operating System Services (1995)](https://scholar.google.com/scholar?cluster=4910839957971330989&hl=en&as_sdt=0,5)
**Summary.**
Many operating systems were built a long time ago, and their performance was
tailored to the applications and workloads at the time. More recent
applications, like databases and multimedia applications, are quite different
than these applications and can perform quite poorly on existing operating
systems. SPIN is an extensible microkernel that allows applications to tailor
the operating system to meet their needs.

Existing operating systems fit into one of three categories:

1. They have no interface by which applications can modify kernel behavior.
2. They have a clean interface applications can use to modify kernel behavior
   but the implementation of the interface is inefficient.
3. They have an unconstrained interface that is efficiently implemented but
   does not provide isolation between applications.

SPIN provides applications a way to efficiently and safely modify the behavior
of the kernel. Programs in SPIN are divided into the user-level code and a
spindle: a portion of user code that is dynamically installed and run in the
kernel. The kernel provides a set of abstractions for physical and logical
resources, and the spindles are responsible for managing these resources. The
spindles can also register to be invoked when certain kernel events (i.e. page
faults) occur. Installing spindles directly into the kernel provides
efficiency. Applications can execute code in the kernel without the need for a
context switch.

To ensure safety, spindles are written in a typed object-oriented language.
Each spindle is like an object; it contains local state and a set of methods.
Some of these methods can be called by the application, and some are registered
as callbacks in the kernel. A spindle checker uses a combination of static
analysis and runtime checks to ensure that the spindles meet certain kernel
invariants. Moreover, SPIN relies on advanced compiler technology to ensure
efficient spindle compilation.

General purpose high-performance computing, parallel processing, multimedia
applications, databases, and information retrieval systems can benefit from the
application-specific services provided by SPIN. Using techniques such as

- extensible IPC;
- application-level protocol processing;
- fast, simple, communication;
- application-specific file systems and buffer cache management;
- user-level scheduling;
- optimistic transaction;
- real-time scheduling policies;
- application-specific virtual memory; and
- runtime systems with memory system feedback,

applications can be implemented more efficiently on SPIN than on traditional
operating systems.

## [The HP AutoRAID hierarchical storage system (1996)](https://goo.gl/x8Ps2t) ##
**Summary.**
The HP AutoRAID is a hierarchical storage system which caches write-active data
using RAID-1 and stores write-inactive data using RAID-5. The main idea is that
[RAID-1 has higher throughput and lower latency for random
writes](http://pages.cs.wisc.edu/~remzi/OSTEP/file-raid.pdf) but requires more
storage than RAID-5. The AutoRAID system essentially introduces a new level in
the memory hierarchy by using one RAID level (i.e. RAID-1) to cache another
(i.e. RAID-5).

The AutoRAID system implements the following features and functionality:

- Data is dynamically moved between the RAID-1 cache and the RAID-5 storage
  based on the workload.
- Devices can be installed into the system while it is running. For example, a
  new disk can plugged in to an HP AutoRAID device while it is being used. This
  allows for things like rolling disk upgrades.
- If a controller fails, another is ready as backup.
- Writes are written sequentially to the tail of the RAID-5 storage similar to
  a log structured file system.

An HP AutoRAID devices comprises the typical RAID hardware: microprocessors,
disks, volatile memory, non-volatile memory, parity modules, etc. The disks are
divided into (typically 1MB) *physical extents* (PEX) which are grouped into
*physical extent groups* (PEG). Moreover, each PEX is divided into *segments*:
the atomic units used for mirroring or striping.

The logical data model is based on 64 KB *relocation blocks* (RB) that are the
atomic unit of logical data movement. Each AutoRAID device contains multiple
levels of maps (i.e. virtual device table, PEG table, PEX table) to map logical
addresses to physical addresses.

The HP AutoRAID device caches reads and caches writes into NVRAM before issuing
them to the backend (which allows for things like *write coalescing*). If data
is being written and is in the RAID-1 cache, it is written there. Otherwise, it
is promoted from the RAID-5 storage and brought into the RAID-1 cache. Reads
and writes issued to the RAID-1 cache are trivial. Reads from the RAID-5
storage is also simple. Writes to the RAID-5 storage are always appended and
also batched to reduce the number of times parity bits that have to be read,
computed, and written.

Background operations are triggered when the disk is idle and include:

- *Compaction and hole plugging.* Holes in the RAID-1 cache are formed when
  blocks are demoted to RAID-5. Dually, holes in RAID-5 are formed when they
  are promoted to RAID-1 cache. RAID-1 data can be relocated to fill RAID-1
  holes. Similarly, RAID-5 PEGs with few holes can have their holes filled by
  other RAID-5 PEGs that have lots of holes.  Alternatively, RAID-5 PEGs with
  lots of holes can be appended to the RAID-5 storage and reclaimed.
- *Migration* Data is sometimes preemptively demoted from RAID-1 in
  anticipation of accommodating write bursts.
- *Workload Logging*. To create disk traces, I/O requests are logged.

**Commentary.**
- There are a lot of RAID levels and each one has different capacity,
  performance, and reliability. Moreover, performance depends heavily on
  workload. This paper presents the AutoRAID system without much explanation
  for when or why it's expected to perform better than alternatives. For
  example, is AutoRAID only better for random-write workloads? The
  microbenchmarks show that it performs well under sequential writes as well,
  but isn't RAID-1 supposed to perform poorly with sequential writes?
- The AutoRAID system is supposed to be easier to configure than traditional
  RAID devices. The authors argue that typical RAID configuration requires
  detailed knowledge of the expected workloads which can be difficult to come
  by. However, the AutoRAID system also depends on expected workload knowledge.
  The paper admits that the working write set has to be small and slowly
  changing. Later in the paper, the authors also say "it is fairly easy to
  predict or detect the environments that have a large write working-set and to
  avoid them if necessary"; a bit contradictory from earlier. Finally, the
  summary says "there are workloads that do not suit its algorithms well".
- I think the evaluation of the system makes it unclear whether the
  hierarchical storage or the NVRAM caching is responsible for the performance
  improvements.
- The AutoRAID system logs all I/O operations made to it in order to form disk
  traces. Implementing this functionality in the disk device is arguably a
  violation of the end-to-end principle and could have been implemented in the
  OS. However, they argue that the performance overhead is negligible.

## [Disco: Running Commodity Operating Systems on Scalable Multiprocessors (1997)](https://scholar.google.com/scholar?cluster=17298410582406300869&hl=en&as_sdt=0,5)
**Summary.**
Operating systems are complex, million line code bases. Multiprocessors were
becoming popular, but it was too difficult to modify existing commercial
operating systems to take full advantage of the new hardware. Disco is a
*virtual machine monitor*, or *hypervisor*, that uses virtualization to run
commercial virtual machines on cache-coherent NUMA multiprocessors. Guest
operating systems running on Disco are only slightly modified, yet are still
able to take advantage of the multiprocessor. Moreover, Disco offers all the
traditional benefits of a hypervisor (e.g. fault isolation).

Disco provides the following interfaces:

- *Processors.* Disco provides full virtualization of the CPU allowing for
  restricted direct execution. Some privileged registers are mapped to memory
  to allow guest operating systems to read them.
- *Physical memory.* Disco virtualizes the guest operating system's physical
  address spaces, mapping them to hardware addresses. It also supports page
  migration and replication to alleviate the non-uniformity of a NUMA machine.
- *I/O devices.* All I/O communication is simulated, and various virtual disks
  are used.

Disco is implemented as follows:

- *Virtual CPUs.* Disco maintains the equivalent of a process table entry for
  each guest operating system. Dicso runs in kernel mode, guest operating
  systems run in supervisor mode, and applications run in user mode.
- *Virtual physical memory.* To avoid the overhead of physical to hardware
  address translation, Disco maintains a large software physical to hardware
  TLB.
- *NUMA.* Disco migrates pages to the CPUs that access them frequently, and
  replicates read-only pages to the CPUs that read them frequently. This
  dynamic page migration and replications helps mask the non-uniformity of a
  NUMA machine.
- *Copy-on-write disks.* Disco can map physical addresses in different guest
  operating systems to a read-only page in hardware memory. This lowers the
  memory footprint of running multiple guest operating systems. The shared
  pages are copy-on-write.
- *Virtual network interfaces.* Disco runs a virtual subnet over which guests
  can communicate using standard communication protocols like NFS and TCP.
  Disco uses a similar copy-on-write trick as above to avoid copying data
  between guests.

## [T Spaces: The Next Wave (1999)](https://goo.gl/mxIv4g) ##
**Summary.**
T Spaces is a

> tuplespace-based network communication buffer with database capabilities that
> enables communication between applications and devices in a network of
> heterogeneous computers and operating systems

Essentially, it's Linda++; it implements a Linda tuplespace with a couple new
operators and transactions.

The paper begins with a history of related tuplespace based work. The notion of
a shared collaborative space originated from AI *blackboard systems* popular in
the 1970's, the most famous of which was the Hearsay-II system. Later, the
Stony Brook microcomputer Network (SBN), a cluster organized in a torus
topology, was developed at Stony Brook, and Linda was invented to program it.
Over time, the domain in which tuplespaces were popular shifted from parallel
programming to distributed programming, and a huge number of Linda-like systems
were implemented.

T Spaces is the marriage of tuplespaces, databases, and Java.

- *Tuplespaces* provide a flexible communication model;
- *databases* provide stability, durability, and advanced querying; and
- *Java* provides portability and flexibility.

T Spaces implements a Linda tuplespace with a few improvements:

- In addition to the traditional `Read`, `Write`, `Take`, `WaitToRead`, and
  `WaitToTake` operators, T Spaces also introduces a `Scan`/`ConsumingScan`
  operator to read/take all tuples matched by a query and a `Rhonda` operator
  to exchange tuples between processes.
- Users can also dynamically register new operators, the implementation of
  which takes of advantage of Java.
- Fields of tuples are indexed by name, and tuples can be queried by named
  value. For example, the query `(foo = 8)` returns *all* tuples (of any type)
  with a field `foo` equal to 8. These indexes are similar to the inversions
  implemented in Phase 0 of System R.
- Range queries are supported.
- To avoid storing large values inside of tuples, files URLs can instead be
  stored, and T Spaces transparently handles locating and transferring the file
  contents.
- T Spaces implements a group based ACL form of authorization.
- T Spaces supports transactions.

To evaluate the expressiveness and performance of T Spaces, the authors
implement a collaborative web-crawling application, a web-search information
delivery system, and a universal information appliance.

## [Generalized Isolation Level Definitions (2000)](TODO) ##
**Summary.**
In addition to serializability, ANSI SQL-92 defined a set of weaker isolation
levels that applications could use to improve performance at the cost of
consistency. The definitions were implementation-independent but ambiguous.
Berenson et al. proposed a revision of the isolation level definitions that was
unambiguous but specific to locking. Specifically, they define a set of
*phenomena*:

- P0: `w1(x) ... w2(x) ...`      *"dirty write"*
- P1: `w1(x) ... r2(x) ...`      *"dirty read"*
- P2: `r1(x) ... w2(x) ...`      *"unrepeatable read"*
- P3: `r1(P) ... w2(y in P) ...` *"phantom read"*

and define the isolation levels according to which phenomena they preclude.
This preclusion can be implemented by varying how long certain types of locks
are held:

| write locks | read locks | phantom locks | precluded      |
| ----------- | ---------- | ------------- | -------------- |
| short       | short      | short         | P0             |
| long        | short      | short         | P0, P1         |
| long        | long       | short         | P0, P1, P2     |
| long        | long       | long          | P0, P1, P2, P3 |

This locking-specific *preventative* approach to defining isolation levels,
while unambiguous, rules out many non-locking implementations of concurrency
control. Notably, it does not allow for multiversioning and does not allow
non-committed transactions to experience weaker consistency than committed
transactions. Moreover, many isolation levels are naturally expressed as
invariants between multiple objects, but these definitions are all over a
single object.

This paper introduces implementation-independent unambiguous isolation level
definitions. The definitions also include notions of predicates at all levels.
It does so by first introducing the definition of a *history* as a partial
order of read/write/commit/abort events and total order of commited object
versions.  It then introduces three dependencies: *read-dependencies*,
*anti-dependencies*, and *write-dependencies* (also known as write-read,
read-write, and write-write dependencies). Next, it describes how to construct
dependency graph and defines isolation levels as constraints on these graphs.

For example, the G0 phenomenon says that a dependency graph contains a
write-dependency cycle. PL-1 is the isolation level that precludes G0.
Similarly, the G1 phenomenon says that either

1. a committed transaction reads an aborted value,
2. a committed transaction reads an intermediate value, or
3. there is a write-read/write-write cycle.

The PL-2 isolation level precludes G1 (and therefore G0) and corresponds
roughly to the READ-COMMITTED isolation level.

## [The Click Modular Router (2000)](https://goo.gl/t1AlsN) ##
**Summary.**
Routers are more than routers. They are also firewalls, load balancers, address
translators, etc. Unfortunately, implementing these additional router
responsibilities is onerous; most routers are closed platforms with inflexible
designs. The Click router architecture, on the other hand, permits the creation
of highly modular and flexible routers with reasonable performance. Click is
implemented in C++ and compiles router specifications to routers which run on
general purpose machines.

Much like how Unix embraces modularity by composing simple *programs* using
*pipes*, the Click architecture organizes a network of *elements* connected by
*connections*. Each element represents an atomic unit of computation (e.g.
counting packets) and is implemented as a C++ object that points to the other
elements to which it is connected. Each element has input and output ports, can
be provided arguments as configuration strings, and can expose arbitrary
methods to other elements and to users.

Connections and ports are either *push-based* or *pull-based*. The source
element of a push connection pushes data to the destination element. For
example, when a network device receives a packet, it pushes it to other
elements. Dually, the destination element of a pull connection can request to
pull data from the source element or receive null if no such data is available.
For example, when a network device is ready to be written to, it may pull data
from other elements. Ports can be designated as pull, push, or agnostic. Pull
ports must be matched with other pull ports, push ports must be matched with
other push ports, and agnostic ports are labeled as pull or push ports during
router initialization. *Queues* are packet storing elements with a push input
port and pull output port; they form the interface between push and pull
connections.

Some elements can process packets with purely local information. Other elements
require knowledge of other elements. For example, a packet dropping element
placed before a queue might integrate the length of the queue in its packet
dropping policy. As a compromise between purely local information and complete
global information, Click provides *flow-based router context* to elements
allowing them to answer queries such as "what queue would a packet reach if I
sent it out of my second port?".

Click routers are specified using a simple declarative configuration language.
The language allows users to group elements into *compound elements* that
behave as a single element.

The Click kernel driver is a single kernel thread that processes elements on a
task queue. When an element is processed, it may in turn push data to or pull
data from other elements forcing them to be processed. To avoid interrupt and
device management overheads, the driver uses polling. Every input and output
device polls for data whenever it is run by the driver. Router configurations
are loaded and element methods are called via the `/proc` file system. The
driver also supports hot-swapping in new router configurations which take over
the state of the previous router.

The authors implement a fully compliant IP router using Click and explore
various extensions to it including scheduling and dropping packets. The
performance of the IP router is measured and analyzed.

## [Chord: A Scalable Peer-to-peer Lookup Service for Internet Applications (2001)](https://goo.gl/AOCTas) ##
**Summary.**
Chord is a peer-to-peer lookup service---today, we'd call it a distributed hash
table---mapping keys to servers that provides load balancing, decentralization,
scalability, availability, and flexible key naming. In a Chord cluster with `N`
nodes, each server stores O(log `N`) data, lookup takes O(log `N`) messages,
and node joining requires O(log `N` * log `N`) messages. Note that Chord maps
keys to *servers*, not *values*, but a key-value interface can easily be built
on top of Chord. In fact, Chord leaves all higher level functionality, like
caching and replication, to the user.

Chord hashes keys and servers (represented by their IP address) into the space
of `m`-bit bitstrings modulo `2^m`. The key `k` is managed by the first server
`n` greater than or equal to `k`. This server is known as the successor of `k`.
Intuitively, keys and servers are assigned to points on a circle of values
ranging from `0` to `2^m`. The successor of a point `k` is the first server
reached by rotating clockwise starting at `k`.

Each node `n` maintains `m` fingers. The `i`th finger is the successor of `n +
2^{i-1}`. For example, the first finger is the successor of `n + 1`, the second
finger is the successor of `n + 2`, the third finger is the successor of `n +
4`, etc. Note that the first finger is the successor of `n`. Under this scheme,
nodes only know about a small fraction of the nodes and cannot identify the
successor of an arbitrary key. In order to find the server assigned to a key
`k`, we follow fingers to the latest node that precedes `k`. Once `k` falls
between a node and its immediate successor, that successor is the successor of
`k`.

In order to handle the joining of a node, servers also maintain a pointer to
their predecessor. When a node `n` joins, it contacts an arbitrary Chord node
`n'` and performs the following procedure:

1. *Update the predecessor and fingers of `n`.* `n` computes its predecessor
   and fingers by simply asking `n'` to look them up. Naively, this requires
   `O(m * log N)` messages. As a small optimization, a node can avoid looking
   up the successor of `n + 2^i` if the successor of `n + 2^{i - 1}` is greater
   than `n + 2^i`. In this case the `i`th finger is the same as the `i-1`th
   finger. This requires `O(log N * log N)` operations. Other optimizations can
   reduce the message complexity to `O(log N)`.
2. *Update the predecessor and fingers of other nodes.* `n` is the `i`th finger
   of a node `p` if `p` precedes `n` by at least `2^i` and `n` precedes the
   `i`th finger of `p`. For `i = 1` to `i = m`, the predecessor `p` of `n -
   2^i` is found. If `n` precedes `p`'s `i`th finger, then its fingers are
   updated. If the fingers are updated, `p` is set to the predecessor of `p`
   and the process repeats.
3. *Notify the application to migrate data to `n`.* Applications are
   responsible for migrating data and therefore must be notified when a new
   node joins the system. Typically a node `n` will take data only from its
   successor.

In order to handle the concurrent joining and leaving of nodes, Chord
periodically runs a stabilization procedure that aims to maintain the
correctness of successor pointers. If a lookup is performed during
stabilization, one of three things can happen:

1. If the successors and fingers are correct, the lookup succeeds as usual.
2. If the successors are correct but the fingers are incorrect, the lookup
   succeeds but may take longer than usual.
3. If the successors are incorrect, the lookup may fail. In this case, the
   application may issue a retry.

When a node `n` joins, it contacts an existing Chord node `n'`. It asks `n'` to
find its successor. Periodically, servers check to see if their successor's
predecessor should be their successor, and they inform their successor about
their existence. They also periodically refresh fingers.

Note that the stabilization cannot handle certain perverse scenarios. For
example, if two disjoint rings form, stabilization will not merge the two
rings.

To handle node failures, nodes maintain `r` successors. When a successors node
fails, lookup is routed to the next of the `r` successors.

## [Inferring Internet Denial-of-Service Activity (2001)](TODO) ##
**Summary.**
This paper uses *backscatter analysis* to quantitatively analyze
denial-of-service attacks on the Internet. Most flooding denial-of-service
attacks involve IP spoofing, where each packet in an attack is given a faux IP
address drawn uniformly at random from the space of all IP addresses. If the
packet elicits the victim to issue a reply packet, then victims of
denial-of-service attacks end up sending unsolicited messages to servers
uniformly at random. By monitoring this *backscatter* at enough hosts, one can
infer the number, intensity, and type of denial-of-service attacks.

There are of course a number of assumptions upon which backscatter depends.

1. *Address uniformity*. It is assumed that DOS attackers spoof IP addresses
   uniformally at random.
2. *Reliable delivery*. It is assumed that packets, as part of the attack and
   response, are delivered reliably.
3. *Backscatter hypothesis*. It is assumed that unsolicited packets arriving at
   a host are actually part of backscatter.

The paper performs a backscatter analysis on 1/256 of the IPv4 address space.
They cluster the backscatter data using a *flow-based classification* to
measure individual attacks and using an *event-based classification* to measure
the intensity of attacks. The findings of the analysis are best summarized by
the paper.

## [SEDA: An Architecture for Well-Conditioned, Scalable Internet Services (2001)](https://goo.gl/wrn04s) ##
**Summary.**
Writing distributed Internet applications is difficult because they have to
serve a huge number of requests, and the number of requests is highly variable
and bursty. Moreover, the applications themselves are also complicated pieces
of code. This paper introduces the *staged event-driven architecture* (SEDA)
which has the following goals.

- *Massive concurrency.* Applications written using SEDA should be able to
  support a very large number of clients.
- *Simplify the construction of well-conditioned services.* A
  *well-conditioned* service is one that gracefully degrades. Throughput
  increases with the number of clients until it saturates at some threshold. At
  this point, throughput remains constant and latency increases proportionally
  with the number of clients. SEDA is designed to make writing well-conditioned
  services easy.
- *Enable introspection.* SEDA applications should be able to inspect and adapt
  to incoming request queues. Some request-per-thread architectures, for
  example, do not enable introspection. Control over thread scheduling is left
  completely to the OS; it cannot be adapted to the queue of incoming
  requests.
- *Self-tuning resource management.* SEDA programmers should not have to tune
  knobs themselves.


SEDA accomplishes these goals by structuring applications as a *network* of
*event-driven stages* connected by *explicit message queues* and managed by
*dynamic resource controllers*. That's a dense sentence, so let's elaborate.

Threading based concurrency models have scalability limitations due to the
overheads of context switching, poor caching, synchronization, etc. The
[event-driven concurrency
model](http://pages.cs.wisc.edu/~remzi/OSTEP/threads-events.pdf) involves
nothing more than a single-threaded loop that reads messages and processes
them. It avoids many of the scalability limitations that threading models face.
In SEDA, the atomic unit of execution is a *stage* and is implemented using an
event-driven concurrency model. Each stage has an input queue of messages which
are read in batches by a thread pool and processed by a user-provided event
handler which can in turn write messages to other stages.

A SEDA application is simply a network (i.e. graph) of interconnected stages.
Notably, the input queue of every stage is finite. This means that when one
stage tries to write data to the input queue of another stage, it may fail.
When this happens, stages have to block (i.e. pushback), start dropping
requests (i.e. load shedding), degrade service, deliver an error to the user,
etc.

To ensure SEDA applications are well-conditioned, various resource managers
tune SEDA application parameters to ensure consistent performance. For example,
the *thread pool controller* scales the number of threads within stages based
on the number of messages in its input queue. Similarly, the *batching
controller* adjusts the size of the batch delivered to each event handler.

The authors developed a SEDA prototype in Java dubbed Sandstorm. As with all
event-driven concurrency models, Sandstorm depends on asynchronous I/O
libraries. It implements asynchronous network I/O as three stages using
existing OS functionality (i.e. select/poll). It implements asynchronous file
I/O using a dynamically resizable thread pool that issues synchronous calls; OS
support for asynchronous file I/O was weak at the time. The authors evaluate
Sandstorm by implementing and evaluating an HTTP server and Gnutella router.

## [The Google File System (2003)](https://scholar.google.com/scholar?cluster=98210925508218371&hl=en&as_sdt=0,5)
**Overview.**
Today, Google is a huge company with a huge amount of data. But even in 2003,
when this paper was published, Google was still managing large amounts of data
and needed a distributed file system to store it all. This paper introduces the
*Google File System*: a distributed file system that differentiate itself from
other distributed file systems by taking advantage of Google's assumptions and
requirements:

- Failures are not uncommon; in fact, they are frequent.
- The file system stores a modest number of large, multi GB files.
- Most reads are large streaming reads along with a small number of random
  reads.
- Almost all writes are appends; there are seldom random writes.
- Bandwidth is prioritized over latency.

**Design.**
- *Interface.* GFS doesn't support a POSIX API, but it does support a standard
  interface of `read`, `write`, `open`, etc. It also supports atomic snapshot
  and a record append operation.
- *Architecture.* GFS is composed of a *single master*, multiple *chunk
  servers*, and multiple *clients*. Data is divided into 64 MB *chunks* and
  replicated on chunk servers as files. The master manages system metadata and
  assigns each chunk a unique *chunk handle*. Neither clients nor chunk servers
  cache data.
- *Single master.* Having a single master greatly simplifies the design of the
  design of the system. To avoid the master becoming a bottleneck, clients
  interact with it exclusively for metadata and interact with chunk servers
  exclusively for data. A client sends a filename and chunk index to the server
  which responds with a chunk handle and server locations. The client caches
  this metadata and then interacts directly with chunk servers.
- *Chunk size.* Chunks are 64 MB which is rather large compared to other file
  systems. The large chunk size has a number of advantages: fewer master
  interactions, persistent chunk server connections, and less metadata stored
  on the master. It also has some disadvantages: internal fragmentation
  (partially alleviated by lazy space allocation), and potential hot spots.
- *Metadata.* The master manages (a) file namespaces, (b) file to chunk
  mappings, and (c) chunk to location mappings. (a) and (b) are replicated and
  persisted in an oplog, while (c) is constructed by contacted chunk servers.
  The metadata typically fits in RAM on a single machine, and the price of
  buying more RAM is worth the simplicity a single master affords. The master
  also periodically checkpoints the oplog to avoid long recovery.
- *Consistency model.* All file namespace operations are atomic. File
  modification is more complex. We say a region of a file is *consistent* if
  all clients read the same value no matter which chunk server they contact. We
  say a file region is *defined*  if it is consistent and it reflects the most
  recent write. Serial random writes produce defined regions. Concurrent random
  writes produce consistent regions. A record append guarantees that data is
  appended at least once. Serial or random record appends produce regions of
  defined data interspersed with inconsistent data. If any write fails, the
  data is inconsistent. Applications can cope with repeated or inconsistent
  data by using checksums and unique identifiers.

**System Interactions.**
Each chunk is replicated to multiple chunk servers. One of the replicas is
granted a lease from the master and designated the *primary*. These leases last
something like 60 seconds but can be renewed by the primary. After a client
receives the chunk handle and location of the chunk it is trying to modify, it
streams data through the replicas in an order than minimizes distance between
the servers. Each chunk server buffers the data. The client then contacts the
primary and requests the data be written. The primary serializes the updates
and then contacts the replicas and relays their responses back to the client.

Record appends are performed almost identically. The only difference is that
the primary determines to which offset the data should be written.

Snapshots are performed using a copy-on-write technique. When a client issues a
snapshot of a file to the master, it first revokes all leases on the file. It
then modifies the file namespace to create the snapshot which points to the old
chunks. Whenever data is written to the file, the master first copies the
chunk.

**Master Operation.**
- *Namespace management and locking.* GFS does not maintain directory entries
  which contain a list of the files within it. Instead, GFS maintains a map
  from filepath to metadata in a prefix-compressed form. A tree locking scheme
  is used to allow the metadata to be concurrently updated.

- *Creation, re-replication, and rebalancing.* When a chunk is created, it has
  to be placed on a number of replicas. The master takes many factors into
  account when deciding which replicas to use. (1) The master tries to place
  chunks on servers with low disk utilization. (2) The master tries to place
  chunks on servers without many recent file creations since this implies there
  will be imminent writes.  (3) The master tries to place chunks across racks
  for improved fault tolerance. The master also re-replicates data when the
  number of replicas for a chunk is too low. It also rebalances chunks to
  balance load.
- *Garbage collection.* When a file is deleted, its chunks are not reclaimed.
  Instead, the file is renamed to a hidden filename and the file is kept around
  for a couple of days. The file can also be renamed to a normal file name to
  undo the deletion. Periodically, the master scans the namespace and deletes
  hidden files that are too old. It also periodically reclaims orphaned chunks.
  This form of garbage collection is necessary since the system can get into
  weird states due to failures.
- *Stale replica detection.* Chunks are given increasing version numbers by the
  master whenever a lease is granted. This allows the server to detect stale
  replicas.

**Fault Tolerance and Diagnosis.**
GFS achieves high availability by ensuring that masters and chunk servers can
recover from failures quickly. It also replicates the master and provides
read-only shadow masters that lag the real master. Data integrity is ensured
with checksums.

## [Xen and the Art of Virtualization (2003)](https://scholar.google.com/scholar?cluster=11605682627859750448&hl=en&as_sdt=0,5)
**Summary.**
Many virtual machine monitors, or *hypervisors*, aim to run unmodified guest
operating systems by presenting a completely virtual machine. This lets any OS
run on the hypervisor but comes with a significant performance penalty. Xen is
an x86 hypervisor that uses *paravirtualization* to reduce virtualization
overheads. Unlike with full virtualization, paravirtualization only virtualizes
some components of the underlying machine. This paravirtualization requires
modifications to the guest operating systems but not the applications running
on it. Essentially, Xen sacrifices the ability to run unmodified guest
operating systems for improved performance.

There are three components that need to be paravirtualized:

- *Memory management.* Software page tables and tagged page tables are easier
  to virtualize. Unfortunately, x86 has neither. Xen paravirtualizes the
  hardware accessible page tables leaving guest operating systems responsible
  for managing them. Page table modifications are checked by Xen.
- *CPU.* Xen takes advantage of x86's four privileges, called *rings*. Xen runs
  at ring 0 (the most privileged ring), the guest OS runs at ring 1, and the
  applications running in the guest operating systems run at ring 3.
- *Device I/O.* Guest operating systems communicate with Xen via bounded
  circular producer/consumer buffers. Xen communicates to guest operating
  systems using asynchronous notifications.

The Xen hypervisor implements mechanisms. Policy is delegated to a privileged
domain called dom0 that has accessed to privileges that other domains don't.

Finally, a look at some details about Xen:

- *Control transfer.* Guest operating systems request services from the
  hypervisor via *hypercalls*. Hypercalls are like system calls except they are
  between a guest operating system and a hypervisor rather than between an
  application and an operating system. Furthermore, each guest OS registers
  interrupt handlers with Xen. When an event occurs, Xen toggles a bitmask to
  indicate the type of event before invoking the registered handler.
- *Data transfer.* As mentioned earlier, data transfer is performed using a
  bounded circular buffer of I/O descriptors. Requests and responses are pushed
  on to the buffer. Requests can come out of order with respect to the
  requests. Moreover, requests and responses can be batched.
- *CPU Scheduling.* Xen uses the BVT scheduling algorithm.
- *Time and timers.* Xen supports real time (the time in nanoseconds from
  machine boot), virtual time (time that only increases when a domain is
  executing), and clock time (an offset added to the real time).
- *Virtual address translation.* Other hypervisors present a virtual contiguous
  physical address space on top of the actual hardware address space. The
  hypervisor maintains a shadow page table mapping physical addresses to
  hardware addresses and installs real page tables into the MMU. This has high
  overhead. Xen takes an alternate approach. Guest operating systems issue
  hypercalls to manage page table entries that are directly inserted into the
  MMU's page table. After they are installed, the page table entries are
  read-only.
- *Physical memory.* Memory is physically partitioned into reservations.
- *Network.* Xen provides virtual firewall-routers with one or more virtual
  network interfaces, each with a circular ring buffer.
- *Disk.* Xen presents virtual block devices each with a ring buffer.

## [MapReduce: Simplified Data Processing on Large Clusters (2004)](https://scholar.google.com/scholar?cluster=10940266603640308767&hl=en&as_sdt=0,5)
In order to be robust and efficient, programs that process huge amounts of data
have to take into account how to parallelize work, distribute work, handle
failures, and load balance work. The MapReduce framework implements these
complicated pieces of boilerplate, allowing programmers to process huge amounts
of data only having to write simple map and reduce functions.

**Programming Model.**
Logically, users implement map and reduce functions of the following type:

- `map: (k1, v1) -> (k2, v2) list`
- `reduce: (k2, v2 list) -> v2 list`

though in reality, the MapReduce framework deals only with strings. In addition
to providing map and reduce functions, the user also provides the name of
inputs, the name of outputs, tuning parameters, etc. The MapReduce framework
is expressive enough to implement distributed grep, URL access counts, reverse
web-link graph, inverted index, and distributed sort.

**Implementation.**
The MapReduce interface can be implemented in many different ways. For example,
a simple single-threaded implementation could be used for debugging, or a NUMA
multi-processor implementation could be used for datasets that fit in memory.
Here, we discuss an implementation for a cluster of commodity machines.
Execution proceeds in a number of steps:

1. The input data is split into `M` 16-64 MB partitions. A master and a
   collection of workers are spawned.
2. The single master assigns the `M` map tasks and `R` reduce tasks to workers.
3. A mapper reads in its assigned partitions and applies the user provided map
   function to them. The intermediate key-value pairs are buffered in memory.
4. Periodically, a mapper writes intermediate data to a local disk and sends
   the location of the files to the master who forwards them to the reducers.
5. Reduces read the data written by the mappers using RPC when prompted to do
   so by the master. The reducer then (externally) sorts the data by
   intermediate key.
6. The reducer applies the user provided reduce function creating one output
   file for each of the `R` output partitions.
7. Finally, the user program is awoken.

- *Master data structure.* For each task, the master records the status of the
  task (idle, in-progress, completed) and the worker assigned to the task.
  Moreover, for each map task, it records the location of the output data.
- *Worker fault tolerance.* Masters detect worker failure via heartbeats. If a
  worker fails, the master reassigns all the map tasks assigned to it, even the
  ones that have completed. We have to redo completed map tasks because the
  intermediate data is stored locally on the machine and therefore may be
  unavailable. Workers are also notified of the failure and informed to read
  from the new mapper.
- *Master fault tolerance.* The current implementation has no fault tolerance.
  Simple master checkpointing could be implemented.
- *Failure semantics.* If map and reduce functions are deterministic, then the
  computation is deterministic.
- *Locality.* The master attempts to schedule map tasks on the same machine
  where the data is located.
- *Task granularity.* Increasing `M` and `R` improves load balancing and
  decreases the time it takes for work to be redone after a worker failure.
  However, if there are too many tasks, the master becomes a memory bottleneck.
- *Backup tasks.* Near the end of the computation, the master begins to
  reassign incomplete work to mitigate stragglers.

**Refinements.**
- *Partition function.* Users can specify custom partition functions to replace
  the default (i.e. `hash(k) % R`). This is useful if, for example, users want
  to place logically grouped keys on the same reducer.
- *Combiner function.* Users can specify a reduce function, known as a
  combiner, to run at the mapper.
- *Input/output types.* There are a number of default input types (e.g. text,
  SSTable) and users can implement their own as well.
- *Skipping bad records.* Sometimes a bug in the code causes a map or reduce
  function to deterministically crash. The MapReduce framework will inform
  workers to skip this task. Each worker registers a signal handler which sends
  a last-breathe UDP message to the master informing of the record which caused
  the crash. If the master sees multiple failures caused by the same record, it
  tells workers not to process it.
- *Local execution.* The MapReduce framework supports a local execution to
  allow for easier debugging.
- *Status information.* The master exports an HTTP server which includes a
  bunch of useful statistics and metadata.
- *Counters.* The MapReduce framework also supports global counters which can
  be incremented and decremented by workers. The increments and decrements are
  aggregated by the master and are displayed on the master's HTTP server.

## [Analysis and Evolution of Journaling File Systems (2005)](TODO) ##
**Summary.**
The authors develop and apply two file system analysis techniques dubbed
*Semantic Block-Level Analysis* (SBA) and *Semantice Trace Playback* (STP) to
four journaled file systems: ext3, ReiserFS, JFS, and NTFS.

- Benchmarking a file system can tell you for *which* workloads it is fast and
  for which it is slow. But, these benchmarks don't tell you *why* the file
  system performs the way it does. By leveraging semantic information about
  block traces, SBA aims to identify the cause of file system behavior.

  Users install an SBA driver into the OS and mount the file system of interest
  on to the SBA driver. The interposed driver intercepts and logs all
  block-level requests and responses to and from the disk. Moreover, the SBA
  driver is specialized to each file system under consideration so that it can
  interpret each block operation, categorizing it as a read/write to a journal
  block or regular data block. Implementing an SBA driver is easy to do,
  guarantees that no operation goes unlogged, and has low overhead.
- Deciding the effectiveness of new file system policies is onerous. For
  example, to evaluate a new journaling scheme, you would traditionally have to
  implement the new scheme and evaluate it on a set of benchmarks. If it
  performs well, you keep the changes; otherwise, you throw them away. STP uses
  block traces to perform a light-weight simulation to analyze new file system
  policies without implementation overhead.

  STP is a user-level process that reads in block traces produced by SBA and
  file system operation logs and issues direct I/O requests to the disk. It can
  then be used to evaluate small, simple modifications to existing file
  systems. For example, it can be used to evaluate the effects of moving the
  journal from the beginning of the file system to the middle of the file
  system.

The authors spend the majority of the paper examining *ext3*: the third
extended file system. ext3 introduces journaling to ext2, and ext2 resembles
the [Unix FFS](#a-fast-file-system-for-unix-1984) with partitions divided into
groups each of which contains bitmaps, inodes, and regular data. ext3 comes
with three journaling modes:

1. Using *writeback* journaling, metadata is journaled and data is
   asynchronously written to disk. This has the weakest consistency guarantees.
2. Using *ordered* journaling, data is written to disk before its associated
   metatada is journaled.
3. Using *data* journaling, both data and metadata is journaled before being
   *checkpointed*: copied from the journal to the disk.

Moreover, operations are grouped into *compound transactions* and issued in
batch. ext3 SBA analysis led to the following conclusions:

- The fastest journaling mode depends heavily on the workload.
- Compound transactions can lead to *tangled synchrony* in which asynchronous
  operations are made synchronous when placed in a transaction with synchronous
  operations.
- In ordered journaling, ext3 doesn't concurrently write to the journal and the
  disk.

SPT was also used to analyze the effects of

- Journal position in the disk.
- An adaptive journaling mode that dynamically chooses between ordered and data
  journaling.
- Untangling compound transactions.
- Data journaling in which diffs are journaled instead of whole blocks.

SBA and STP was also applied to ReiserFS, JFS, and NTFS.

## [Live Migration of Virtual Machines (2005)](https://scholar.google.com/scholar?cluster=7787668674412123324&hl=en&as_sdt=0,5)
**Summary.**
Virtual machines have become increasingly popular in cloud and cluster
environments. In these environments, VM migration---the process in which a
virtual machine running on one physical machine is migrated to run on another
physical machine---is incredibly useful. For example, it allows cluster
administrators to migrate VMs off of a machine before decommisioning it for
repair. In this paper, the authors present their live migration scheme that
allows them to migrate Xen virtual machines with a very small amount of
downtime.

A key step in migrating a virtual machine is migrating its memory. There are
typically three phases of memory migration:

1. *Push*. In the push phase, the migrating VM is up and running while pages of
   memory are pushed to the new physical machine.
2. *Stop-and-copy*. In the stop-and-copy phase, the VM is stopped, its state is
   transfered along with any pages that weren't pushed, and the VM is then
   started on the new machine.
3. *Pull*. In the pull phase, the VM is running on the new machine and pages
   from the old machine are faulted over.

A pure stop-and-copy approach is simple, but has down times proportional to the
number of memory. A pure demand-migration scheme has small downtime but makes
VMs slow until enough pages have been shipped over. This paper uses an
iterative pre-copy scheme:

0. *Pre-migration.* Before migration, assume a VM is running on host A.
1. *Reservation.* Resources are reserved on host B.
2. *Iterative pre-copy.* Memory from host A is iteratively copied to host B.
   The pages copied in one round are the pages that were dirtied since the last
   round.
3. *Stop-and-copy.* The VM is stopped on host A and the remaining state and
   memory is sent to host B.
4. *Commitment.* Host B confirms that it has received everything correctly and
   tells host A it is free to destroy itself.
5. *Activation.* Host B starts the VM.

The migration scheme also performs some ARP tricks to transfer the IP address
of the machines. Note that local disk copying is not included in this paper.

Imagine a VM that never writes any memory. Then a single iteration of
pre-copying is sufficient. Alternatively, assume a VM dirties every page of
memory at a rate faster than pre-copying. Then, pre-copying could carry on
indefinitely. Most programs lie within these two extremes. The amount of memory
they quickly write is dubbed the *writable working set* (WWS). The paper
performs some experiments showing the size of the WWS for various benchmarks.

There are two ways to implement VM migration:

1. *Managed Migration.* In managed migration, VM migration is managed by
   migration daemons running in dom0 of each machine. Xen inserts a shadow page
   table underneath the operating system. It marks all the pages in the OS's
   page table as read-only. Then, when an OS goes to write the page, the
   machine traps into Xen which marks the page as writeable and notes that the
   page was dirtied. After each round of pre-copying, the shadow page table and
   dirty bits are reset. When pre-copying is done, Xen notifies the OS to get
   ready to be migrated, then ships over the last bits of state.
2. *Self Migration.* In self migration, each OS is responsible for migrating
   itself. Iterative pre-copy is performed as before. Now, stop-and-copy is
   performed in two phases to allow an OS to stop itself while still running.

A higher transfer bandwidth reduces the downtime of migration but interferes
with active services running on the machines. Conversely, a lower transfer
bandwidth increases the downtime of migration but does not interfere as much
with active services. This paper uses a dynamic rate limiting scheme. An
administrator sets a lower and upper bandwidth limit. Transfer begins at the
lower limit. The next iteration uses the estimated page dirtying rate from the
last round plus a constant. This process terminates when the upper limit is
reached or there is not much memory left to transfer. Moreover, pages that are
repeatedly dirtied are not pre-copied.

Migration can also take advantage of paravirtualization. OSes can sleep
processes that write a lot, and they can report which pages are free to Xen to
limit a full memory scan.

## [The Chubby lock service for loosely-coupled distributed systems (2006)](https://scholar.google.com/scholar?cluster=14281901705280588674&hl=en&as_sdt=0,5)
Chubby is Google's distributed lock service, implemented using consensus,
that allows clients to synchronize and agree on values. Chubby is like a file
system in which clients read and write entire files, but it also supports
advisory locks, event notification and more. Chubby's main goals are
reliability, availability, and easy-to-understand semantics. It's secondary
goals are throughput and storage capacity.

**Rationale.**
Chubby is a centralized lock service that internally uses Paxos. There are a
number of advantages to implementing a lock service rather than providing a
Paxos library that clients can use directly:

1. Programs are often not written like state machines which makes it difficult
   to integrate something like Paxos. It is significantly easier to use change
   code to use a lock service.
2. Often, when Chubby is used for leader election, the elected leader has to
   advertise its address. Thus, it makes sense to add data serving
   functionality into the lock service.
3. Locks are more familiar to programmers than state machines.
4. To be fault tolerant and available, consensus algorithms require a minimum
   number of servers. By using a lock service, the need for multiple servers is
   centralized. Clients don't have to run their own clusters.

Moreover, there are a number of other features that are useful to have in
Chubby including event notification, consistent file caching, and security

Also note that Chubby is designed for coarse, as opposed to fine, grained
locking. That is to say, clients hold locks for minutes to hours instead of
seconds or less. This reduces the load on Chubby and makes clients less
susceptible to Chubby crashes. Finer grained locking can also be implemented on
top of Chubby.

**System Structure.**
Chubby consists of a client library linked into applications and a small
cluster (typically 5) of servers called *replicas*. Replicas use consensus to
elect a master for the duration of a master lease which can be renewed by the
master. The master handles reads and writes which are copied to the replicas
using consensus. DNS is used to find replicas, and replicas forward clients to
the current master. If a replica is down for too long (e.g. multiple hours),
then it is replaced, the DNS entries are updated to point at the new server,
the new server receives data from backups and other replicas, and finally the
master introduces it into the cluster.

**Files, directories, and handles.**
Files in Chubby are identified by a path `ls/cell/dir1/dir2/.../dirn/filename`
where `ls` (lock service) identifies a Chubby file, `cell` is the name of a
cell, and the rest of the path is a usual path. This naming structure
integrates well into Google's existing file libraries and tools. Chubby makes a
couple of simplifying assumptions about files:

- Files cannot be moved between directories.
- Directories do not maintain last modified times.
- Permissions are per-file and do not depend on parents.
- There are no soft or hard links; everything in the file tree is a *node*.

Files can act as advisory locks, and file ACLs are themselves stored as files.
Moreover, files can be *permanent* or *ephemeral*. An ephemeral file is deleted
when no clients have it open; an ephemeral directory is deleted when it is
empty.

Each node maintains four numbers which increase over time:

1. An instance number which is incremented every time a file with a given name
   is created.
2. A content generation number which is incremented every time a file changes.
3. A lock generation number, which is incremented whenever a lock is acquired.
4. An ACL generation number which is incremented whenever ACLs are written.

Files also include 64 bit checksums.

**Locks and sequencers.**
Using distributed locks can be tricky, especially when messages are delayed and
locks are rapidly released and acquired. Each file in Chubby can act as an
advisory reader-writer lock. Lock owners may request a sequencer which is an
opaque byte-string with a name, mode, lock generation number etc. A client
passes the sequencer to a server, like a file server, which can check the
validity of the sequencer with Chubby. There is also a hackier way to prevent
anomalies in which locks cannot be re-acquired since the last release until a
*lock-delay* has passed.

**Events.**
Chubby clients can subscribe to events including:

- file changes,
- children nodes are added, deleted, or changed,
- master fail-over, and
- a lock is acquired.

**API.**
The Chubby API includes `Open`, `Close`, `Poison`, `GetContentsAndStat`,
`GetStat`, `ReadDir`, `SetContents` (with compare-and-swap semantics if need
be), `Delete`, `Acquire`, `TryAcquire`, `Release`, `GetSequencer`,
`SetSequencer`, and `CheckSequencer` calls.

**Caching.**
To reduce read load on Chubby, clients maintain a consistent write-through
cache. The master tracks what each client has cached and issues leases and
invalidations. Chubby uses a simple algorithm to keep caches consistent: when a
client issues a write, the server sends invalidations to all clients who have
the file cached. After all client caches have been invalidated, either by an
acknowledgement or a lease expiration, the write is written.

**Sessions and keep-alives.**
Chubby sessions between clients and servers are maintained by periodic
KeepAlive RPCs. Each session has a lease timeout before which the session is
guaranteed not to expire, and leases are periodically renewed in three
situations:

1. session creation,
2. master failover, and
3. KeepAlive delivery.

Masters do not respond to KeepAlive RPCs immediately. Instead, they wait until
just before the lease expires before responding. They also sent cache
invalidations and event notifications as part of the RPC. Clients respond to
KeepAlive RPCs immediately. Clients also conservatively estimate their lease
timeouts with some assumptions on clock skew and packet delivery times. When a
session expires, clients enter a 45 second grace period where they drop their
cache. If they reconnect to a master, things continue as usual. Otherwise, they
deliver an error to the user.

**Fail-over.**
If a master fails over, the client may reconnect to the new master during its
grace period. The new master follows the following procedure:

1. Generates a new epoch number that is larger than all previous epoch numbers.
2. Responds to master location requests.
3. Builds its in-memory state from the database.
4. Responds to KeepAlive RPCs.
5. Delivers fail-over events to clients.
6. Waits for all existing sessions to expire or for fail-over messages to be
   acked.
7. Responds to all operations.
8. Recreates old handles when necessary.
9. Deletes ephemeral files after some delay.

**DB.**
Originally, Chubby used BerkeleyDB (BDB), but BDB's replication support was
young. Moreover, Chubby didn't need all of the functionality. Ultimately,
Chubby threw out BDB and implemented a persistence layer itself.

**Backup.**
Periodically, Chubby masters back up their state in another cell in GFS.

**Mirroring.**
Chubby's event notifications make it easy to mirror directories.

## [Architecture of a Database System (2007)](https://scholar.google.com/scholar?cluster=11466590537214723805&hl=en&as_sdt=0,5) ##
**Chapter 1 -- Introduction.**
Database textbooks often focus on data structures and algorithms in the context
of a single database component. This paper, as opposed most database textbooks,
focuses instead on *database architecture*: the design and best practices of
modern databases that are otherwise undocumented or exist only as tribal
knowledge.

Modern relational database management systems (RDBMS) comprise five components,
most of which are discussed in detail in this paper:

1. The *client communications manager* is responsible for interfacing with
   client connections.
2. The *process manager* is responsible allocating workers to requests using
   some combination of OS processes, OS threads, and user-level threads. It is
   also responsible for *admission control*: the process by which request
   processing is delayed until sufficient resources are available.
3. The *relational query processor* is responsible for translating a SQL query
   into an optimal query plan. It is also responsible for authorization.
4. The *transactional storage manager* implements the data structures and
   algorithms to store and read data from disk. It is also responsible for
   managing concurrent transactions. It includes the buffer manager, the lock
   manager, and the log manager.
5. Finally, there are miscellaneous *shared components and utilities*.

**Chapter 2 -- Process Models.**
Database management systems have to handle multiple user requests concurrently.
The process manager is responsible for mapping logical DBMS workers, which
handle a DBMS client requests, to OS processes, OS threads, user-level threads,
or some combination of the three. To simplify matters, this chapter discusses
process models only for unikernels. There are three main process models:

1. *Process per DBMS worker*. In this model, a new process is spawned for every
   request. This method is easy to implement and provides a small amount of
   isolation (e.g. a memory overflow in one process won't crash another
   process). However, this model is complicated by shared in-memory data
   structures such as the buffer pool and lock table. These are traditionally
   shared through OS supported shared memory. Moreover, context switching
   between processes is more expensive than context switching between threads.
   IBM DB2, PostgreSQL, and Oracle use this process model.
2. *Thread per DBMS worker*. In this model, a new thread (OS or user-level) is
   spawned for every request. This model can handle more requests than the
   previous model, and shared in-memory data structures (e.g. buffer pool, log
   tail) can simply reside in the heap. However, it is less portable and harder
   to implement and debug.  IBM DB2, Microsoft SQL Server, MySQL, Informix, and
   Sybase use this process model.
3. *Process pool*. In this model, requests are dispatched to a fixed number of
   processes; a new process is not spawned for every request. This approach has
   all the benefits of the process per DBMS worker approach but with much less
   overhead.

The process manager is also responsible for admission control: the process by
which a request is not serviced until sufficient resources are available.
Without admission control, a database can start thrashing; for example, imagine
a situation in which the working set of the database is larger than the buffer
pool, and all I/Os become cache misses. Admission control provides *graceful
degradation*; throughput should not decrease, and latency should scale
proportionally with the number of requests. Typically a two-tier admission
control policy is used:

- First, the client communication manager limits the number of concurrently
  open connections.
- Second, the execution admission controller runs after a query has been
  planned and delays execution until there are enough resources available to
  satisfy the query optimizer's estimated

    - disk access and number of I/Os,
    - CPU load, and
    - memory footprint.

The memory footprint is particularly important because it is most commonly the
cause of thrashing.

**Chapter 3 -- Parallel Architecture: Processes and Memory Coordination.**
Parallel hardware is ubiquitous. This chapter builds off the previous and
explores process models for database systems with multiple cores and multiple
machines.

1. *Shared Memory.* In a shared memory model, all processors can access shared
   RAM and disk with roughly the same performance. All three of the process
   models presented in the last chapter (i.e. process-per-worker,
   thread-per-worker, and process-pool/thread-pool) work well in a shared
   memory model. The OS transparently schedules processes and threads across
   the processors taking advantage of the parallel hardware without much
   effort. It is much more difficult to modify the query evaluator to take
   advantage of the multiple processors. Also, systems employing user-level
   threading must implement thread migration to take full advantage of multiple
   cores.
2. *Shared-Nothing.* A shared-nothing system is a networked cluster of
   independent machines that share, well, nothing. In a shared-nothing system,
   all coordination and communication is left to the DBMS. Typically, tables
   are *horizontally partitioned* between machines. That is, each machine is
   assigned a subset of the tuples in each table using range partitioning, hash
   partitioning, round-robin partitioning, or some sort of hybrid partitioning.
   Each machine uses a shared memory model and receives queries, creates query
   plans, and execute queries as usual. The big difference is that queries are
   now evaluated on multiple machines at once, and the DBMS has to orchestrate
   the exchange of control and data messages. The database also has to
   implement very challenging distributed protocols like distributed deadlock
   detection and two-phase commit. Worse, by virtue of being a distributed
   system, shared-nothing architectures can experience *partial failure* which
   can be handled in one of many ways.

    1. Every machine can be stopped whenever any machine fails. This makes a
       shared-nothing architecture as fault-tolerant as a shared-memory
       architecture.
    2. Some database systems, like Informix, simply skip over data hosted by a
       failed machine. This has weak and unpredictable semantics.
    3. Data can be replicated to tolerate failures. For example, a full
       database backup could be maintained, or more sophisticated techniques
       like [chained
       declustering](https://scholar.google.com/scholar?cluster=10345968159835311656&hl=en&as_sdt=0,5)
       could be employed.

   Despite the complexities that arise from a shared-nothing architecture, they
   achieve unparalleled scalability.
3. *Shared Disk.* In a shared disk system, processors all have access to a
   shared disk with roughly equal performance; they do not share RAM. Shared
   disk systems are much less complicated than shared-nothing systems because
   the failure of any machine does not lead to data unavailability. Still,
   shared disk systems require explicit coordination for in-memory data sharing
   including distributed lock managers, distributed buffer pools, etc.
4. *NUMA.* NUMA systems provide a shared memory model over a cluster of
   independent shared-nothing machines. NUMA clusters are dead, but the idea of
   non-uniform memory access lives on in shared-memory multiprocessors. In
   order to scale to a large number of cores, shared-memory NUMA processors
   organize CPUs and memories into pods where intra-pod memory access is fast
   but inter-pod memory access is slow. Databases may be able to ignore the
   non-uniformity of a NUMA multiprocessor, or they can employ certain
   optimizations:

    - Memory should always be local to a processor.
    - Tasks should be scheduled on the same processor it was previously run.

Almost all databases support shared memory systems, and most support either
shared-disk or shared-nothing architectures as well.

**Chapter 4 -- Relational Query Processor.**
The *relational query processor* is responsible for converting a textual query
into an optimized dataflow query plan that's ready to be executed.

The first step in processing a query is that of *query parsing and
authorization*. The query parser must check the syntactic well-formedness of a
textual query, convert the text into an internal query representation, type
check the query by resolving table and column references against information in
the catalog, and perform any necessary authorization. Certain forms of
authorization must be deferred to query execution. For example, a database may
restrict a user's access to tuples from a table that satisfy a certain
predicate. This *row-level security* depends on the values of the tuples and
must be deferred to query execution. In fact, some authorization which *could*
be performed during query parsing is deferred anyway.  For example, deferring
authorization checks to execution time allows for queries to be cached and
reused between multiple clients with varying privileges.

Next, a query processor performs *query rewrites*: logical transformations that
*simplify* and *normalize* a query without altering its semantics. Query
rewrites include:

- *View expansion.* View references in a query must be iteratively unwrapped
  until the final query includes only base table references.
- *Constant folding.* Expressions like `1 + R.a + 2 > 3` can be simplified to
  `R.a > 0`.
- *Logical predicate rewrites.* A query processor can sometimes deduce that a
  collection of predicates is unsatisfiable (e.g. `R.a < 0 AND R.a > 10`).
  Unsatisfiable predicates can be replaced with `FALSE` which enable further
  simplifications and optimizations. In some distributed databases that
  horizontally partition tables, predicates can be used to reduce the number of
  servers that are contacted. For example, if a server is responsible for a
  partition of a table `R` for all tuples where `0 < R.a < 100`, then it need
  not be contacted for a query like `SELECT R.a FROM R WHERE R.a > 10000`.
  Finally, certain *transitive predicates* can be deduced. For example, the
  predicates `R.a = S.b AND S.b = 100` imply `R.a = 100`.
- *Semantic optimization.* Using semantic information from the database catalog
  can be used to further simplify queries. For example, consider an `Employee`
  relation that has a foreign key into a `Department` relation. With this
  information, the query

        SELECT E.name
        FROM Emp E, Department D
        WHERE E.deptno = D.deptno

  can be simplified to

        SELECT E.name
        FROM Emp E
- *Subquery flattening.* Query optimizers are so complicated that they often
  narrow their scope to operate only on SELECT-FROM-WHERE blocks. To enable as
  many optimizations as possible, queries are often canonicalized and
  subqueries are flattened when possible.

Finally, a query is *optimized*. System R compiled queries into executable
code. Later, System R developers regarded this as a mistake. Ingres compiled
queries into interpretable dataflow diagrams. Later, Ingres developers somewhat
ironically also regarded this as a mistake. Both compilation targets have their
merits, but most modern databases have gone the way of Ingres to ensure
portability. Typically, this involves optimizing individual SELECT-FROM-WHERE
blocks into relational operator trees before stitching them all together.
Optimizations involve:

- *Plan space.* System R only considered left-deep query plans and deferred all
  Cartesian products to the top of the plan. Modern databases sometimes
  consider bushier trees with Cartesian products lower in the tree.
- *Selectivity estimation.* System R's selectivity estimation was based solely
  on relation and index cardinalities and is considered naive by today's
  standards. Modern databases summarize data distributions using histograms and
  other sketching data structures and use sampling to avoid expensive
  statistics computations. Moreover, algorithms like histogram joining improve
  selectivity estimation for joins.
- *Search algorithms.* In addition to System R's dynamic programming bottom-up
  approach, other databases have explored top-down approaches. Both have proven
  successful.
- *Parallelism.* In addition to inter-query parallelism, databases often
  implement intra-query parallelism. In a *two-phase approach*, the best
  single-node query plan is formed in one phase and then parallelized in a
  second phase. In a *one-phase approach*, the optimizer takes cluster
  information into account during optimization to try and find an optimal
  distributed plan. It's questionable whether the performance benefits of a
  two-phase plan warrant its complexity.
- *Auto-tuning.* Some databases use query traces to automatically tune the
  databases by, for example, suggesting new indexes to include.

Query optimizers also have to deal with query caching and recompilation. Many
databases allow for queries to be parsed, compiled, and stored ahead of time.
These *prepared* queries can also include placeholders that are filled in at
runtime. Prepared statements occasionally need to be re-optimized when, for
example, an index is dropped. Certain databases avoid re-optimization to ensure
predictability over performance; others aggressively re-optimize to ensure the
best performance.  Prepared queries can improve the performance of an
application, but preparing queries ahead of time can be burdensome.
Consequently, databases also support query caching to reuse (parts of) queries
without necessitating ahead-of-time preparation.

Once a query is parsed, rewritten, and optimized into a dataflow plan, it must
be executed. Typically, query plans are implemented as a tree of iterators with
*exchange nodes* thrown in for parallelism. These iterators typically operate
over *tuple references*: tuples of tuple pointers and column offsets. The
actual tuple data is either stored in the buffer pool (BP-tuples) or copied
from the buffer pool into the heap (M-tuples). Using BP-tuples avoids copies
but is hard to implement correctly and may lead to a page being pinned in the
buffer pool for prohibitively long. Using M-tuples can lead to unnecessary
copies but is much simpler to implement.

Data modification statements (e.g. INSERT, UPDATE, etc) are typically compiled
into simple linear dataflow diagrams. However, care must be taken to avoid
things like the *Halloween problem* in which updates invalidate the index
iterators used to perform the updates.

**Chapter 5 -- Storage Management.**
TODO

**Chapter 6 -- Transactions: Concurrency Control and Recovery.**
TODO

**Chapter 7 -- Shared Components.**
TODO

## [Dryad: Distributed Data-Parallel Programs from Sequential Building Blocks (2007)](https://scholar.google.com/scholar?cluster=4381257520443107986&hl=en&as_sdt=0,5)
Parallel and distributed programming is hard. Dryad is a programming framework,
inspired by parallel databases and MapReduce, which makes distributed
data-parallel program easy by abstracting resource allocation, scheduling,
failure handling, straggler mitigation, etc. In Dryad, computation is expressed
as a directed acyclic graph. The vertexes run user code. The edges, also known
as channels, send data in some form (e.g TCP, files). Users write vertexes and
build graphs using a simple graph description language.

**System Overview.**
Logically, Dryad computations are expressed as directed acyclic graphs of
vertexes (code) and edges (transport). Vertexes can send arbitrary data across
edges; they are responsible for handling data appropriately. Physically, Dryad
execution is orchestrated by a single *Job Manager* (JM) which can run on a
user's machine or in a cluster. The JM contacts a *Name Service* (NS) to get
the addresses and locations of a cluster of workers. Each worker runs a daemon
which coordinates with the JM. The JM, NS, and workers can also be run on a
programmer's machine for debugging. The whole system uses a distributed file
system like GFS.

As an example, consider the following SQL query over a database of celestial
objects. The query computes the set of planets with a similarly colored
neighbor. A RDBMS can execute this query as a pair of index-only joins: one
index on photoObjAll and one on neighbors.

```
SELECT DISTINCT p.objID
FROM photoObjAll p
JOIN neighbors n
ON p.objID = n.objID
    AND n.objID < n.neighborObjID
    AND p.mode = 1
JOIN photoObjAll l
ON l.objID = n.neighborObjID
    AND l.mode = 1
    AND abs((p.u - p.g) - (l.u - l.g)) < 0.05
    AND abs((p.g - p.r) - (l.g - l.r)) < 0.05
    AND abs((p.r - p.i) - (l.r - l.i)) < 0.05
    AND abs((p.i - p.z) - (l.i - l.z)) < 0.05
```

In Dryad, we can represent it as a graph where:

1. Partition the index into pairs `U1, N1, ..., Un, Nn`.
2. One node reads each partition and computes a local join.
3. The data is shuffled and partitioned based on neighborObjID.
4. The data is sorted on neighborObjID.
5. The data is joined with photoObjAll to get the color of the neighbors and
   filtered.
6. The data is sent to a single node for distinct.

**Describing a Dryad Graph.**
Dryad uses a C++ DSL to merge subgraphs into bigger graphs. A graph is a
four-tuple `G = (V_G, E_G, I_G, E_G)` where

- `V_G` is a vector of vertexes,
- `E_G` is a set of edges over `V_G`,
- `I_G` is a subset of `V_G`: the input vertexes, and
- `O_G` is a subset of `V_G`: the output vertexes.

Edges are ordered and multiple edges can exist between a pair of vertexes. Here
are the operation we can use to build or compose graphs:

- Vertex programs are written as a subclass of a C++ `Vertex` class. Given a
  vertex, we can construct the graph `([v], {}, {v}, {v})`.
- Given a graph `G`, we can duplicate the graph `n` times, denoted `G^n`.
- `A >= B` hooks up the outputs of `A` in a round-robin fashion to the inputs
  of `B`.
- `A >> B` hooks up the every output of `A` to every input of `B`.
- `A || B` merges `A` and `B` by taking the union of the vertex, edge, input,
  and output sets.

By default, the edges between vertexes are represented as temporary files. The
upstream node writes to a local file, and the downstream node reads form it. In
addition to this file-based edge, multiple vertexes can also be run in the same
process and communicate via an in-memory pipe. Alternatively, nodes can
communicate via TCP.

**Writing a Vertex Program.**
Vertexes are represented as arbitrary C++ classes that subclass from a `Vertex`
interface. The interface exposes a `Main` method which takes in a number of
readers and writers. Vertexes can also report their status to the JM. Writing
custom vertexes can sometimes be difficult. Dryad also includes a standard set
of default vertexes like map, filter, join. Legacy binaries can also be run as
vertexes.

**Job Execution.**
There is a single JM which is a point-of-failure. Of course, the JM could use
snapshotting or replication. Multiple copies of the same vertex may be run due
to failures and re-execution, so it assumed that vertexes are deterministic.
Vertexes can also provide a preference list describing which machine they want
to run on. This allows a graph to place computation near its input or to group
similar nodes. There are also nice monitoring tools to view the progress of a
Dryad execution.

Each vertex in a graph is put into a stage. Each stage has a stage manager
which manages the execution of the stage. The stage managers do things like
re-execute tasks to avoid stragglers.

The Dryad graph can also be changed at runtime to support optimization like
tree aggregation or partial aggregation.

**Building on Dryad.**
Dryad provides a rather low-level C++ interface. Other teams have developed
higher-level interfaces to Dryad including a Nebual scripting language,
integration with a RDBMS, and hope for integration with SQL or LINQ.

## [Dynamo: Amazon's Highly Available Key-value Store (2007)](https://scholar.google.com/scholar?cluster=5432858092023181552&hl=en&as_sdt=0,5)
**Overview.**
Amazon has a service-oriented infrastructure which consists of a large number
of networked services, each with a strict *SLA*: a formal contract between the
clients and server which guarantees the server meet certain performance
benchmarks (e.g. 99.9% of responses are within 500 milliseconds). Amazon's
user-facing business model makes it more important to meet the SLAs by
providing availability, scalability, and low-latency than it is to provide
strong consistency. Dynamo is Amazon's distributed higly-available eventually
consistent zero-hop distributed hash table (a.k.a. key-value store) that uses
consistent hashing, vector clocks, quorums, gossip, and more.

**System Interface.**
Dynamo is a key-value store where the values are arbitrary blobs of data. Users
can issue `get(key)` requests which returns either an object or a list of
conflicting objects and a context. If multiple objects are returned, the user
is responsible for merging them. Moreover, users can issue `put(key, context,
value)` requests where `context` is used to maintain version clocks.

**Partitioning Algorithm.**
Dynamo uses consistent hashing to partition data very similarly to Chord. Data
is hashed into a circular space. Nodes are broken down into virtual nodes, each
of which is randomly provided a point in the circular key space. Each node is
responsible for all the keys between it and its predecessor. The number of
virtual nodes at each physical node can be tuned according to the capacity of
the node.

**Replication.**
Data is sent to a *coordinator* which writes the data locally and also sends
the data to N-1 other nodes. Moreover, each data item has a *preference list*
of nodes where it should be written, and each node in the system knows the
preference list for all data items.

**Data Versioning.**
Data in Dynamo is timestamped with a vector clock. If a write `a` happens
before a write `b`, then the two writes can be reconciled trivially; this is
known as *syntactic reconciliation*. However, if `a` and `b` are concurrent,
then the system or the user has to perform *semantic reconciliation*. To avoid
vector clocks of unbounded size, vector clocks are given a maximum size, and
each entry in a vector clock is timestamped with a physical time. When the
vector clock exceeds its maximum size, the oldest entry is evicted.

**Execution of `get()` and `put()`.**
To execute a `get()` or `put()`, a Dynamo client can

1. Issue a request to a load balancer, or
2. issue it itself if it is a partition aware client (more on this later).

Dynamo uses quorums to write data. A read must be acknowledged by `R` servers,
a write must be acknowledged by `W` servers, and `R + W > N`.

**Handling Failures.**
Dynamo uses a *sloppy quorum* where data can be stored at a node outside its
preference list. The data is tagged with the node where the data should be, and
the node transfers it there eventually. Moreover, preference lists span
multiple data centers.

**Handling Permanent Failures.**
Nodes user Merkle trees to determine what state has diverged from one another.

**Membership and Failure Detection.**
Membership changes are initiated manually by a human. Nodes gossip membership
information and use it transfer data to the newly joined and removed nodes.
There are also seed nodes in the ring which nodes always gossip with to avoid
a split ring.

**Implementation.**
Dynamo is implemented with a pluggable storage engine and uses a SEDA
architecture implemented in Java.

**Experiences and Lessons Learned.**
Amazon has learned a lot from its experience with Dynamo:

- *Balancing performance and durability.* Improving durability can decrease
  performance. For example, if we want to write to `W` nodes, then increasing
  `W` decreases the availability of the system. Dynamo allows writes to be
  buffered by nodes, rather than written to their disks to increase
  availability at the cost of durability.
- *Ensuring uniform load.* Assuming there are enough hot keys, hashing data
  into a circular key space should ensure uniform load. However, the
  partitioning scheme described above where each node is divided into some
  number of virtual nodes, the virtual nodes are placed randomly on the ring,
  and the node placement determines data partitioning has some downsides. Two
  alternatives are to divide the key space into equal sized partitions and give
  each node a random number of virtual nodes. Or, to divide the key space into
  equal sized partitions and adjust the total number of tokens as nodes join
  and leave the system.
- *How many divergent versions.* Evaluation showed that there were not many
  divergent data items.
- *Client or server coordination.* Instead of communicating with a load
  balancer, clients can periodically request membership information and route
  requests themselves.
- *Balancing foreground and background.* Dynamo uses a resource controller to
  implement admission control for background tasks, preventing them from
  interfering with important foreground tasks.

## [Bigtable: A Distributed Storage System for Structured Data (2008)](https://scholar.google.com/scholar?cluster=535416719812038974&hl=en&as_sdt=0,5)
Bigtable is a non-relational database developed at Google that is distributed
across 1000s of machines storing petabytes of data. Bigtable is built using a
lot of other Google technology (e.g. GFS, Chubby, Borg, SSTable) and can
achieve both high throughput and low latency.

**Data Model and API.**
In short, Bigtable is a "sparse, distributed, persistent multi-dimensional
sorted map" of type: `(key: string * column: string * timestamp: int64) ->
value: string`. For example, a web Bigtable could be keyed by URL with columns
like language or content. Each entry in this Bigtable could have multiple
timestamped versions.

Operations on a single Bigtable row are transactional, though Bigtable does not
support multi-row transactions. The keys are also sorted lexicographically and
a range of keys, known as a *tablet*, is the unit of replication and locality.
This lets users colocate two keys by making the keys lexicographically close to
one another.

Columns in Bigtable are grouped into (a modest number of static) *column
families* where each column family is a unit of access control. Bigtables can
be scanned by row and by column.

Timestamps are 64 bit integers and can either be assigned by Bigtable in an
increasing way or assigned by the user program. Values can be garbage collected
by keeping at most `n` objects at a time or by keeping all objects that are `n`
seconds old.

**Implementation.**
BigTable consists of a single *master*, a bunch of *tablet servers*, and a
bunch of *clients*. The master is responsible for system administration. It
assigns tablets to tablet servers, oversees the addition and removal of tablet
servers, performs garbage collection, orchestrates load balancing, and manages
schema changes. Tablet servers host 10-1000 tables and are responsible for
servicing reads and writes. They also split tablets that have grown too large.

- *Tablet location.* The mapping from tablets to tablet servers is managed by a
  hierarchical metadata tree. First, a Chubby file contains the location of a
  *root tablet*: the first tablet in the METADATA table. The root tablet
  contains the addresses of the other tablets in the METADATA table which in
  turn store the locations of user tablets. Clients walk this hierarchy in
  order to find tablet servers. Note that tablets can find the location of a
  tablet without talking to the master.
- *Tablet assignment.* Each tablet is assigned to a single tablet server, and
  tablet assignments are managed by the master.

  When a tablet server wants to join Bigtable, it acquires an exclusive lock on
  a file in the `servers` directory in Chubby. So long as it has the lock, it
  can serve tablets. If it loses the lock, it tries to reacquire it, and if the
  file is deleted, the tablet server kills itself. The master heartbeats
  servers periodically. If the server doesn't respond or the server reports it
  lost the lock, the master tries to acquire the lock. If it does, it deletes
  the file and reassigns tablets.

  When a master joins Bigtable, it acquires an exclusive lock on a master file.
  It then scans the `servers` directory, contacts the servers to see what
  tablets they own, and compares it to the METADATA table. It can then start
  assigning unassigned tablets. It also manages the creation, deletion, and
  merging of tablets. Tablet servers manage the splitting of tablets.
- *Tablet serving.* Tablet servers commit updates to a log and also buffer the
  most recent updates in a sorted *memtable*. The rest of the data is stored in
  a series of SSTables in GFS. Each SSTable is a persistent immutable ordered
  mapping from strings to strings. If a tablet server crashes, its state can be
  recovered from its log. When a tablet server services a read or write, it
  performs access checks and whatnot and then resolves the read using the
  memtable and SSTables.
- *Compactions.* Periodically, the memtable is written as an SSTable to GFS;
  this is called a *minor compaction*. Also periodically, the memtable and
  multiple SSTables are merged into a single SSTable; this is called a *merging
  compaction*. If all data is compacted into a single SSTable, it's called a
  *major compaction*.

**Refinements.**
- *Locality groups.* Bigtable column families can be grouped together into a
  *locality group* for individual storage. Locality groups can also be
  designated in-memory.
- *Compression.* Users can specify that data should be compressed. Bigtable
  uses a two-pass compression scheme that favors compression speed or space,
  but in practice, the compression is pretty good.
- *Caching.* Tablet servers have a *scan cache* for temporal locality and a
  *block cache* for spatial locality.
- *Bloom filters.* When a tablet server services a read, it may have to read
  through every SSTable which can be expensive. To avoid this, tablet servers
  can use Bloom filters to probabilistically determine that an SSTable doesn't
  have a certain key.
- *Commit log.* Having a commit log per tablet leads to lots of random writes
  and smaller batches, so tablet servers merge updates for multiple tablets
  into a single commit log. This can complicate tablet recovery when multiple
  servers read a commit log applying a small fraction of the updates. To avoid
  this, the commit log can be sorted before being scanned.
- *Immutability.* The immutability of SSTables allows for concurrent reads
  without synchronization.

**Lessons.**
- Failures happen all the time, and the failures can be strange and exotic.
- [YAGNI](https://en.wikipedia.org/wiki/You_aren%27t_gonna_need_it).
- System monitoring is essential.
- Be simple.

## [DryadLINQ: A System for General-Purpose Distributed Data-Parallel Computing Using a High-Level Language (2008)](#dryadlinq-a-system-for-general-purpose-distributed-data-parallel-computing-using-a-high-level-language-2008)
DryadLINQ is a language extension and system which allows users to write
data-parallel code in a high-level language (i.e. LINQ supported languages like
C#, VB, F#, etc.) that operates over typed .NET objects. The code is then
automatically and transparently distributed by translating it to an optimized
Dryad job. MapReduce

- doesn't really have a type system;
- forces users to express complex programs as multiple MapReduce jobs; and
- doesn't support interstage optimizations.

Relation databases

- have very primitive type systems; and
- SQL is hard to use to express complex iterative workloads.

DryadLINQ doesn't suffer from these disadvantages.

**System Architecture.**
DryadLINQ compiles LINQ programs to Dryad jobs. To review, Dryad represents
computation as a graph where vertexes run C++ code and edges transport messages
between vertexes using temporary files, TCP connections, or in-memory pipes.
Dryad

1. instantiates dataflow graphs;
2. schedules tasks on workers;
3. handles faults by re-running failed vertexes;
4. monitors the progress of a job and exports it to users; and
5. facilitates runtime graph optimizations.

DryadLINQ execution follows the following the steps:

1. .NET programs lazily generate a DryadLINQ expression.
2. The program calls `ToDryadTable` which forces the execution of the
   expression.
3. The system compiles the expression into a Dryad graph.
4. A Job Manager (JM) is launched.
5. The JM spawns workers on the cluster.
6. Workers execute vertexes.
7. When the Dryad job finishes, the output is written into a table.
8. A `DryadTable` object is created.
9. Control is returned to the user.
10. Repeat.

**Programming with DryadLINQ.**

*LINQ.*
Users write programs using LINQ. The base type in LINQ is `IEnumerable<T>`
which represents an arbitrary iterable collection. There is also an
`IQueryable<T>` subclass of `IEnumerable<T>` which represents an unevaluated
LINQ expression which produces an `IEnumerable<T>`. `IQueryable`s are computed
lazily. LINQ supports a SQL-like declarative interface as well as a more
imperative object-oriented style.

*DryadLINQ.*
DryadLINQ adds a couple features and restrictions to LINQ. DryadLINQ datasets
are stored in `DryadTable<T>`s: a distributed collection partitioned across a
number of machines. A `DryadTable` is a subtype of `IQueryable` and can be
stored in a distributed file system, a relational database, etc. Input and
output `DryadTable`s are generated by calling `GetTable` and `ToDryadTable`.
DryadLINQ adds new operators to partition data. It also provides two escape
hatch operators: `Apply` and `Fork`. `Apply` calls arbitrary code with an
iterator over a `DryadTable`. `Fork` calls arbitrary code which generates
multiple different outputs. In the worst case, a `Fork` or `Apply` is executed
at a single node. DryadLINQ also allows users to specify optimization hints to
the compiler.

*Building on DryadLINQ.*
It's easy to build even higher level interfaces on top of DryadLINQ. For
example, it's easy to build MapReduce and ML libraries on top of DryadLINQ.

**System Implementation.**

*Execution Plan Graph.*
DryadLINQ converts LINQ expressions into Execution Plan Graphs (EPG) which are
like query plans in a traditional relational database query optimizer. The EPG
is optimized and acts as a skeleton for the final Dryad graph it is converted
to. Vertexes of the EPG are annotated with type information and the edges are
annotated with partitioning and ordering information.

*Static Optimizations.*
- *Pipelining.* If multiple vertexes are chained directly together, Dryad can
  run them in a single process.
- *Removing Redundancy.* DryadLINQ tries to avoid unnecessary or redundant
  partitions since they are expensive.
- *Eager Aggregation.* If an aggregation can be pushed before a partitioning,
  it is.
- *I/O Reduction.* DryadLINQ tries to make all edges TCP pipes or in-memory
  pipes. It also compresses data before a partition.

*Dynamic Optimizations.*
DryadLINQ performs tree aggregation, but the structure of the tree depends on
scheduling decisions that are made at runtime. It also can dynamically
determine the number of partitions or how a dataset is partitioned at runtime
based on the size of the dataset. Traditional databases would often have to
estimate this information and could be wildly wrong.

*Code Generation.*
When an EPG is converted into a Dryad graph, code is generated to (a) process
data on vertexes and (b) serialize and deserialize data. Also, any values that
the LINQ programs reference have to be serialized and shipped with the code.
Any libraries the code depends on also have to be shipped.

*Leveraging Other LINQ Providers.*
PLINQ is a system which runs LINQ programs in parallel on a single multicore
machine. DryadLINQ uses PLINQ within a vertex. It also uses a LINQ-to-SQL
converter which allows vertexes to run SQL queries constructed from LINQ
expressions.

*Debugging.*
DryadLINQ's typechecking helps catch a lot of bugs. An entire cluster can also
be run on a user's local machine to debug the execution of the program. If a
node fails while running, the system can also re-run the node to see what the
problem is. Performance debugging is doable but still challenging.

## [CRDTs: Consistency without concurrency control (2009)](https://scholar.google.com/scholar?cluster=9773072957814807258&hl=en&as_sdt=0,5)
**Overview.**
Concurrently updating distributed mutable data is a challenging problem that
often requires expensive coordination. *Commutative replicated data types*
(CRDTs) are data types with commutative update operators that can provide
convergence without coordination. Moreover, non-trivial CRTDs exist; this paper
presents Treedoc: an ordered set CRDT.

**Ordered Set CRDT.**
An ordered set CRDT represents an ordered sequence of atoms. Atoms are
associated with IDs with five properties:
1. Two replicas of the same atom have the same ID.
2. No two atoms have the same ID.
3. IDs are constant for the lifetime of an ordered set.
4. IDs are totally ordered.
5. The space if IDs is dense. That is for all IDS P and F where P < F, there
   exists an ID N such that P < N < F.

The ordered set supports two operations:

1. insert(ID, atom)
2. delete(ID)

where atoms are ordered by their corresponding ID. Concretely, Treedoc is
represented as a tree, and IDs are paths in the tree ordered by an infix
traversal. Nodes in the tree can be *major nodes* and contain multiple
*mini-nodes* where each mini-node is annotated with a totally ordered
*disambiguator* unique to each node.

Deletes simply tombstone a node. Inserts work like a typical binary tree
insertion. To avoid the tree and IDs from getting too large, the tree can
periodically be *flattened*: the tree is restructured into an array of nodes.
This operation does not commute nicely with others, so a coordination protocol
like 2PC is required.

**Treedoc in the Large Scale.**
Requiring a coordination protocol for flattening doesn't scale and runs against
the spirit of CRDTs. It also doesn't handle churn well. Instead, nodes can be
partitioned into a set of core nodes and a set of nodes in the nebula. The core
nodes coordinate while the nebula nodes lag behind. There are protocols for
nebula nodes to catch up to the core nodes.

## [The Five-Minute Rule 20 Years Later (2009)](https://scholar.google.com/scholar?cluster=4362732859860672431&hl=en&as_sdt=0,5)
In 1987, Gray and Putzolu introduced the [five-minute
rule](https://github.com/mwhittaker/five_minute_rule). Ten years later, the
price and performance of hardware had changed, and a new five-minute rule was
formed. Twenty years later, hardware has changed even more (e.g. the rise of
multi-core, hardware virtualization, etc.). One new piece of hardware, flash
memory, is becoming increasingly popular. Flash memory sits between RAM and
disk in terms of cost, latency, bandwidth, power consumption etc. Since flash
memory is still young, there are a lot of questions to be answered:

1. What will be the interface to flash memory?
2. Should flash memory be treated like an extended RAM or like an extended
   disk?
3. How can databases and operating systems best take advantage of flash memory?
4. How can B-trees be optimized for flash memory?

This paper revisits the five-minute rule in the era of flash and attempts to
answer some of these questions.

**Assumptions.**
This paper makes the following assumptions.

*Flash Memory.*
- We assume that flash memory is going to act as a buffer pool sitting between
  RAM and disk. That is, pages will be moved between RAM, flash, and memory
  using some algorithm (e.g. LRU).
- We assume that the data structures used to manage this page migration will be
  stored in memory.
- We assume the hardware details of the flash devices are abstracted behind common hardware interfaces.
- We assume data can be transferred between RAM and flash using DMA. Also, data
  can be transferred between flash and disk either using DMA or by using an
  in-RAM transfer buffer.
- We assume flash memory implements wear leveling.
- We assume data is asynchronously moved from flash to disk.

*File Systems.*
- We assume files are often read in their entirety, modified slightly, and
  written back in their entirety. Thus, the file system performs best when
  files are allocated contiguously.
- We assume that file systems maintain their consistency by cleverly ordering
  writes and performing expensive fsck-like recovery upon rebooting. The file
  systems do not use journaling.

*Databases.*
- We assume databases heavily use B+-trees.
- We assume databases use an ARISE like write-ahead-logging recovery algorithm
  with frequent checkpointing.

*Other Hardware.*
- We assume we have a lot of RAM.
- We assume we have fast CPUs.

**The Five-Minute Rule**
The paper says the original 1987 five-minute rule uses the following formula:
```
                                PagesPerMBofRAM * Price-PerDiskDrive
BreakEvenIntervalinSeconds = ------------------------------------------
                             AccessesPerSecondPerDisk * PricePerMBofRAM
```
which can be derived from equating the cost of storing an object on disk and in
RAM. The paper does not provide the derivation, but we can. Consider an object
that is `MemSize` MB large. We consider the cost of storing the object on disk
and in RAM:

- **On Disk.** Let `NumPages` be the number of pages needed to store our
  object. We can calculate `NumPages` as `NumPages = PagesPerMBofRAM * MemSize`
  The required bandwidth needed to read the object is then `NumPages /
  BreakEvenIntervalinSeconds`. If a disk delivers `AccessesPerSecondPerDisk` of
  bandwidth, then the bandwidth we need for our object takes up the following
  fraction of the disk:

  ```
                PagesPerMBofRAM * MemSize
  -----------------------------------------------------
  BreakEvenIntervalinSeconds * AccessesPerSecondPerDisk
  ```

  Multiplying by the price of a whole disk, we get the price we have to pay for
  the fraction of the disk that we need.
  ```
      PagesPerMBofRAM * MemSize * Price-PerDiskDrive
  -----------------------------------------------------
  BreakEvenIntervalinSeconds * AccessesPerSecondPerDisk
  ```
- **In RAM.** The cost of storing the object in RAM is straightforward.

    ```
    PricePerMBofRAM * MemSize
    ```

If we equate the two equation, we can solve for `BreakEvenIntervalinSeconds`

```

                               PagesPerMBofRAM * MemSize * Price-PerDiskDrive
PricePerMBofRAM * MemSize = -----------------------------------------------------
                            BreakEvenIntervalinSeconds * AccessesPerSecondPerDisk

                         PagesPerMBofRAM * Price-PerDiskDrive
PricePerMBofRAM = -----------------------------------------------------
                  BreakEvenIntervalinSeconds * AccessesPerSecondPerDisk

                                PagesPerMBofRAM * Price-PerDiskDrive
BreakEvenIntervalinSeconds = ------------------------------------------
                             AccessesPerSecondPerDisk * PricePerMBofRAM
```

Plugging in modern values for the variables in the formulas above, we get the
following insights:

- The break-even time for 4KB objects on RAM/disk is 1.5 hours. This was 2
  minutes 20 years ago. Interestingly, RAM prices have dropped by five orders
  of magnitude, but the break-even time has only dropped by 2 orders of
  magnitude. This is because the prices of disks has also dropped while the
  performance of disks has risen.
- The break-even time for 64KB objects on RAM/disk is 5 minutes.
- The break-even time for 4KB objects on RAM/flash is 15 minutes.
- The break-even time for 4KB objects on flash/disk is 2.5 hours.

Note that almost all active data will stay in RAM and flash. Also note that the
5 minute break-even time applies for much larger objects than before.

**Page Movement.**
Since flash memory acts as a buffer pool, data has to be transferred between
RAM and flash and between flash and disk. The page movement policies can be
similar to existing page movement polices for RAM and disk. Moreover, if flash
acts as an extended disk, then movement between flash and disk will look like
movement during defragmentation.

**Tracking Pages Locations.**
File systems track page locations using pointer pages (e.g. indirect blocks).
Databases, on the other hand, use B+-trees. B+-trees have also been heavily
optimized. For example, there are B+-tree variants which are designed to reduce
the overheads of maintaining sibling pointers and of logging. Thus, moving data
around in a file system is not as efficient as in a database. As a result, a
file system would not want to use flash as an extended disk.

**Checkpoint Processing.**
Databases are frequently checkpointing, moving data to the disk. If databases
used flash memory as an extended RAM, then all the data written to it would
still have to be moved to disk. However, if they use it as an extended disk,
they can avoid a lot of checkpointing overhead.

**Page Sizes.**
The size of nodes in a B+-tree is a tunable parameter. If the nodes are small,
then there's not a lot of overhead to reading a node. If the nodes are big,
then the tree is short. The optimal B+-tree node size is not too big and not
too small.

The optimal node size for modern disks is rather large. Most of the overhead of
reading a node from the disk is seek time, not transfer time. Thus, increasing
the block size comes mostly for free. On the other hand, the optimal node size
for modern flash memory is much smaller because the seek time is much lower.
Some B+-tree designs allow for different block sizes, allowing a B+-tree to
optimally use both disk and flash.

**Database Query Processing.**
Some algorithms used in database systems are carefully designed to take
advantage of memory and disk. For example, external sort takes advantage of
memory and disk. These algorithms can be redesigned to take advantage of flash
as well. Query planning may also have to change. For example, reading from an
unclustered index in flash may be less expensive than doing a full table scan,
even though the full table scan may be less efficient that reading the
unclustered index from disk.

## [SnowFlock: Rapid Virtual Machine Cloning for Cloud Computing (2009)](https://scholar.google.com/scholar?cluster=3030124086251534312&hl=en&as_sdt=0,5)
**Summary.**
Public clouds like Amazon's EC2 or Google's Compute Engine allow users to
elastically spawn a huge number virtual machines on a huge number of physical
machines. However, spawning a VM can take on the order of minutes, and
typically spawned VMs are launched in some static initial state. SnowFlock
implements the VM fork abstraction, in which a parent VM forks a set of
children VMs all of which inherit a snapshot of the parent. Moreover, SnowFlock
implements this abstraction with subsecond latency. A subsecond VM fork
implementation can be used for sandboxing, parallel computation (the focus of
this paper), load handling, etc.

SnowFlock is built on top of Xen. Specifically, it is a combination of
modifications to the Xen hypervisor and a set of daemons running in dom0 which
together forms a distributed system that manages virtual machine forking.
Guests use a set of calls (i.e. `sf_request_ticket`, `sf_clone`, `sf_join`,
`sf_kill`, and `sf_exit`) to request resources on other machines, fork
children, wait for children, kill children, and exit from a child. This implies
that applications must be modified.  SnowFlock implements the forking mechanism
and leaves policy to pluggable cluster framework management software.

SnowFlock takes advantage of four insights with four distinct implementation
details.

1. VMs can get very large, on the order of a couple of GBs. Copying these
   images between physical machines can saturate the network, and even when
   implemented using things like multicast, can still be slow. Thus, SnowFlock
   must reduce the amount of state transfered between machines. SnowFlock takes
   advantage of the fact that a newly forked VM doesn't need the entire VM
   image to start running. Instead, SnowFlock uses *VM Descriptors*: a
   condensed VM image that consists of VM metadata, a few memory pages,
   registers, a global descriptor table, and page tables. When a VM is forked,
   a VM descriptor for it is formed and sent to the children to begin running.
2. When a VM is created from a VM descriptor, it doesn't have all the memory it
   needs to continue executing. Thus, memory must be sent from the parent when
   it is first accessed. Parent VMs use copy-on-write to maintain an immutable
   copy of memory at the point of the snapshot. Children use Xen shadow pages
   to trap accesses to pages not yet present and request them from the parent
   VM.
3. Sometimes VMs access memory that they don't really need to get from the
   parent. SnowFlock uses two *avoidance heuristics* to avoid the transfer
   overhead. First, if new memory is being allocated (often indirectly through
   a call to something like malloc), the memory contents are not important and
   do not need to be paged in from the parent. A similar optimization is made
   for buffers being written to by devices.
4. Finally, parent and children VMs often access the same code and data.
   SnowFlock takes advantage of this data locality by prefetching; when one
   child VM requests a page of memory, the parent multicasts it to all
   children.

Furthermore, the same copy-on-write techniques to maintain an immutable
snapshot of memory are used on the disk. And, the parent and children virtual
machines are connected by a virtual subnet in which each child is given an IP
address based on its unique id.

## [BOOM Analytics: Exploring Data-Centric, Declarative Programming for the Cloud (2010)](TODO) ##
**Summary.**
Programming distributed systems is hard. Really hard. This paper conjectures
that *data-centric* programming done with *declarative programming languages*
can lead to simpler distributed systems that are more correct with less code.
To support this conjecture, the authors implement an HDFS and Hadoop clone in
Overlog, dubbed BOOM-FS and BOOM-MR respectively, using orders of magnitude
fewer lines of code that the original implementations. They also extend BOOM-FS
with increased availability, scalability, and monitoring.

An HDFS cluster consists of NameNodes responsible for metadata management and
DataNodes responsible for data management. BOOM-FS reimplements the metadata
protocol in Overlog; the data protocol is implemented in Java. The
implementation models the entire system state (e.g. files, file paths,
heartbeats, etc.) as data in a unified way by storing them as collections. The
Overlog implementation of the NameNode then operates on and updates these
collections. Some of the data (e.g. file paths) are actually views that can be
optionally materialized and incrementally maintained. After reaching (almost)
feature parity with HDFS, the authors increased the availability of the
NameNode by using Paxos to introduce a hot standby replicas. They also
partition the NameNode metadata to increase scalability and use metaprogramming
to implement monitoring.

BOOM-MR plugs in to the existing Hadoop code but reimplements two MapReduce
scheduling algorithms: Hadoop's first-come first-server algorithm, and
Zaharia's LATE policy.

BOOM Analytics was implemented in order of magnitude fewer lines of code thanks
to the data-centric approach and the declarative programming language. The
implementation is also almost as fast as the systems they copy.

## [The Declarative Imperative: Experiences and Conjectures in Distributed Logic (2010)](https://scholar.google.com/scholar?cluster=1374149560926608837&hl=en&as_sdt=0,5)
**Summary.**
With (a strict interpretation of) Moore's Law in decline and an overabundance
of compute resources in the cloud, performance necessitates parallelism. The
rub?  Parallel programming is difficult. Contemporaneously, Datalog (and
variants) have proven effective in an increasing number of domains from
networking to distributed systems. Better yet, declarative logic programming
allows for programs to be built with orders of magnitude less code and permits
formal reasoning. In this invited paper, Joe discusses his experiences with
distributed declarative programming and conjectures some deep connections
between logic and distributed systems.

Joe's group has explored many distributed Datalog variants, the latest of which
is *Dedalus*. Dedalus includes notions of time: both intra-node atomicity and
sequencing and inter-node causality. Every table in Dedalus includes a
timestamp in its rightmost column. Dedalus' rules are characterized by how they
interact with these timestamps:

- *Deductive rules* involve a single timestamp. They are traditional Datalog
  stamements.
- *Inductive rules* involve a head with a timestamp one larger than the
  timestamps of the body. They represent the creation of facts from one point
  in time to the next point in time.
- *Asynchronous rules* involve a head with a non-deterministically chosen
  timestamp. These rules capture the notion of non-deterministic message
  delivery.

Joe's group's experience with distributed logic programming lead to the
following conclusions:

- Datalog can very concisely express distributed algorithms that involved
  recursive computations of transitive closures like web crawling.
- Annotating relations with a location specifier column allows tables to be
  transparently partitions and allows for declarative communication: a form of
  "network data independence". This could permit many networking optimizations.
- Stratifying programs based on timesteps introduces a notion of
  transactionality. Every operation taking place in the same timestamp occurs
  atomically.
- Making all tables ephemeral and persisting data via explicit inductive rules
  naturally allows transience in things like soft-state caches without
  precluding persistence.
- Treating events as a streaming join of inputs with persisted data is an
  alternative to threaded or event-looped parallel programming.
- Monotonic programs parallelize embarrassingly well. Non-monotonicity requires
  coordination and coordination requires non-monotonicity.
- Logic programming has its disadvantages. There is code redundancy; lack of
  scope, encapsulation, and modularity; and providing consistent distributed
  relations is difficult.

The experience also leads to a number of conjectures:

- The CALM conjecture stats that programs that can be expressed in monotonic
  Datalog are exactly the programs that can be implemented with
  coordination-free eventual consistency.
- Dedalus' asynchronous rules allow for an infinite number of traces. Perhaps,
  all these traces are confluent and result in the same final state. Or perhaps
  they are all equivalent for some notion of equivalence.
- The CRON conjecture states that messages sent to the past lead only to
  paradoxes if the message has non-monotonic implications.
- If computation today is so cheap, then the real computation cost comes from
  coordination between strata. Thus, the minimum number of Dedalus timestamps
  required to implement a program represents its minimum *coordination
  complexity*.
- To further decrease latency, programs can be computed approximately either by
  providing probabilistic bounds on their outputs or speculatively executing
  and fixing results in a post-hoc manner.

## [Conflict-free Replicated Data Types (2011)](https://scholar.google.com/scholar?cluster=4496511741683930603&hl=en&as_sdt=0,5) ##
**Summary.**
Eschewing strong consistency in favor of weaker consistency allows for higher
availability and lower latency. On the other hand, programming with weaker
consistency models like eventual consistency has traditionally been ad-hoc and
error-prone. CRDTs provide a way to achieve eventual consistency in a
principled way.

This paper considers a set of non-byzantine processes `{p_1, ..., p_n}`
connected by an asynchronous network. Each process `p_i` maintains some state
`s_i` that is updated over time. Processes use some mechanism like gossip to
communicate states to one another, and whenever a process `p_i` receives state
`s_j` from process `p_j`, it merges `s_j` into its state `s_i`.

More formally, each process maintains a *state-based object* which we model as
a five tuple `(S, s^0, q, u, m)` where

- `S` is a set of states,
- `s^0 \in S` is the initial state,
- `q: S -> 'a` is a query method,
- `u: S -> 'a -> S` is an update method, and
- `m: S -> S -> S` is a merge method.

All process begin with the initial state `s^0`. Clients can query the value of
the object with `q` (i.e. `s.q()`) and update the object with `u` (e.g.
`s.u(a)`). When processes communicate state, they are merged with `m` (e.g.
`s_i.m(s_j)`).

Method invocations (i.e. invocations of `q`, `u`, or `m`) are totally ordered
on each process and sequentially numbered starting from 1. States are similarly
ordered and numbered, updating with every method invocation:

    s^0 . f^1 . s^1 . f^2 . s^2 . ...

An object's *state-based causal history* traces its updates over logical time.
Formally, a causal history `C = [c_1, ..., c_n]` is an `n`-tuple of versioned
sets `c_i^0, c_i^1, c_i^2, ...` (one for each process) where each set contains
updates (e.g.  `u_i^k(a)`). A causal history is updated at a process `i` as
follows:

- When `i` issues a query, `c_i` is unchanged. That is, `c_i^k = c_i^{k+1}`.
- When `i` issues an update, it is added to `c_i`. That is, `c_i^{k+1} = c_i^k
  \cup {u_i^k}`.
- When `i` merges another state `s_j`, `c_i` and `c_j` are merged. That is,
  `c_i^{k+1} = c_i^k \cup c_j^k`.

An update is *delivered* at `i` when it enters `c_i`. An update `u` *happens
before* another update `v_i^k` when `u` is in `c_i^k`. Two updates are
*concurrent* if neither happens before the other.

These definitions are notationally dense but are a simple formalization of
Lamport's notions of logical time. Here is an illustration of how causal
history evolves over time for a three process system.


    p_0: {} -[u(a)]-> {u(a)} -.
                               \
    p_1: {} -[q]----> {} -------`-[m]-> {u(a)}---.-[m]-> {u(a), v(b)}
                                                /
    p_2: {} -[v(b)]-> {v(b)} ------------------'


We define an object to be *eventually consistent* if it satisfies the following
three properties:

- *Eventual delivery:* An update delivered at a correct process is eventually
  delivered to all correct processes.
- *Convergence:* If two processes have the same updates, they will *eventually*
  have the same states.
- *Termination:* All methods terminate.

A *strongly eventually consistent* (SEC) object is one that is eventually
consistent and additionally satisfies the following property.

- *Strong Convergence:* If two processes have the same updates, they have the
  same states.

A state based object imbued with a partial order `(S, <=, s^0, q, u, m)` is a
*monotonic semilattice* if

- `(S, <=)` is a join semilattice,
- `s_i.m(s_j)` computes the least upper bound (or join) or `s_i` and `s_j`, and
- state is monotonically non-decreasing over updates (i.e. `s <= s.u(x)`).

State-based objects that are monotonic semilattices (CvRDTs) are SEC.

Dual to state-based objects are *operation-based objects*. In the
operation-based model, processes communicate updates to one another rather than
states. When a process receives an update it applies it to its state. Formally,
an op-based object is a 6 tuple `(S, s^0, q, t, u, P)` where

- `S` is a set of states,
- `s^0 \in S` is the initial state,
- `q` is a query method,
- `t` is a side-effect free prepare-update method,
- `u` is an effect-update method, and
- `P` is a precondition (not used much in this paper).

Calls to `u` must be immediately preceded by calls to `t`. Op-based causal
histories are defined similarly to state-based causal histories where `t`
behaves like `q` and `m` is now missing. The definition of happens-before and
concurrent operations extends naturally from earlier. An op-based object whose
updates commute is a CvRDT. If messages are delivered exactly once, message
delivery respect the causal order, then CvRDTs are SEC.

Turns out, CvRDTs and CmRDTs are equivalent in that they can simulate one
another. Moreover SEC is incomparable to sequential consistency. Consider an
add-wins set CRDT in which one process performs `add(e); remove(e')` and
another performs `add(e'); remove(e)`. When the two sets are merged, they
contain `e` and `e'`, but under sequential consistency, one of the removes must
occur last.

Example CRDTs include vector clocks, increment-only counters,
increment-decrement counters add-only sets, add-remove sets, maps, logs, and
graphs.

## [Consistency Analysis in Bloom: a CALM and Collected Approach (2011)](https://scholar.google.com/scholar?cluster=9165311711752272482&hl=en&as_sdt=0,5) ##
**Summary.**
Strong consistency eases reasoning about distributed systems, but it requires
coordination which entails higher latency and unavailability. Adopting weaker
consistency models can improve system performance but writing applications
against these weaker guarantees can be nuanced, and programmers don't have any
reasoning tools at their disposal. This paper introduces the *CALM conjecture*
and *Bloom*, a disorderly declarative programming language based on CALM, which
allows users to write loosely consistent systems in a more principled way.

Say we've eschewed strong consistency, how do we know our program is even
eventually consistent? For example, consider a distributed register in which
servers accept writes on a first come first serve basis. Two clients could
simultaneously write two distinct values `x` and `y` concurrently. One server
may receive `x` then `y`; the other `y` then `x`. This system is not eventually
consistent. Even after client requests have quiesced, the distributed register
is in an inconsistent state. The *CALM conjecture* embraces Consistency As
Logical Monotonicity and says that logically monotonic programs are eventually
consistent for any ordering and interleaving of message delivery and
computation. Moreover, they do not require waiting or coordination to stream
results to clients.

*Bud* (Bloom under development) is the realization of the CALM theorem. It is
Bloom implemented as a Ruby DSL. Users declare a set of Bloom collections (e.g.
persistent tables, temporary tables, channels) and an order independent set of
declarative statements similar to Datalog. Viewing Bloom through the lens of
its procedural semantics, Bloom execution proceeds in timesteps, and each
timestep is divided into three phases. First, external messages are delivered
to a node. Second, the Bloom statements are evaluated. Third, messages are sent
off elsewhere. Bloom also supports modules and interfaces to improve
modularity.

The paper also implements a key-value store and shopping cart using Bloom and
uses various visualization tools to guide the design of coordination-free
implementations.

## [Dedalus: Datalog in Time and Space (2011)](https://scholar.google.com/scholar?cluster=4658639044512647014&hl=en&as_sdt=0,5)
**Summary.**
Researchers have explored using Datalog variants to implement distributed
systems, often with orders of magnitude less code. However, there are two
distributed system concepts that are difficult to model in Datalog:

1. *mutable state*, and
2. *asynchronous processing and communication*.

Some Datalog variants add imperative features to Datalog, but this leads to
lost optimization opportunities and ambiguous semantics. *Dedalus* is a subset
of Datalog with negation, aggregation, an infinite successor relation, and
non-deterministic choice that introduces the notion of time to cleanly model
both mutability and asynchrony.

Let Ci and Ai range over constants and variables respectively and consider an
infinite successor relation over an abstract domain Z. We first develop the
core of Dedalus, *Dedalus0*, which is Datalog with negation with the additional
syntactic restrictions:

1. *Schema*. The final attribute of every predicate is from the domain Z. This
   *time suffix* connotes the timestamp of the record.
2. *Time Suffix*. Every subgoal must be unified on a common time suffix `T` and
   the head of every rule can take of of two forms:
    - In *deductive rules*, the head's time suffix `S` is equal to `T`. These
      rules connote deductions in a single time step.
    - In *inductive rules*, the head's time suffix `S` is the successor of `T`.
      These rules connote inductions across time.
3. *Positive and Negative Predicates*. Every extensional predicate `p` has a
   corresponding positive and negative predicate `p_pos` and `p_neg`.
4. *Guarded EDB*. No rule, except for those above, can involve extensional
   predicates.

Dedalus models the *persistence* of a predicate using a simple identity
inductive rule:

    p_pos(A, B, ..., Z) @next <- p_pos(A, B, ..., Z).

*Mutable state* is modeled using a conjunction of `p_pos` and `p_neg`:

    p_pos(A, B, ..., Z) @ next <-
        p_pos(A, B, ..., Z),
        not p_neg(A, B, ..., Z).

This pattern is so common, it is expressed using the `persist[p_pos, p_neg, n]`
macro. We can model a number that increases whenever an event occurs (aka a
*sequence*) as follows:


    seq(B) <- seq(A), event(_), succ(A, B).
    seq(A) <- seq(A), not event(_).
    seq(0)@0.

We can use aggregation to implement a priority queue. Consider a predicate
`priority_queue(X, P)` with values `X` and priority `P`. We can flatten out the
queue's contents over time by buffering values in `m_priority_queue` and
dequeueing them into `priority_queue` in order of priority.

    persist[m_priority_queue_pos, m_priority_queue_neg, 2]

    the_min(min<P>) <- m_priority_queue(_, P).

    priority_queue(X, P)@next <-
        m_priority_queue(X, P),
        the_min(P)

    m_priority_queue_neg(X, P)@next <-
        m_priority_queue(X, P),
        the_min(P)

A Dedalus0 program is *syntactically stratifiable* if there are no cycles in the
predicate dependency graph that contain a negative edge. Similarly, a program
is *temporally stratifiable* if the *deductive reduction* of it is
syntactically stratifiable. Every Dedalus0 program that is temporally
stratifiable has a unique perfect model.

A Datalog program (or Dedalus0 program) is considered *safe* if it has a finite
model. The following syntactic restrictions imply safety:

1. No functions.
2. Range restricted variables. That is, all variables appearing in the head
   must appear in a non-negated subgoal.
3. The EDB is finite.

We say a Dedalus0 rule is *instantaneously safe* if it is deductive,
function-free (1), and range-restricted (2). A Dedalus0 program is
instantaneously safe if all rules in its deductive reduction are
instantaneously safe. Two sets of ground atoms are *equivalent modulo time* if
the two sets are equal after projecting out the time suffix. A Dedalus0 program
is quiescent at time T if it its atoms are equivalent modulo time to those at
time T-1. A Dedalus0 program is *temporally safe* if it is *henceforth
quiescent* after some time T. The paper provides three syntactic restrictions
that are sufficient for a temporally stratifiable program to be temporally
safe.

Dedalus is a superset of Dedalus0 that introduces asynchronous choice.
Intuitively, Dedalus introduces a third *asynchronous* type of rule that allows
the head's timestamp to be a non-deterministic value, even values that are
earlier than the timestamp of the body! In addition, Dedalus introduces
horizontal partitioning by designating the first column of a predicate to be a
*location specifier*. Entanglement:

    p(A, B, N)@async <- p(A, B)@N

can be used to implement things like Lamport clocks.

## [Dominant Resource Fairness: Fair Allocation of Multiple Resource Types (2011)](https://scholar.google.com/scholar?cluster=9727161448401355953&hl=en&as_sdt=0,5)
Scheduling and resource allocation policies are at the heart of many systems.
However, most resource allocation policies, including very popular ones like
*max-min fairness*, involve allocating a single resource type. In distributed
data processing frameworks like Hadoop and Dryad, resources often come in many
forms (e.g. CPU, GPU, disk size, memory size, network bandwidth, etc.) and
clients often have varying resource requirements. This paper introduces
*dominant resource fairness* (DRF): a new resource allocation policy that
handles multiple resource types. DRF satisfies a good number of desirable
properties and is implemented in Mesos.

**Allocation Properties.**
It's not easy to describe what makes a resource allocation policy "good" or
"fair", especially since most resource allocation policies don't concern
multiple object types. Here, we present a set of desirable properties that any
resource allocation policy ought to have. The first four are essential; the
last four are nice.

1. **Sharing incentive.** No user should be better off if resource were
   statically and equally partitioned between users.
2. **Strategy proofness.** Users should be able to improve their allocations by
   lying about their requirements.
3. **Envy-freeness.** No user should prefer the allocation of another user.
4. **Pareto efficient.** We should not be able to allocate more resources to a
   user without taking away resources from another.
5. *Single resource fairness.* For a single resource, the solution should
   reduce to max-min fairness.
6. *Bottleneck fairness.* If all users want the same resource the most, the
   solution should reduce to max-min fairness.
7. *Population monotonicity.* When a user leaves the system, nobody's
   allocation should decrease.
8. *Resource monotonicity.* When new resources are introduced into the system,
   nobody's allocation should decrease.

**Dominant Resource Fairness.**
Assume our system has 9 CPU and 18 GB of memory. Assume job A has tasks that
require `<1 CPU, 4 GB>` and job B has tasks that require `<3 CPU, 1 GB>`. Tasks
of A need `1/9` of the CPU and `4/18 = 2/9` of the memory. Tasks of B need
`3/9 = 1/3` of the CPU and `1/18` of the memory. We say that CPU is the
*dominant share* of A and memory is the dominant share of B. Generally, given
resource capacity vector `<c_1, ..., c_n>` and a user task requirement vector
`<a_1, ..., a_n>`, the dominant share is the max of all `a_i/c_i`.

DRF aims to maximize allocations such that all users have an equal dominant
share. In the example above, we want to find `x` allocations for A and `y`
allocations for B such that:

```
max(x, y)
x + 3y <= 9
4x + y <= 18
2x/9    = y/3
```

Algorithmically, we repeatedly choose the user with the lowest dominant share
and grant it its resource requirements if necessary while updating its dominant
share. If we assign a weight vector `<w_1, ..., w_n>` to each user and scale
each dominant share by `s_i` by `w_i`, then we have *weighted DRF*.

**Alternate Policies.**
There are other resource allocation policies that can be used to manage
multiple resource types.

First is *Asset Fairness*. In asset fairness, we maximize resource allocation
while ensuring that each user has an equal total allocation. In our example,

```
max(x, y)
x + 3y <= 9
4x + y <= 18
6x = 7y
```

Here, we imagine each CPU costs $2 and each GB costs $1. Job A spends $6 per
task, and job B spends $7 per task.

Second is *Competitive Equilibrium from Equal Incomes* (CEEI) where the product
of all dominant shares is maximized. That is,

```
max(xy)
x + 3y <= 9
4x + y <= 18
```

**Analysis.**

| Property                 | Asset | CEEI | DRF |
| ------------------------ | ----- | ---- | --- |
| Sharing Incentive        |       | ✓    | ✓   |
| Strategy-proofness       | ✓     |      | ✓   |
| Envy-freeness            | ✓     | ✓    | ✓   |
| Pareto efficiency        | ✓     | ✓    | ✓   |
| Single Resource Fairness | ✓     | ✓    | ✓   |
| Bottleneck Fairness      |       | ✓    | ✓   |
| Population Monotonicity  | ✓     |      | ✓   |
| Resource Monotonicity    |       |      |     |

Note that Asset Fairness does not provide Sharing Incentive and CEEI is not
strategy proof. DRF does not provide Resource Monotonicity, but it can be shown
that no resource allocation policy that satisfies sharing incentive and Pareto
efficiency can provide resource monotonicity.

Up until now, we've assumed continuous allocations. We can extend DRF to use
discrete allocations for K machines where each machine has at least a max-task
amount of resources. Here the max-task is `<max d_i1, ..., max d_im>`. It's
guaranteed that the difference in dominant share allocations between any two
users is less than one max-task compared to the continuous case.

## [Mesos: A Platform for Fine-Grained Resource Sharing in the Data Center (2011)](https://scholar.google.com/scholar?cluster=816726489244916508&hl=en&as_sdt=0,5)
See [`https://github.com/mwhittaker/mesos_talk`](https://github.com/mwhittaker/mesos_talk).

## [Don't Settle for Eventual: Scalable Causal Consistency for Wide-Area Storage with COPS (2011)](https://scholar.google.com/scholar?cluster=16870210484225303236&hl=en&as_sdt=0,5)
**Overview.**
Many modern applications require high **a**vailability, low **l**atency,
**p**artition tolerance, and **s**calability. The CAP theorem tells us that
these kind of applications, dubbed *ALPS* applications, cannot be implemented
with strong consistency. However, weaker consistency models can be implemented
with high availability. This paper introduces *causal consistency with
convergent conflict handling* (causal+ consistency), one of the strongest
consistency models that can be supported with high availability, and a causal+
consistent key-value store COPS. It also extends COPS with multi-key get
transactions in a system dubbed COPS-GT.

**Causal+ Consistency.**
We model our system as set of independently executing threads of execution that
issue `get(key)` and `put(key, val)` operations against one replica of a
key-value store. We say operation `a` happens before operation `b`, denoted `a
-> b` if

1. `a` happens before `b` in a single thread of execution,
2. `a = get(key)` returns the value written by `b = put(key, val)`, or
3. there exists a `c` such that `a -> c` and `c -> b`.

A system is causally consistent if reads respect the causal ordering of events.
That is, it must appear the operation that writes a value occurs after all
operations that causally proceed it. For example, imagine Alice adds a photo to
Facebook and then adds a reference to the photo to an album. If Bob sees the
photo reference in the album, then the photo being referenced must exist.
Causal+ consistent is weaker than linearizability and sequential consistency
but is stronger than causal, PRAM, per-key sequential, and eventual
consistency.

Causal consistency governs the behavior of causally related operations; it
doesn't help much to resolve causally unrelated operations. For example, when
two unrelated writes to the same object happen, a causally consistent system is
allowed to indefinitely return either value. Convergent conflict handling
requires that all replicas resolve the conflict in the same way using an
associative and commutative merge operator. COPS allows the user to specify
application specific merge operators but otherwise defaults to use a
last-writer-wins rule.

In COPS, values are versioned with the guarantee that if `x_i -> x_j` then `i <
j`. That is, values are versioned with Lamport timestamps. Moreover, once COPS
returns a value, it guarantees to return the same version of a causally later
one, so version numbers are monotonically non-decreasing.

**Design.**
A COPS system is divided between multiple data centers connected by a wide-area
network. Each data center represents one logical replica of the entire
key-value store. Within the data center, the key space is sharded between
servers and linearizability is provided by using chained replication. Updates
within a data center are asynchronously sent to other data centers in a causal+
consistent fashion.

COPS clients issue get and put operations to COPS. Furthermore, each operation
is associated with a context and each context can have at most one outstanding
operation at a time. The operations associated with a context for a logical
thread of execution. The context maintains a table of (key, version,
dependency) pairs. When a client issues a write, it includes a list of the
versions the write depends on (i.e. the list of versions that causally precede
this write). COPS servers block until the dependencies are themselves committed
to the database to ensure causal consistency.

Get operations in COPS are rather straightforward, but they are often
inexpressive. For example, imagine Alice unfriends Bob and then uploads a
private photo that she doesn't want Bob to see. Bob's client could read Alice's
old friend list, see that Bob is still her friend, and then read Alice's new
private photo. COPS-GT extends COPS with get transactions which allow multiple
keys to be read simultaneously in a causally consistent way. A get transaction
for keys `k1, ..., kn` proceeds in two steps:

1. First, the latest versions and dependencies of every key is read from COPS.
   For example, COPS may return `z_3 = 4` which depends on `x_2` and `y_2`.
2. Next, for every dependency `x_i` where `x` is a key in the transaction, if
   the value of `x` read is not at least as large as `i`, then another read is
   issued to read a value at least as large as `i`.

**Garbage Collection.**
There are miscellaneous pieces of metadata in COPS that can be garbage
collected. For example, COPS-GT maintains many versions of each key-value pair
so that a get transaction can issue a read request at older values. If we're
not careful, the size of this metadata could grow without bound. COPS bounds
the duration of get transactions to some arbitrary time `T`, restarting any
transactions that last longer than `T`. Then, once a new version of a value is
written, the old ones can be garbage collected after roughly `T` amount of
time.

**Fault Tolerance.**
Fault tolerance is provided using chain-replication. If a data center goes
down, data could be lost, but woe is the way of a highly available system.

## [Logic and Lattices for Distributed Programming (2012)](TODO) ##
**Summary.**
CRDTs provide eventual consistency without the need for coordination. However,
they suffer a *scope problem*: simple CRDTs are easy to reason about and use,
but more complicated CRDTs force programmers to ensure they satisfy semilattice
properties. They also lack composability. Consider, for example, a Students set
and a Teams set. (Alice, Bob) can be added to Teams while concurrently Bob is
removed from Students. Each individual set may be a CRDT, but there is no
mechanism to enforce consistency between the CRDTs.

Bloom and CALM, on the other hand, allow for mechanized program analysis to
guarantee that a program can avoid coordination. However, Bloom suffers from a
*type problem*: it only operates on sets which procludes the use of other
useful structures such as integers.

This paper merges CRDTs and Bloom together by introducing *bounded join
semilattices* into Bloom to form a new language: Bloom^L. Bloom^L operates over
semilattices, applying semilattice methods. These methods can be designated as
non-monotonic, monotonic, or homomorphic (which implies monotonic). So long as
the program avoids non-monotonic methods, it can be realized without
coordination. Moreover, morphisms can be implemented more efficiently than
non-homomorphic monotonic methods. Bloom^L comes built in with a family of
useful semilattices like booleans ordered by implication, integers ordered by
less than and greater than, sets, and maps. Users can also define their own
semilattices, and Bloom^L allows smooth interoperability between Bloom
collections and Bloom^L lattices. Bloom^L's semi-naive implementation is
comparable to Bloom's semi-naive implementation.

The paper also presents two case-studies. First, they implement a key-value
store as a map from keys to values annotated with vector clocks: a design
inspired from Dynamo. They also implement a purely monotonic shopping cart
using custom lattices.

## [Making Geo-Replicated Systems Fast as Possible, Consistent when Necessary (2012)](#making-geo-replicated-systems-fast-as-possible-consistent-when-necessary-2012)
Strong consistency is easy to reason about but hurts performance; weak
consistency is hard to reason about but can be implemented efficiently.
Allowing operations to run at different consistency levels is a first step
towards compromising between strong and weak consistency, but databases
that allow for mixed consistency operations place a burden on users to think
critically about which operations should run at which consistency level to
ensure application correctness. This paper makes three contributions:

1. It introduces *RedBlue Consistency*. Intuitively, red operations are
   synchronized while blue operations execute locally and are asynchronously
   replicated.
2. It provides sufficient conditions for which operations can be blue and which
   can be red while still ensuring the system is confluent and also
   invariant-confluent.
3. It introduces a novel way to decompose operations into side-effect free
   *generator operations* and *shadow operations* which are asynchronously
   applied at all sites. This decomposition improves the commutativity of the
   system allowing for more blue operations.

It also implements a RedBlue consistent database dubbed *Gemini*.

**Related Work.**
There are a huge number of databases that can be characterized by various
properties like low latency, causality, state convergence, single value, stable
histories, general operations, invariants, and eventual propagation.  Moreover,
there are a huge number of different consistency models like linearizability,
timeline consistency, snapshot consistency, fork consistency, and eventual
consistency.

**System Model.**
In our model, data is replicated across `k` sites. Each site has a state `S`
and executes operations `u, v`. We denote the application of an operation `u`
to a database state `S` as `S + u`. We say two operations `u, v` commute if for
all database stats `S`, `S + u + v = S + v + u`. We say a database state `S` is
valid if satisfies user's invariants. Every update is applied at a single site
before being propagated to other sites.

**RedBlue Consistency.**
Given disjoint sets of operations `R`, and `B`, a *RedBlue order* is a partial
order (U = R cup B, <) where elements in `R` are totally ordered. That is for
all `r, r'` in `R`, either `r < r'` or `r' < r`.

For a site `i`, an *i-causal serialization* of `O = (U, <)` is a total order
`(O, <<)`, where

1. `<<` extends `<`. That is for all `u, v` if `u < v` then `u << v`.
2. If `site(v) = i` and `u << v` then `u < v`. This implies that if `v` on `i`
   is concurrent with operation `u` not on `i`, then `v` must come before `u`
   in `<<`.

A system is *O-RedBlue consistent* if every site `i` runs an i-causal
serialization of `O`. Note that if all operations are red, then RedBlue
consistency is equivalent to serializability. If all operations are blue, then
it is equivalent to eventual consistency.

We say a RedBlue consistent system is *state convergent* if all causal
serializations reach the final state.  Note that not all RedBlue consistent
systems are state convergent. Two causal serializations could result in
different states. If all blue operations commute with all other operations
(i.e. they are *globally commutative*), then RedBlue consistency implies state
convergence.

**Replicating Side Effects.**
Some blue operations are not commutative but can be transformed into a
commutative alternative. We decompose an operation `u` into a generator
operation `g_u` and shadow operation `h_u(S)` where `S + g+u = S` and `S +
h_u(S) = S + u`. This is similar to how updates are decomposed in operation
based CRDTs.

Given a site `i`, a set of shadow operations `U` and the set of generator
operations `V_i` executed at `i`, an i-causal serialization of `O = (U, <)` is
a total order `O_i = (U cup V_i, <<)` where

- `<<` extends `<`.
- For any `h_v(S)` in `U` generated by `g_v` in `V_i`, `S` is the state
  obtained by applying all the shadow operations before `g_v`.
- For any `g_v` in `V_i` and `h_u(S)` in `U`, `h_u(S) << g_v` if and only if
  `h_u(S) < h_v(S')`. Again this says that if `h_u(S)` and `h_v(S')` are
  concurrent, then `g_v` and `h_v(S')` should come first.

Note that even if a RedBlue consistent system is state convergent, user
invariants can still be broken.  We say shadow operation `h_u(S)` is *invariant
safe* if for all valid stats `S` and `S'`, `S' + h_u(S)` is valid. In words,
replicating operations on valid states to other valid states preserves
validity. If all blue operations are invariant safe and globally commutative
then no site will ever enter an invalid state and sites will converge. This
gives us guidelines on what operations can be blue and which must be red.

- All non-commutative pairs of operations should be red.
- All invariant violating operations should be red.
- All other operations can be blue.

**Gemini.**
Gemini is a 10K line Java prototype implemented on top of MySQL. Each node in
Gemini consists of a storage engine, proxy server, concurrency coordinator, and
data writer. Users contact the proxy which executes the generator operations
and delivers shadow operations to the concurrency coordinator. It uses
optimistic concurrency control and vector clocks to ensure causal consistency.
It is not fault tolerant.

## [Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing (2012)](TODO) ##
**Summary.**
Frameworks like MapReduce made processing large amounts of data easier, but
they did not leverage distributed memory. If a MapReduce was run iteratively,
it would write all of its intermediate state to disk: something that was
prohibitively slow. This limitation made batch processing systems like
MapReduce ill-suited to *iterative* (e.g. k-means clustering) and *interactive*
(e.g. ad-hoc queries) workflows. Other systems like Pregel did take advantage
of distributed memory and reused the in-memory data across computations, but
the systems were not general-purpose.

*Spark* uses *Resilient Distributed Datasets* (RDDs) to perform general
computations in memory. RDDs are immutable partitioned collections of records.
Unlike pure distributed shared memory abstractions which allow for arbitrary
fine-grained writes, RDDs can only be constructed using coarse-grained
transformations from on-disk data or other RDDs. This weaker abstraction can be
implemented efficiently. Spark also uses RDD lineage to implement low-overhead
fault tolerance. Rather than persist intermediate datasets, the lineage of an
RDD can be persisted and efficiently recomputed. RDDs could also be
checkpointed to avoid the recomputation of a long lineage graph.

Spark has a Scala-integrated API and comes with a modified interactive
interpreter. It also includes a large number of useful *transformations* (which
construct RDDs) and *actions* (which derive data from RDDs). Users can also
manually specify RDD persistence and partitioning to further improve
performance.

Spark subsumed a huge number of existing data processing frameworks like
MapReduce and Pregel in a small amount of code. It was also much, much faster
than everything else on a large number of applications.

## [Apache Hadoop YARN: Yet Another Resource Negotiator (2013)](https://scholar.google.com/scholar?cluster=3355598125951377731&hl=en&as_sdt=0,5)
**Summary.**
Hadoop began as a MapReduce clone designed for large scale web crawling. As big
data became trendy and data became... big, Hadoop became the de facto
standard data processing system, and large Hadoop clusters were installed in
many companies as "the" cluster. As application requirements evolved, users
started abusing the large Hadoop in unintended ways. For example, users would
submit map-only jobs which were thinly guised web servers. Apache Hadoop YARN
is a cluster manager that aims to disentangle cluster management from
programming paradigm and has the following goals:

- Scalability
- Multi-tenancy
- Serviceability
- Locality awareness
- High cluster utilization
- Reliability/availability
- Secure and auditable operation
- Support for programming model diversity
- Flexible resource model
- Backward compatibility

YARN is orchestrated by a per-cluster *Resource Manager* (RM) that tracks
resource usage and node liveness, enforces allocation invariants, and
arbitrates contention among tenants. *Application Masters* (AM) are responsible
for negotiating resources with the RM and manage the execution of single job.
AMs send ResourceRequests to the RM telling it resource requirements, locality
preferences, etc. In return, the RM hands out *containers* (e.g.  <2GB RAM, 1
CPU>) to AMs. The RM also communicates with Node Managers (NM) running on each
node which are responsible for measuring node resources and managing (i.e.
starting and killing) tasks. When a user want to submit a job, it sends it to
the RM which hands a capability to an AM to present to an NM. The RM is a
single point of failure. If it fails, it restores its state from disk and kills
all running AMs. The AMs are trusted to be faul-tolerant and resubmit any
prematurely terminated jobs.

YARN is deployed at Yahoo where it manages roughly 500,000 daily jobs. YARN
supports frameworks like Hadoop, Tez, Spark, Dryad, Giraph, and Storm.

## [Discretized Streams: Fault-Tolerant Streaming Computation at Scale (2013)](https://scholar.google.com/scholar?cluster=4915408824809554257&hl=en&as_sdt=0,5)
**Summary.**
Many big data applications necessitate streaming (aka real-time, interactive,
or low-latency) data processing. For example

- a social network like Facebook may want to determine trending topics,
- a site like Twitter may want to detect recent spam, or
- a service operator may want to continuously monitor logs for failures.

There are many streaming data processing systems out there (e.g. Storm,
TimeStream, MapReduce Online, S4) and most are based on the *continuous
operator model*. In this model, computation is modelled as a network of
stateful operators that receive data from an upstream operator, update their
internal state, and send data to downstream operators. The continuous operator
model typically employs one of two recovery mechanisms.

1. *Hot replication.* Operators are replicated and data is sent to every one of
   the replicas. Synchronization between replicas can ensure they produce
   equivalent results. Replication has reasonably fast recovery, but it doubles
   deployment cost (at least) and operator synchronization can slow down
   processing.
2. *Upstream backup.* Operators buffer their outputs until downstream consumers
   checkpoint. When a downstream operator fails, a new operator is launched and
   its buffered inputs are sent from the upstream backup. Upstream backup
   doesn't greatly increase deployment cost, but recovery can be slow.

In this paper, Zaharia et al. introduce a new stream processing model,
*discretized streams* (D-Streams), that recovers quickly, handles stragglers,
scales to hundreds of nodes, and aims to support latencies of 0.5 - 2 seconds.
In this model, streams are divided into *stateless, determinstic batch
computations on small time intervals*.  Determinism and lineage tracking allow
for parallel recovery. Moreover, D-Streams are implemented in Spark Streaming
allowing a unified interface between batch and streaming computation: something
of great practical value.

In the discretized streams model, all the data for a given time interval is
collected into an RDD, tasks then operate on the RDD, much like they would in
Spark, to produce outputs (possibly to external systems) or temporary state
that is fed into the next time interval.

- *Timing.* Spark Streaming batches events based on their *arrival time*,
  though some systems may want to batch events based on some external
  *timestamp*. Spark Streaming has some configurable slack time to wait before
  processing a batch, but otherwise this functionality can be solved by
  applications.
- *API.* The D-Stream interface is very similar to that of Spark but also
  includes windowing, incremental aggregation, tracking, and data exporting.
- *Consistency.* Some implementations of the continuous operator model have
  ambiguous consistency guarantees. Some operators can run ahead of others
  giving an inconsistent view of the system. D-Streams design makes consistency
  clear. D-Streams are discretized by interval, so once an RDD for a given
  interval is complete, it is a consistent snapshot of the system.
- *Integration with batch.* Batch and stream processing are married by the
  integration of Spark Streaming into Spark. This allows users to do things
  like join streaming data with a static RDD or issue interactive queries to a
  streaming job.

The Spark Streaming architecture is divided between three main components:

1. The *master* tracks lineage and schedules tasks.
2. The *workers* receive data, store partitions, and execute tasks.
3. The *client* injects data into the system.

Data is either periodically read from a data store like HDFS or is sent into
the system by a client. Workers run an LRU block store to manage RDD partitions
and coordinate with the master which tracks which nodes have which blocks. The
system employs traditional data processing optimizations and also introduces
optimizations to I/O, pipelining, scheduling, etc. Moreover, since streaming
computations run 24/7, Spark Streaming had to introduce master recovery. Master
periodically write their state to HDFS which can be recovered by another
master.

## [From L3 to seL4 What Have We Learnt in 20 Years of L4 Microkernels? (2013)](https://scholar.google.com/scholar?cluster=10669327659248325680&hl=en&as_sdt=0,5)
**Summary.**
Inter process communication (IPC) was on the critical path of microkernels like
Mach. Traditionally, each IPC took roughly 100 microseconds and was considered
a bit high. In 1993, Jochen Liedtke introduced the L4 microkernel and showed
that IPC could be implemented 10 to 20 times faster. In the next 20 years, L4
developed a rich family of descendants and many lessons have been learned.

Lessons in design:

- *Minimality.* All functionality that can live outside the kernel should live
  outside the kernel. Moreover, all policy should be lifted out of the kernel
  leaving only mechanisms.
- *Synchronous IPC.* The original L4 implementation used synchronous RPC in
  which both the sender and receiver block until the sender calls send and the
  receiver calls receive. This can be implemented efficiently, but forces
  programs to be multithreaded in order to handle multiple inputs. Later
  version of L4 implemented asynchronous IPC in which a sender could send
  without blocking and a receiver could block or poll the receipt of a message.
- *IPC message structure.* The original L4 implementations allowed processes to
  send messages directly via physical registers. This technique was limited by
  the number of physical registers and was a bit clunky. Later version of L4
  introduced virtual registers which were either backed by physical registers
  or by a pinned page of memory.
- *IPC destinations.* Originally, IPC destinations were thread identifiers, but
  this exposed a great deal of information. Later, IPC destinations were
  changed to more port-like endpoints.
- *IPC timeouts.* If a client and server are exchanging messages via IPC, then
  a client can send a message, the server will receive the message and issue a
  send response. If the client never calls receive, then the server is blocked.
  This allows clients to DOS servers. To avoid this, tasks can associate a
  timeout with an IPC call. Originally timeouts could be set at 0, infinity, or
  anywhere from microseconds to weeks. In reality, most programs only used 0 or
  infinity.
- *Communication control.* Tasks were organized into groups called *clans* and
  every clan had a designated *chief*. Messages within a clan could flow
  freely, but messages between clans had to be forwarded through the chief.
  This was weird and later dropped.
- *User-level device drivers.* Putting device drivers in user space has
  remained a good idea.
- *Process hierarchy.* A finite number of task IDs was allocated and
  distributed in a first come first serve fashion. This was later replaced with
  a capability based system.
- *Time.* L4 implements a round-robin priority scheduler. People have tried to
  move the scheduling into user space, but it has yet to be done without
  significant overhead.

Lessons in implementation:

- *Process orientation and virtual TCB array.* Threads' kernel stack was
  allocated above their TCB.
- *Lazy scheduling.* When a thread blocked, it was marked as blocked but not
  yet removed from the ready queue. When the scheduler selected the next task
  to run, it would iterate through the ready queue until it found an actually
  ready task. This meant that scheduling time was bounded only by the number of
  threads. This was later replaced by Benno scheduling.
- *Direct process switch.* When a thread blocked on IPC, the scheduler would
  immediately run another task in its quantum regardless of priority. This
  direct process switch mechanism was kept but made to respect priorities.
- *Preemption.* Most L4 implementations have a nonpreemptable kernel with
  strategic preemption points.
- *Non-portability.* L4 used to be platform specific but now is very portable.
- *Non-standard calling convention.* To get maximum performance, L4 was written
  in assembly and uses weird calling conventions. When implementations moved to
  C and C++, this was removed.
- *Implementation language.* Implementation moved to C and a bit of C++.

## [Innovative Instructions and Software Model for Isolated Execution (2013)](https://scholar.google.com/scholar?cluster=11948934428694485446&hl=en&as_sdt=0,5)
**Summary.**
Applications are responsible for managing an increasing amount of sensitive
information. Intel SGX is a set of new instructions and memory access changes
that allow users to put code and data into secured *enclaves* that are
inaccessible even to privileged code. The enclaves provide confidentiality,
integrity, and isolation.

A process' virtual memory space is divided into different sections. There's a
section for the code, a section for the stack, a section for the heap, etc. An
enclave is just another region in the user's address space, except the enclave
has some special properties. The enclave can store code and data. When a
process is running code in the enclave, it can access data in the enclave.
Otherwise, the enclave data is off limits.

Each enclave is composed of a set of pages, and these pages are stored in the
*Enclave Page Cache (EPC)*.

    +---------------------------------+
    | .-----. .-----. .-----. .-----. |
    | |    \| |    \| |    \| |    \| |
    | |     | |     | |     | |     | | EPC
    | |     | |     | |     | |     | |
    | '.....' '.....' '.....' '.....' |
    +---------------------------------+

In addition to storing enclave pages, the EPC also stores SGX structures (I
guess Enclave Page and SGX Structures Cache (EPSSC) was too long of an
acronym). The EPC is protected from hardware and software access. A related
structure, the *Enclave Page Cache Map* (EPCM), stores a piece of metadata for
each active page in the EPC. Moreover, each enclave is assigned a *SGX Enclave
Control Store* (SECS). There are instructions to create, add pages to, secure,
enter, and exit an enclave.

Computers have a finite, nay scarce amount of memory. In order to allow as many
processes to operate with this scarce resource, operating systems implement
paging. Active pages of memory are stored in memory, while inactive pages are
flushed to the disk. Analogously, in order to allow as many processes to use
enclaves as possible, SGX allows for pages in the EPC to be paged to main
memory. The difficulty is that the operating system is not trusted and neither
is main memory.

    +---------------------------------+
    | .-----. .-----.         .-----. |
    | |    \| |    \|         |    \| |
    | | VA  | |  ^  |    |    |     | | EPC (small and trusted)
    | |     | |  |  |    |    |     | |
    | '.....' '..|..'    |    '.....' |
    +------------|-------|------------+
                 |       | (paging)
    +------------|-------|-----------------------
    | .-----.    |    .--|--. .-----. .-----.
    | |    \|    |    |  | \| |    \| |    \|
    | |     |    |    |  v  | |     | |     | ... main memory (big but not trusted)
    | |     |         |     | |     | |     |
    | '.....'         '.....' '.....' '.....'
    +--------------------------------------------

In order to page an EPC page to main memory, all cached translations that point
to it must first be cleared. Then, the page is encrypted. A nonce, called a
version, is created for the page and put into a special *Version Array* (VA)
page in the EPC. A MAC is taken of the encrypted contents, the version, and the
page's metadata and is stored with the file in main memory. When the page is
paged back into the EPC, the MAC is checked against the version in the VA
before the VA is cleared.

## [MillWheel: Fault-Tolerant Stream Processing at Internet Scale (2013)](https://scholar.google.com/scholar?cluster=11192973635829532709&hl=en&as_sdt=0,5)
**Summary.**
MillWheel is a stream processing system built at Google that models computation
as a dynamic directed graph of *computations*. MillWheel allows user's to write
arbitrary code as part of an operation yet still transparently enforces
idempotency and exactly-once message delivery. MillWheel uses frequent
checkpointing and upstream backup for recovery.

Data in MillWheel is represented by (key, value, timestamp) triples. Values and
timestamps are both arbitrary. Keys are extracted from records by user provided
key extraction functions. Computations operate on inputs and the computations
for a single key are serialized; that is, no two computations on the same key
will every happen at the same time. Moreover, each key is associated with some
persistent state that a computation has access to when operating on the key.

MillWheel also supports *low watermarks*. If a computation has a low watermark
of `t`, then it's guaranteed to have processed all records no later than `t`.
Low watermarks use the logical timestamps in records as opposed to arrival time
in systems like Spark Streaming.  Low watermark guarantees are not actually
guarantees; they are approximations.  *Injectors* inject data into MillWheel
and can still violate low watermarks semantics. When a watermark violating
record enters the system, computations can choose to ignore it or try to
correct it. Moreover, the MillWheel API allows users to register for code,
known as *timers*, to execute at a certain wall clock or low watermark time.

MillWheel guarantees messages are logically sent exactly once. Messages may be
sent multiple times within the system, but measures are taken to ensure that
the system appears to have delivered them only once. Similar care must be taken
to ensure that per-key persistent state is not updated erroneously. When a
message is received at a computation, the following events happen:

- The data is deduplicated by checking first against a Bloom filter and then a
  disk.
- User code is executed.
- Pending changes to the persistent state are committed.
- Senders are acknowledged.
- Produced data is sent downstream.

Imagine if a computation commits a change to persistent state but crashes
before acking senders. If the computation is replayed, it may modify the
persistent state twice. To avoid this, each record in the system is given a
unique id. When state is modified in response to a record, the id of the record
is atomically written with the state change. If a computation later
re-processes the record and attempts to re-modify the state, the modification
is ignored.

Before a record is sent downstream, it is checkpointed. These are called
*strong productions*. Programs can opt out of strong productions and instead
use *weak productions*: non-checkpointed productions. When using weak
productions, MillWheel still performs occasional checkpoints to avoid a long
chain of acknowledgements to be blocked by a straggling worker. Programs can
also opt-out of exactly-once semantics.

To avoid zombie writers from corrupting state, each write is associated with a
sequencer token that is invalidated when a new writer becomes active.

MillWheel is implemented using a replicated master that manages key-range
assignments and a single central authority for computing low watermarks.

**Commentary.**
- Spark Streaming claims that using replication for recovery reduces the
  latency of the system. MillWheel frequently checkpoints data. This leads me
  to wonder if MillWheel experiences a latency hit, but the evaluation only
  considers single stage pipelines!
- The evaluation does not evaluate the recovery time of the system, something
  that Spark Streaming would say is very slow.
- The paper also says the Spark Streaming model is limiting and depends heavily
  on the RDD API. I wish there were a concrete example demonstrating Spark
  Streaming's inexpressiveness.
- I'm having trouble understanding how the per-key API allows for things like
  joins.

## [Naiad: A Timely Dataflow System (2013)](https://scholar.google.com/scholar?cluster=2514717880214148696&hl=en&as_sdt=0,5)
Naiad is a data-parallel dataflow framework that supports *low-latency*, *high
throughput*, *iterative and incremental computations*, and *consistent
intermediate results*. There are a lot of batch processing systems, stream
processing systems, graph processing systems, etc. out there. Most of these
systems satisfy most of these properties. Naiad achieves *all* of them in a
single system making things more efficient, succinct, and maintainable. Naiad
uses a new computation model, *timely dataflow*, which includes the following
features:

1. structured loops which allow iterative computation and feedback,
2. stateful vertices, and
3. vertex notifications for the end of a epoch or the end of an iteration.

**Timely Dataflow.**
Computation in the timely dataflow model is expressed as a directed, possibly
cyclic graph of stateful vertices and edges that deliver messages. Messages are
annotated with logical timestamps that allow vertices to distinguish messages
from different epochs and loop iterations. Moreover, vertices are notified when
all of the messages of a given timestamp have been delivered.

*Graph Structure.*
A timely dataflow graph has specially designated *input vertices* and *output
vertices*. Inputs are pumped into the input vertices by an external system, and
all inputs are tagged with an epoch. The output vertices pump outputs to an
external system, and they also tag outputs with epochs.

Timely dataflow graphs can contain cycles, but not arbitrary cycles. Cycles
must be organized into possibly nested structures called *loop contexts*. Each
loop context contains three specially designated vertices:

1. an *ingress vertex* at the beginning of the loop,
2. an *egress vertex* at the end of the loop, and
3. a *feedback vertex* along a cycle within the loop context.

Each message in a timely dataflow is labelled with a logical timestamp of the
form `(e, (c_1, ..., c_k))` where `e` is an epoch and `(c_1, ..., c_k)` is a
list of `k` loop counters. As data flows through a loop context, the three
special nodes adjust the timestamp as follows:

- ingress: `(e, <c_1, ..., c_k>) -> (e, <c_1, ..., c_k, 0>)`
- egress: `(e, <c_1, ..., c_k, c_{k+1}>) -> (e, <c_1, ..., c_k>)`
- feedback: `(e, <c_1, ..., c_k>) -> (e, <c_1, ..., c_k + 1>)`

Timestamps are compared pairwise and loop counters are compared
lexicographically.

*Vertex Computation.*
Vertices implement two callbacks:

- `v.OnRecv(e: Edge, m: Message, t: Timestamp)`
- `v.OnNotify(t: Timestamp)`

and can invoke two library functions:

- `this.SendBy(e: Edge, m: Message, t: Timestamp)`
- `this.NotifyAt(t: Timestamp)`

`OnRecv` is called when a message is received. `SendBy` is used to send a
message. `v.NotifyAt(t)` causes a `v.OnNotify(t)` timestamp to be called
later once all messages with timestamp less than or equal to `t` have been
delivered. `OnRecv` and `OnNotify` can run arbitrary code so long as they don't
send messages back in time.

*Achieving Timely Dataflow.*
Here, we outline how to implement timely dataflow in a single-threaded process.
Let an *event* be a message or notification request. Each event in a timely
dataflow has a position and timestamp, dubbed a *pointsamp*, of the form `(t:
Timestamp, l: Edge | Vertex)`. Specifically,

- a `v.SendBy(e, m, t)` produces an event with pointstamp `(t, e)`, and
- a `v.NotifyAt(t)` produces an event with pointstamp `(t, v)`.

Define a *could-result-in* relation where `(t1, l1)` could-result-in `(t1, l2)`
if there exists a path of vertices and edges `psi = (l1, ..., l2)` where
`psi(t1) <= t2`. That is, there is some path from `l1` to `l2` which updates
`t1` to a timestamp less than `t2`. There may be multiple paths from `l1` to
`l2`, so the system maintains a map `Psi[l1, l2]` of the minimum path.

Our single-threaded timely dataflow scheduler maintains a set of *active
pointstamps*: the set of pointstamps for which there is a pending event. Each
active pointstamp is mapped to an *occurrence count* (the number of pending
events with this particular pointstamp) and a *precursor count* (the number of
active pointstamps that could-result-in this one). When an new pending event is
created, the occurrence count is updated and it's initial precursor count is
created. Moreover, the precursor counts of downstream nodes are updated. All
nodes with a precursor count of 0 are said to be on the frontier, and the
system can deliver notifications on the frontier. Finally, input vertices set
up the appropriate active pointstamps which update as data is fed into the
system.

**Distributed Implementation.**
Naiad is a distributed implementation of the timely dataflow model. Like other
batch and stream processing systems, Naiad uses data parallelism to increase
the aggregate computation, memory, and bandwidth of the system. A logical
dataflow graph is compiled and expanded a physical dataflow graph which
includes physical details like the partitioning along each edge.

Workers are responsible for delivering the messages and notifications to the
vertices it runs. Each vertex is run single-threaded, and if a vertex sends a
message to anther vertex in the same worker, the worker can immediately
transfer control between the two vertexes. Vertexes can specify a level of
re-entrancy but are otherwise assumed to be non-reentrant.

Workers also participate in a global progress tracking protocol. Workers
maintain local occurrence and precursor counts. When a worker invokes one of
the four functions above, it broadcasts updates to the occurrence counts. The
protocol maintains the invariant that if some pointstamp `p` is on the local
frontier at a vertex, it is on the global frontier as well. As an optimization,
the active pointstamps are maintained on the global graph instead of the
physical graph. Deltas are also accumulated and buffered.

For fault tolerance, stateful vertices implement `Checkpoint` and `Restore`
methods. Naiad uses these to orchestrate global checkpoints and restorations.

Micro-stragglers---nodes with unusually high latency---are the main obstacle
for a low-latency system. Micro-stragglers can come about from TCP overheads,
data structure contention, garbage collection etc. Naiad has mechanisms to
mitigate these micro-stragglers.

**Writing Naiad Programs.**
Naiad provides lower-level interfaces for constructing timely dataflow graphs.
Higher-level libraries and interfaces (e.g. SQL, LINQ) can be built on top of
this lower-level interface. This separation of library code from system code
allows for greater flexibility.

## [Replicated Data Consistency Explained Through Baseball (2013)](https://scholar.google.com/scholar?cluster=3008756295145383805&hl=en&as_sdt=0,5)
Different cloud providers provide cloud storage with different mixes of
consistency:

- *Azure* provides strong consistency.
- *Amazon S3* provides eventual consistency.
- *Amazon DynamoDB* provides eventual and strong consistency.
- *Amazon SimpleDB* provides eventual and strong consistency.
- *Google App Engine Datastore* provides eventual and strong consistency.
- *PNUTS* provides read-any, read-critical, and read-latest.
- Some systems provide quorum reads.

Cloud providers are motivated to provide multiple consistency guarantees due to
the inherit tradeoff between consistency, performance, and availability. This
paper presents six consistency models in plain English and motivates their use
with a baseball application.

**Read Consistency Guarantees.**
Assuming writes linearized using something like Paxos, we define six read
consistency guarantees:

- *Strong consistency (SC).* Reads reflect all writes at the point of the read.
- *Eventual consistency (EC).* Reads reflect some subset of the writes that
  happened at the point of the read.
- *Consistent prefix (CP).* Reads reflect some prefix of writes at the point of
  the read. This is most useful for multi-object reads.
- *Bounded staleness (BS).* Reads reflect all writes before some time `T`
  before the point of the read.
- *Monotonic reads (MR).* This session guarantee says that repeated reads
  reflect monotonically non-decreasing sets of writes.
- *Read my writes (RMW).* Reads followed by writes reflect the values written.

These consistency guarantees can be organized using the following partial
order:

```
    SC
CP BS MR RMW
    EC
```

**Baseball as a Sample Application.**
Imagine a key-value store which provides all of the read consistency guarantees
outlined above. Consider a baseball application that writes home and visitor
runs into two separate variables in this key value store. Consider the following game:

- `Write("home", 1)`
- `Write("visitors", 1)`
- `Write("home", 2)`
- `Write("home", 3)`
- `Write("visitors", 2)`
- `Write("home", 4)`
- `Write("home", 5)`

|          | 1   | 2   | 3   | 4   | 5   | 6   | 7   | 8   | 9   | RUNS   |
| -------- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ------ |
| Visitors | 0   | 0   | 1   | 0   | 1   | 0   | 0   |     |     | 2      |
| Home     | 1   | 0   | 1   | 1   | 0   | 2   |     |     |     | 5      |

The read consistency guarantees allow for the following reads:

| Read Guarantee | Permitted Reads                                      |
| -------------- | ---------------------------------------------------- |
| SC             | `2-5`                                                |
| EC             | `{0, 1, 2}x{0, 1, 2, 3, 4, 5}`                       |
| CP             | `0-0`,`0-1`,`1-1`,`1-2`,`1-3`,`2-3`,`2-4`,`2-5`      |
| BS             | 1-inning: `2-3`,`2-4`,`2-5`                          |
| MR             | `1-3`: `1-3`,`1-4`,`1-5`,`2-3`,`2-4`,`2-5`           |
| RMW            | writer: `2-5`; other: `{0, 1, 2}x{0, 1, 2, 3, 4, 5}` |

**Read Requirements for Participants.**
Different users of the baseball database have different consistency
requirements:

- *Official scorekeeper.* The official scorekeeper is the sole person
  responsible for updating the scores (e.g. `Write("visitors", Read("visitors")
  + 1)`). The scorekeeper requires strongly consistent writes, but since they
  are the only one writing, they can get by with RMW instead of SC.
- *Umpire.* The umpire reads the score after the top of the 9th inning to see
  if the home team is winning. This requires SC.
- *Radio reporter.* The radio reporter reports the current score of the game
  every 30 minutes. The score doesn't have to be fresh, but it should be a
  score that existed at some point. Moreover, the score should not time travel
  backwards. We can accomplish this with CP and MR. Alternatively, we can use
  30-minute BS.
- *Sports writer.* The sports writer writes a report of the game one hour after
  the game is over. 1 hour BS suffices.
- *Statistician.* The statistician maintains the total yearly runs. They
  require SC to read the daily runs but only RMW to read the yearly runs.
- *Stat watcher.* Stat watchers read the stats every day or so. Eventual
  consistency suffices.

**Lessons.**
- All of the six presented consistency guarantees are useful.
- Different clients may want different consistencies even when accessing the
  same data.
- Even simple databases may have diverse users with different consistency
  needs.
- Clients should be able to choose their desired consistency.

## [Automating the Choice of Consistency Levels in Replicated Systems (2014)](https://scholar.google.com/scholar?cluster=5894108460168532172&hl=en&as_sdt=0,5)
Geo-replicated data stores that replicate data have to choose between incurring
the performance overheads of implementing strong consistency or the brain
boggling semantics of weak consistency. Some data stores allow users to make
this decision on a fine grained level, allowing some operations to operate with
strong consistency while other operations operate under weak consistency. For
example, [*Making Geo-Replicated Systems Fast as Possible, Consistent when
Necessary*](https://scholar.google.com/scholar?cluster=4316742817395056095&hl=en&as_sdt=0,5)
introduced RedBlue consistency in which users annotate operations as red
(strong) or blue (weak). Typically, the process of choosing the consistency
level for each operation has been a manual task. This paper builds off of
RedBlue consistency and automates the process of labeling operations as red or
blue.

**Overview.**
When using RedBlue consistency, users can decompose operations into generator
operations and shadow operations to improve commutativity. This paper automates
this process by converting Java applications that write state to a relational
database to issue operations against CRDTs. Moreover, it uses a combination of
static analysis and runtime checks to ensure that operations are invariant
confluent.

**Generating Shadow Operations.**
Relations are just sets of tuples, so we can model them as CRDT sets. Moreover,
we can model individual fields as CRDTs. This paper presents field CRDTs
(PN-Counters, LWW-registers) and set CRDTs to model relations. Users then
annotate SQL create statements indicating which CRDT to use.

Using a custom JDBC driver, user operations can be converted into a
corresponding shadow operation that issues CRDT operations.

**Classification of Shadow Operations.**
We want to find the database states and transaction arguments that guarantee
invariant-confluence. This can be a very difficult (probably undecidable)
problem in general. Instead, we simplify the problem by considering the
possible trace of CRDT operations through a program. Even with this simplifying
assumption, tracing execution through loops can be challenging. We can again
simplify things by only considering loop where each iteration is independent of
one another.

We model each program as a regular expression over statements. We then unroll
the regular expression to get the set of all execution traces. Using these
traces we can construct a map from traces, or templates, to weakest
preconditions that ensure invariant confluence.

At runtime, we need only look up the weakest precondition given an execution
trace and check that it is true.

**Implementation.**
The system is implemented as 15K lines of Java and 533 lines of OCaml. It
builds off of MySQL, an existing Java parser, and Gemini.

## [Coordination Avoidance in Database Systems (2014)](https://scholar.google.com/scholar?cluster=428435405994413003&hl=en&as_sdt=0,5)
**Overivew.**
Coordination in a distributed system is sometimes necessary to maintain
application correctness, or *consistency*. For example, a payroll application
may require that each employee has a unique ID, or that a jobs relation only
include valid employees. However, coordination is not cheap.  It increases
latency, and in the face of partitions can lead to unavailability.  Thus, when
application correctness permits, coordination should be avoided. This paper
develops the necessary and sufficient conditions for when coordination is
needed to maintain a set of database invariance using a notion of
invariant-confluence or I-confluence.

**System Model.**
A *database state* is a set D of object versions drawn from the set of all
states *D*. Transactions operate on *logical replicas* in *D* that contain the
set of object versions relevant to the transaction. Transactions are modeled as
functions T : *D* -> *D*. The effects of a transaction are merged into an
existing replica using an associative, commutative, idempotent merge operator.
Changes are shared between replicas and merges likewise. In this paper, merge
is set union, and we assume we know all transactions in advance. Invariants are
modeled as boolean functions I: *D* -> 2. A state R is said to be I-valid if
I(R) is true.

We say a system has *transactional availability* if whenever a transaction T
can contact servers with the appropriate data in T, it only aborts if T chooses
to abort. We say a system is *convergent* if after updates quiesce, all servers
eventually have the same state. A system is globally *I-valid* if all replicas
always have I-valid states. A system provides coordination-free execution if
execution of a given transaction does not depend on the execution of others.

**Consistency Sans Coordination.**
A state Si is *I-T-Reachable* if its derivable from I-valid states with
transactions in T. A set of transactions is *I*-confluent with respect to
invariant I if for all I-T-Reachable states Di, Dj with common ancestor Di join
Dj is I-valid. A globally I-valid system can execute a set of transactions T
with global validity, transactional availability, convergence, and
coordination-freedom if and only if T is I-confluent with respect to I.

**Applying Invariant-Confluence.**
I-confluence can be applied to existing relation operators and constraints. For
example, updates, inserts, and deletes are I-confluent with respect to
per-record inequality constraint. Deletions are I-confluent with respect to
foreign key constraints; additions and updates are not. I-confluence can also
be applied to abstract data types like counters.

## [Highly Available Transactions: Virtues and Limitations (2014)](TODO) ##
**Summary.**
Serializability is the gold standard of consistency, but databases have always
provided weaker consistency modes (e.g. Read Committed, Repeatable Read) that
promise improved performance. In this paper, Bailis et al. determine which of
these weaker consistency models can be implemented with high availability.

First, why is high availability important?

1. *Partitions.* Partitions happen, and when they do non-available systems
   become, well, unavailable.
2. *Latency.* Partitions may be transient, but latency is forever. Highly
   available systems can avoid latency by eschewing coordination costs.

Second, are weaker consistency models consistent enough? In short, yeah
probably. In a survey of databases, Bailis finds that many do not employ
serializability by default and some do not even provide full serializability.
Bailis also finds that four of the five transactions in the TPC-C benchmark can
be implemented with highly available transactions.

After defining availability, Bailis presents the taxonomy of which consistency
can be implemented as HATs, and also argues why some fundamentally cannot. He
also performs benchmarks on AWS to show the performance benefits of HAT.

## [In Search of an Understandable Consensus Algorithm (2014)](https://scholar.google.com/scholar?cluster=12646889551697084617&hl=en&as_sdt=0,5)
Modelling a distributed system as a replicated state machine provides the
illusion that the distributed system is really just a single machine. At the
core of the replicated state machine approach is a replicated log that is kept
consistent by a consensus algorithm. Traditionally, consensus has been
synonymous with Paxos. Paxos is taught in schools, and most consensus algorithm
implementations are based on Paxos. However, Paxos has two main disadvantages:

1. It is *hard to understand*. Single-decree Paxos is nuanced, and composing
   single-decree Paxos into multi-Paxos is confusing.
2. It is *hard to implement efficiently*. Multi-Paxos is not very well
   described in the literature, and the algorithm is difficult to implement
   efficiently without modification.

This paper presents the Raft consensus algorithm. Raft provides the same
performance and safety as multi-Paxos but it is designed to be much easier to
understand.

**Basics.**
Every node in a raft cluster is in one of three states: *leader*, *follower*,
or *candidate*. The leader receives requests from users and forwards them to
followers. Followers are completely passive and receive messages from leaders.
Candidates perform leader elections in an attempt to become a leader. In normal
operation, there is a single leader, and every other node is a follower.

Raft proceeds in a series of increasingly numbered terms. Each term consists of
a leader election followed by (potentially) normal operation. There is exactly
one leader elected per term. Moreover, each node participates in monotonically
increasing terms. When a node sends a message in Raft, it annotates it with its
term. If a leader receives a message from a later term, it immediately becomes
a follower. Nodes ignore messages annotated with older terms.

Raft uses two RPCs: RequestVote (for leader election) and AppendEntries (for
replication and heartbeats).

**Leader Election.**
Leaders periodically send heartbeats (AppendEntries RPCs without any entries)
to followers. As long as a follower continues to receive heartbeats, it
continues to be a follower. If a follower does not receive a heartbeat after a
certain amount of time, it begins leader election: it increments its term,
enters the candidate state, votes for itself, and sends RequestVote RPCs in
parallel to all other nodes. Either,

1. *It wins.* Nodes issue a single vote per term on a first come first serve
   basis. If a candidate receives a vote from a majority of the nodes, then it
   becomes leader.
2. *It hears from another leader.* If a candidate receives a message from
   another leader in a term at least as large as it, it becomes a follower.
3. *It times out.* It's possible that a split vote occurs and nobody becomes
   leader in a particular term. If this happens, the candidate times out after
   a certain amount of time and begins another election in the next term.

**Log Replication.**
During normal operation, a leader receives a request from a client, appends it
to its log annotated with the current term, and issues AppendEntries to all
nodes in parallel. An entry is considered *committed* after it is replicated to
a majority of the nodes. Once a log entry is committed, all previous log
entries are also committed. Once a log entry is committed, the leader can apply
it and respond to the user. Moreover, once an entry is committed, it is
guaranteed to eventually execute at all available nodes. The leader keeps track
of the index of the largest committed entry and sends it to all other nodes so
that they can also apply log entries.

Raft satisfies a powerful *log matching invariant*:

1. "If two entries in different logs have the same index and term, then they
   store the same command."
2. "If two entries in different logs have the same index and term, then the
   logs are identical in all preceding entries."

1 is ensured by the fact that a single leader is elected for any given term,
the fact that a leader only creates a single log entry per index, and the fact
that once a log entry is created, it never changes index. 2 is ensured by a
runtime check. When a leader sends an AppendEntries RPC for a particular index,
it also sends its log entry for the previous index. The follower only applies
the AppendEntries RPC if it agrees on the previous index. Inductively, this
guarantees 2.

Followers may have missing or extraneous log entries. When this happens, the
leader identifies the longest prefix on which the two agree. It then sends the
rest of its log. The follower overwrites its log to match the leader.

**Safety.**
The protocol described so far is unsafe. If a new leader is elected, it can
accidentally force followers to overwrite committed values with uncommitted
values. Thus, we must ensure that leaders contain all committed entries. Other
consensus algorithms ensure this by shipping committed values to newly elected
leaders. Raft takes an alternative approach and guarantees that if a leader is
elected, it has every committed entry. To ensure this, Raft must restrict which
nodes can be elected.

A follower rejects a RequestVote RPC if the requesting candidate's log is not
as up-to-date as its log. One log is as up-to-date as another if its last entry
has a higher term or has the same term but is longer.

Since a candidate must receive a majority of votes and committed values have
been replicated to a majority of nodes, a candidate must contact a node with
all committed values during its election which will prevent it from being
elected if it doesn't have all the committed log entries.

To prevent another subtle bug, leaders also do not directly commit values from
previous terms. They only commit values from their own term which indirectly
commits previous log entries from previous terms.

**Cluster Membership Changes.**
A Raft cluster cannot be instantaneously switched from one configuration to
another. For example consider a cluster moving from 3 to 5 nodes. It's possible
that two nodes are elected master for the same term which can lead to a safety
violation. Instead, the cluster transitions to a *joint consensus* phase where
decisions require a majority from both the old and new configuration. Once a
majority of nodes accept the new configuration, the cluster can transition to
it.

## [Shielding Applications from an Untrusted Cloud with Haven (2014)](https://scholar.google.com/scholar?cluster=12325554201123386346&hl=en&as_sdt=0,5)
**Summary.**
When running an application in the cloud, users have to trust (i) the cloud
provider's software, (ii) the cloud provider's staff, and (iii) law enforcement
with the ability to access user data. Intel SGX partially solves this problem
by allowing users to run small portions of program on remote servers with
guarantees of confidentiality and integrity. Haven leverages SGX and Drawbridge
to run *entire legacy programs* with shielded execution.

Haven assumes a very strong adversary which has access to all the system's
software and most of the system's hardware. Only the processor and SGX hardware
is trusted. Haven provides confidentiality and integrity, but not availability.
It also does not prevent side-channel attacks.

There are two main challenges that Haven's design addresses. First, most
programs are written assuming a benevolent host. This leads to Iago attacks in
which the OS subverts the application by exploiting its assumptions about the
OS. Haven must operate correctly despite a *malicious host*. To do so, Haven
uses a library operation system LibOS that is part of a Windows sandboxing
framework called Drawbridge. LibOS implements a full OS API using only a few
core host OS primitives. These core host OS primitives are used in a defensive
way. A shield module sits below LibOS and takes great care to ensure that LibOS
is not susceptible to Iago attacks. The user's application, LibOS, and the
shield module are all run in an SGX enclave.

Second, Haven aims to run *unmodified* binaries which were not written with
knowledge of SGX. Real world applications allocate memory, load and run code
dynamically, etc. Many of these things are not supported by SGX, so Haven (a)
emulated them and (b) got the SGX specification revised to address them.

Haven also implements an in-enclave encrypted file system in which only the
root and leaf pages need to be written to stable storage. As of publication,
however, Haven did not fully implement this feature. Haven is susceptible to
replay attacks.

Haven was evaluated by running Microsoft SQL Server and Apache HTTP Server.

## [Storm @ Twitter (2014)](https://scholar.google.com/scholar?cluster=17632042616176969463&hl=en&as_sdt=0,5)
Storm is Twitter's stream processing system designed to be scalable, resilient,
extensible, efficient, and easy to administer. In Storm, streams of tuples flow
through (potentially cyclic) directed graphs, called *topologies*, of
processing elements. Each processing element is either a *spout* (a source of
tuples) or a *bolt* (a tuple processor).

**Storm Overview.**
Storm runs on a cluster, typically over something like Mesos. Each Storm
cluster is managed by a single master node knows as a Nimbus. The Nimbus
oversees a cluster of workers. Each worker runs multiple worker processes which
run a JVM which run multiple executors which run multiple tasks:

```
worker
+--------------------------------------------------------------+
| worker process                 worker process                |
| +---------------------------+  +---------------------------+ |
| | JVM                       |  | JVM                       | |
| | +-----------------------+ |  | +-----------------------+ | |
| | | executor   executor   | |  | | executor   executor   | | |
| | | +--------+ +--------+ | |  | | +--------+ +--------+ | | |
| | | | +----+ | | +----+ | | |  | | | +----+ | | +----+ | | | |
| | | | |task| | | |task| | | |  | | | |task| | | |task| | | | |
| | | | +----+ | | +----+ | | |  | | | +----+ | | +----+ | | | |
| | | | +----+ | | +----+ | | |  | | | +----+ | | +----+ | | | |
| | | | |task| | | |task| | | |  | | | |task| | | |task| | | | |
| | | | +----+ | | +----+ | | |  | | | +----+ | | +----+ | | | |
| | | +--------+ +--------+ | |  | | +--------+ +--------+ | | |
| | +-----------------------+ |  | +-----------------------+ | |
| +---------------------------+  +---------------------------+ |
| supervisor                                                   |
| +----------------------------------------------------------+ |
| |                                                          | |
| +----------------------------------------------------------+ |
+--------------------------------------------------------------+
```

Users specify a topology which acts as a logical topology. Storm exploits data
parallelism by expanding the logical topology into a physical topology in which
each logical bolt is converted into a replicated set of physical bolts. Data is
partitioned between producer and consumer bolts using one of the following
partitioning scheme:

- *shuffle*: Data is randomly shuffled.
- *fields*: Data is hash partitioned on a subset of fields.
- *all*: All data is sent to all downstream bolts.
- *global*: All data is sent to a single bolt.
- *local*: Data is sent to a task running on the same executor.

Each worker runs a *supervisor* which communicates with the Nimbus. The Nimbus
stores its state in Zookeeper.

**Storm Internals.**

*Nimbus and Zookeeper.*
In Storm, topologies are represented as Thrift objects, and the Nimbus is a
Thrift server which stores topologies in Zookeeper. This allows topologies to
be constructed in any programming language or framework. For example,
Summingbird is a Scala library which can compile dataflows to one of many data
processing systems like Storm or Hadoop. Users also send over a JAR of the code
to the Nimbus which stores it locally on disk. Supervisors advertise openings
which the Nimbus fills. All communication between workers and the Nimbus is
done through Zookeeper to increase the resilience of the system.

*Supervisor.*
Each worker runs a supervisor process which is responsible for communicating
with the Nimbus, spawning workers, monitoring workers, restarting workers, etc.
The supervisor consists of three threads: (1) a *main thread*, (2) an *event
manager thread*, and (3) a *process event manager thread*. The main thread
sends heartbeats to the Nimbus every 15 seconds. The event manager thread looks
for assignment changes every 10 seconds. The process event manager thread
monitors workers every 3 seconds.

*Workers and Executors.*
Each executor is a thread running in a JVM. Each worker process has a thread to
receive tuples and thread to send tuples. The receiving thread multiplexes
tuples to different tasks' input queues. Each executor runs (1) a *user logic
thread* which reads tuples from its input queue and processes them and (2) an
*executor send thread* which puts outbound tuples in a global outbound queue.

**Processing Semantics.**
Storm provides *at most once* and *at least once* semantics. Each tuple in the
system is assigned a unique 64 bit identifier. When a bolt processes a tuple,
it can generate new tuples. Each of these tuples is also given a unique
identifier. The lineage of each tuple is tracked in a lineage tree. When a
tuple leaves the system, all bolts that contributed to it are acknowledged and
can retire their buffered output. Storm implements this using a
memory-efficient representation that uses bitwise XORs.

**Commentary.**
The paper doesn't mention stateful operators.

## [The Homeostasis Protocol: Avoiding Transaction Coordination Through Program Analysis (2015)](https://scholar.google.com/scholar?cluster=7243022338856545577&hl=en&as_sdt=0,5)
Strong consistency is easy to reason about, but typically requires coordination
which increases latency. Weaker consistency can improve performance but is
difficult to reason about. This paper presents a program analysis and
distributed protocol to run transactions with coordination-free strong
consistency.

**Analysing Transactions.**
We model a database as a finite map from objects to integers. Transactions are
ordered sequences of reads, writes, computations, and prints; this is
formalized below. A transaction `T` executes on a database `D` to produce a new
database `D'` state and a log `G'` of printed values. Formally, `eval(D, T) =
<D', G'>`.

*Symbolic tables* categorize the behavior of a transaction based on the initial
database sate. Formally, a symbolic table for transaction `T` is a binary
relation `Q` of pairs `<P, T'>` where `P` is a formula in first order logic
describing the contents of a database, and `T'` is a transaction such that `T`
and `T'` are observationally equivalent when run on databases satisfying `P`. A
symbolic table can also be built for a set of transactions.

Formally, transactions are expressed in a language `L` which is essentially IMP
with database reads, database writes, and prints. A somewhat simple recursive
algorithm walks backwards through the program computing symbolic tables.
Essentially, the algorithm traces all paths through the programs control flow.

There is also a higher-level language `L++` which can be compiled to `L`.

**Homeostasis Protocol.**
Assume data is partitioned (not replicated) across a cluster of `K` nodes. We
model a distributed database as a pair `<D, Loc>` where `D` is a database and
`Loc` is a function from objects to an index between 1 and `K`. Each
transaction `T` runs on a site `i`; formally, `l(T) = i`. For simplicity, we
assume that transactions only write to objects local to the site it is running
on.

Each transaction runs on some site. It reads fresh versions of values on the
site and stale versions of values on other sites. Nodes establish treaties with
one another such that operating with stale data does not affect the correctness
of the transaction. This is best explained by way of example. Imagine the
following transaction is running on a site where x is remote.

```
x' = r(x)
if x' > 0:
    write(y = 1)
else:
    write(y = 2)
```

If we establish the treaty `x > 0`, then it doesn't matter what the actual
value of `x` is. We now formalize this notion.

Given a database `D`, a *local-remote partition* is a function `p` from objects
to booleans. We can represent a database `D` with respect to a local-remote `p`
as a pair `(l, r)` where `l` is a vector of values `x` such that `p(x)`, and
`r` is a vector of values `x` such that `not p(x)`. In words, we can model a
database as disjoint sets of local and remote values.

We say `<(l, r), G> = <(l', r') G'>` if `l = l'` and `r = r'`. Given a database
`D`, local-remote partition `p`, transaction `T`, and set of vectors `L` and
`R`, we say `(L, R)` is a *local-remote slice* (LR-slice) for `T` if `Eval((l,
r), T) = Eval((l, r'), T)` for all `l` in `L` and `r, r'` in `R`. In words, (L,
R) is a local-remote slice for T if T's output depends only on the values of
local values.

A *global treaty* Gamma is a subset of possible database states. A global
treaty is valid for a set of transactions `{T1, ..., Tn}` if `({l | (l, r) in
Gamma}, {r | (l, r) in Gamma})` is an LR-slice for all `T`.

The homoeostasis protocol proceeds in rounds where each round has three phases:

1. *Treaty generation* The system generates a treaty for the current database state.
2. *Normal execution.* Transactions can execute without coordination reading a
   snapshot of remote values. After each site executes a transaction, it checks
   that it does not bring the database to a state outside the treaty. If it
   doesn't, the transaction is committed. If it does, we enter the next phase.
3. *Cleanup.* All sites synchronize and communicate all values that have
   changed since the last round. All sites then run the transaction that caused
   the violation. Finally, we enter the next round.

**Generating Treaties.**
Two big questions remain: how do we generate treaties, and how do we enforce
treaties?

Given an initial database state `D`, we could always pick `Gamma = {D}`. This
requires that we synchronize after every single database modification. We want
to pick the treaties that let us run as long as possible before synchronizing.
We can pick the predicate `P` in the symbolic table that `D` satisfies but this
isn't guaranteed to be a valid treaty. Instead we take the predicate `P` and
divide it into a set of local treaties `P1, ..., PK` where the conjunction of
all local treaties imply the global treaty. Moreover, each local treaty must be
satisfied by the database. The conjunction of the local treaties is our global
treaty and is guaranteed to be valid.

Finding good local treaties is not easy. In fact, it can be undecidable pretty
easily. We limit ourselves to linear arithmetic and leverage SMT solvers to do
the heavy lifting for us. First, we decompose the global treaty into a
conjunction of linear constraints. We then generate templates from the
constraints and instantiate them using Z3.

**Homeostasis in Practice.**
Roy et al. present a homoeostasis prototype. An offline preprocessing component
takes in L++ transactions and computes join symbolic tables, using tricks to
keep the tables small. It them initializes global and local treaties. The
online execution component executes the homeostasis protocol described above.
It is implemented in Java over MySQL. The analysis uses ANTLR-4 and Z3.

## [Impala: A Modern, Open-Source SQL Engine for Hadoop (2015)](https://scholar.google.com/scholar?cluster=14277865292469814912&hl=en&as_sdt=0,5)
**Summary.**
Impala is a distributed query engine built on top of Hadoop. That is, it builds
off of existing Hadoop tools and frameworks and reads data stored in Hadoop
file formats from HDFS.

Impala's `CREATE TABLE` commands specify the location and file format of data
stored in Hadoop. This data can also be partitioned into different HDFS
directories based on certain column values. Users can then issue typical SQL
queries against the data. Impala supports batch INSERTs but doesn't support
UPDATE or DELETE. Data can also be manipulated directly by going through HDFS.

Impala is divided into three components.

1. An Impala daemon (impalad) runs on each machine and is responsible for
   receiving queries from users and for orchestrating the execution of queries.
2. A single Statestore daemon (statestored) is a pub/sub system used to
   disseminate system metadata asynchronously to clients. The statestore has
   weak semantics and doesn't persist anything to disk.
3. A single Catalog daemon (catalogd) publishes catalog information through the
   statestored. The catalogd pulls in metadata from external systems, puts it
   in Impala form, and pushes it through the statestored.

Impala has a Java frontend that performs the typical database frontend
operations (e.g. parsing, semantic analysis, and query optimization). It uses a
two phase query planner.

1. *Single node planning.* First, a single-node non-executable query plan tree
   is formed. Typical optimizations like join reordering are performed.
2. *Plan parallelization.* After a single node plan is formed, it is fragmented
   and divided between multiple nodes with the goal of minimizing data movement
   and maximizing scan locality.

Impala has a C++ backed that uses Volcano style iterators with exchange
operators and runtime code generation using LLVM. To efficiently read data from
disk, Impala bypasses the traditional HDFS protocols. The backend supports a
lot of different file formats including Avro, RC, sequence, plain test, and
Parquet.

For cluster and resource management, Impala uses a home grown Llama system that
sits over YARN.

## [Large-scale cluster management at Google with Borg (2015)](https://scholar.google.com/scholar?cluster=18268680833362692042&hl=en&as_sdt=0,5)
**Summary.**
Borg is Google's cluster manager. Users submit *jobs*, a collection of *tasks*,
to Borg which are then run in a single *cell*, many of which live inside a
single *cluster*. Borg jobs are either high priority latency-sensitive
*production* jobs (e.g. user facing products and core infrastructure) or low
priority *non-production* batch jobs. Jobs have typical properties like name
and owner and can also express constraints (e.g. only run on certain
architectures). Tasks also have properties and state their resource demands.
Borg jobs are specified in BCL and are bundled as statically linked
executables. Jobs are labeled with a priority and must operate within quota
limits.  Resources are bundled into *allocs* in which multiple tasks can run.
Borg also manages a naming service, and exports a UI called Sigma to
developers.

Cells are managed by five-way replicated *Borgmasters*. A Borgmaster
communicates with *Borglets* running on each machine via RPC, manages the Paxos
replicated state of system, and exports information to Sigma. There is also a
high fidelity borgmaster simulator known as the Fauxmaster which can used for
debugging.

One subcomponent of the Borgmaster handles scheduling. Submitted jobs are
placed in a queue and scheduled by priority and round-robin within a priority.
Each job undergoes feasibility checking where Borg checks that there are enough
resources to run the job and then scoring where Borg determines the best place
to run the job. Worst fit scheduling spreads jobs across many machines allowing
for spikes in resource usage. Best fit crams jobs as closely as possible which
is bad for bursty loads. Borg uses a scheduler which attempts to limit
"stranded resources": resources on a machine which cannot be used because other
resources on the same machine are depleted. Tasks that are preempted are placed
back on the queue. Borg also tries to place jobs where their packages are
already loaded, but offers no other form of locality.

Borglets run on each machine and are responsible for starting and stopping
tasks, managing logs, and reporting to the Borgmaster. The Borgmaster
periodically polls the Borglets (as opposed to Borglets pushing to the
Borgmaster) to avoid any need for flow control or recovery storms.

The Borgmaster performs a couple of tricks to achieve high scalability.

- The scheduler operates on slightly stale state, a form of "optimistic
  scheduling".
- The Borgmaster caches job scores.
- The Borgmaster performs feasibility checking and scoring for all equivalent
  jobs at once.
- Complete scoring is hard, so the Borgmaster uses randomization.

The Borgmaster puts the onus of fault tolerance on applications, expecting them
to handle occasional failures. Still, the Borgmaster also performs a set of
nice tricks for availability.

- It reschedules evicted tasks.
- It spreads tasks across failure domains.
- It limits the number of tasks in a job that can be taken down due to
  maintenance.
- Avoids past machine/task pairings that lead to failure.

To measure cluster utilization, Google uses a *cell compaction* metric: the
smallest a cell can be to run a given workload. Better utilization leads
directly to savings in money, so Borg is very focused on improving utilization.
For example, it allows non-production jobs to reclaim unused resources from
production jobs.

Borg uses containers for isolation. It also makes sure to throttle or kill jobs
appropriately to ensure performance isolation.

## [Putting Consistency Back into Eventual Consistency (2015)](https://scholar.google.com/scholar?cluster=12926058981780664697&hl=en&as_sdt=0,5)
Many distributed databases geo-replicate data for (i) lower read latency and
(ii) higher fault tolerance in the face of an extreme failure (e.g. [lightning
striking a data
center](https://status.cloud.google.com/incident/compute/15056)). Implementing
strong consistency over a geo-replicated system can incur tremendous write
latency, as updates have to coordinate between geographically distant data
centers. On the other hand, weak consistency is a real brain buster. This paper
introduces a new consistency model between weak and strong consistency,
*explicit consistency*, which takes into account user specified invariants. It
also presents an explicitly consistent system which

1. performs static analysis to determine which operations can be executed
   without coordination,
2. uses *invariant-repair* or *violation-avoidance* to resolve or avoid
   conflicts, and
3. instruments user code with calls to middleware.

The system is called *Indigo* and is built on an existing causally consistent
key-value store with various properties.

**Explicit Consistency.**
A database is a collection of objects replicated across data centers. Users
issue reads and writes as part of transactions, and these transactions are
asynchronously replicated between data centers. We denote by `t(S)` the
database state achieved by applying transaction `t` to database state `S`.
`S_n` is the database state achieved after the `n`th transaction. That is, `S_n
= t_n(...(t_1(t_init))...)`. `T(S_n) = {t_1, ..., t_n}` is the set of
transactions used to create `S_n`. We say a transaction `t_a` happens before a
transaction `t_b`, denoted `t_a --> t_b`, if `t_a` is in `T(S_b)`. `O = (T,
-->)` is a partial order. `O' = (T, <)` is a serialization of `O` if `<` is
total and respects `-->`.

Given an invariant `I`, we say `S` is `I`-valid if `I(S) = true`. `(T, <)` is
an `I`-valid serialization if `I` holds on all prefixes of the serialization.
If a system ensures that all serializations are `I`-valid, it provides explicit
consistency. In other words, an explicitly consistent database ensures
invariants always hold. This builds off of Bailis et al.'s notion of
invariant-confluence.

**Determining I-offender Sets.**
An *invariant* is a universally quantified first order logic formula in prenex
normal form. The invariant can include uninterpreted functions like `Player(P)`
and `enrolled(P, T)`.

A *postcondition* states how operations affect the truth values of the
uninterpreted functions in invariants. Every operation is annotated with
postconditions. A *predicate clause* directly alters the truth assignments of a
predicate (e.g. not `Player(P)`). A *function clause* relates old and new
database states (e.g. `nrPlayers(T) = nrPlayers(T) + 1`).

This language is rather expressive, as evidenced by multiple examples in the
paper.

A set of transactions is an I-offender if it is not invariant-confluent. First,
pairs of operations are checked to see if a contradictory truth assignment is
formed (e.g. `Player(P) and not Player(P)`). Then, every pair of transactions
is considered. Given the weakest liberal precondition of the transactions, we
substitute the effects of the transactions into the invariant to get a formula.
We then check for the validity of the formula using Z3. If the formula is
valid, the transactions are invariant-confluent.

**Handling I-offender Sets.**
There are two ways to handle I-offenders: invariant-repair and
violation-avoidance. Invariant-repair involves CRDTs; the bulk of this paper
focuses on violation-avoidance which leverages existing reservation and escrow
transaction techniques.

- *UID generation.* Unique identifiers can easily be generated without
  coordination. For example, a node can concatenate an incrementing counter
  with its MAC address.
- *Multi-level lock reservation.* Locking is the most general form of
  reservation. Locks come in three flavors: (i) shared forbid, (ii) shared
  allow, and (iii) exclusive allow. Transactions acquire locks to avoid
  invariant violation. For example, an `enrollTournament` could acquire a
  `sharedForbid` lock on removing players, and `removePlayer` could acquire a
  `sharedAllow` lock. Exclusive allow are used for self-conflicting operations.
- *Multi-level mask reservation.* If our invariant is a disjunction `P1 or ...
  or Pn`, then to preserve the invariant, we only need to guarantee that at
  least one of the disjuncts remains true. A mask reservation is a vector of
  locks where an operation can falsify one of the disjuncts only after
  acquiring a lock on another true disjunct preventing it from being
  falsified.
- *Escrow reservation.* Imagine our invariant is `x >= k` and `x` has initial
  value `x0`. Escrow transactions allocate `x0 - k` *rights*. A transaction can
  decrement `x` only after acquiring and spending a right. When `x` is
  incremented, a right is generated. This gets tricker for invariants like `|A|
  >= k` where concurrent additions could generate too many rights leading to an
  invariant violation. Here, we use *escrow transactions for conditions*, where
  a primary is allocated for each reservation. Rights are not immediately
  generated; instead, the primary is responsible for generating rights.
- *Partition lock reservation.* Partition locks allow operations to lock a
  small part of an object. For example, an operation could lock part of a
  timeline to ensure there are no overlapping timespans.

There are many ways to use reservations to avoid invariant violations. Indigo
uses heuristics and estimated operation frequencies to try and minimize
reservation acquisitions.

**Implementation.**
Indigo can run over any key-value store that offers (i) causally consistency,
(ii) snapshot transactions with CRDTs, and (iii) linearizability within a data
center. Currently, it uses Swiftcloud. Its fault tolerance leverages the
underlying fault tolerance of the key-value store. Each reservation is stored
as an object in the key-value store, where operations are structured as
transfers to avoid some concurrency oddities.

## [Spark SQL: Relational Data Processing in Spark (2015)](https://scholar.google.com/scholar?cluster=12543149035101013955&hl=en&as_sdt=0,5)
**Summary.**
Data processing frameworks like MapReduce and Spark can do things that
relational databases can't do very easily. For example, they can operate over
semi-structured or unstructured data, and they can perform advanced analytics.
On the other hand, Spark's API allows user to run arbitrary code (e.g.
`rdd.map(some_arbitrary_function)`) which prevents Spark from performing
certain optimizations. Spark SQL marries imperative Spark-like data processing
with declarative SQL-like data processing into a single unified interface.

Spark's main abstraction was an RDD. Spark SQL's main abstraction is a
*DataFrame*: the Spark analog of a table which supports a nested data model of
standard SQL types as well as structs, arrays, maps, unions, and user defined
types. DataFrames can be manipulated as if they were RDDs of row objects (e.g.
`dataframe.map(row_func)`), but they also support a set of standard relational
operators which take ASTs, built using a DSL, as arguments. For example, the
code `users.where(users("age") < 40)` constructs an AST from `users("age") <
40` as an argument to filter the `users` DataFrame. By passing in ASTs as
arguments rather than arbitrary user code, Spark is able to perform
optimizations it previously could not do. DataFrames can also be queries using
SQL.

Notably, integrating queries into an existing programming language (e.g. Scala)
makes writing queries much easier. Intermediate subqueries can be reused,
queries can be constructed using standard control flow, etc. Moreover, Spark
eagerly typechecks queries even though their execution is lazy. Furthermore,
Spark SQL allows users to create DataFrames of language objects (e.g. Scala
objects), and UDFs are just normal Scala functions.

DataFrame queries are optimized and manipulated by a new extensible query
optimizer called *Catalyst*. The query optimizer manipulates ASTs written in
Scala using *rules*, which are just functions from trees to trees that
typically use pattern matching. Queries are optimized in four phases:

1. *Analysis.* First, relations and columns are resolved, queries are
   typechecked, etc.
2. *Logical optimization.* Typical logical optimizations like constant folding,
   filter pushdown, boolean expression simplification, etc are performed.
3. *Physical planning.* Cost based optimization is performed.
4. *Code generation.* Scala quasiquoting is used for code generation.

Catalyst also makes it easy for people to add new data sources and user defined
types.

Spark SQL also supports schema inference, ML integration, and query federation:
useful features for big data.

## [Twitter Heron: Stream Processing at Scale (2015)](https://scholar.google.com/scholar?cluster=8583668302818342283&hl=en&as_sdt=0,5)
Storm was Twitter's first stream processing system. Unfortunately, it wasn't
good enough for a number of reasons. Heron is a Storm API compliant rewrite of
Storm.

**Motivation.**
In Storm, computation is expressed as a directed graph, or *topology*, where
vertexes are computations and edges transport tuples. Storm topologies are run
on a cluster of workers overseen by a central *Nimbus*. Each worker runs
multiple worker processes which run a JVM which run multiple executors which
run multiple tasks. The executors run multiple threads, and each worker also
has a multi-threaded supervisor.

This worker architecture was far too complex. Multiple components were making
scheduling decisions (e.g. OS schedules processes, JVM schedules executors;
executors schedule tasks) which made it hard to predict when certain tasks
would be run. Moreover, putting different types of tasks on the same executor
complicated logs, exception handling, garbage collection, etc. The Storm
scheduler was also not good at scheduling tasks with different resource
requirements. The fact that workers were very multi-threaded meant that
messages were traversing a lot of thread boundaries.

The Nimbus was a complicated piece of code that did too much stuff. It also
often became a bottleneck and was a single point of failure. It's scheduling
was so poor, that Twitter used to reserve nodes to exclusively run a single
topology. The Nimbus also communicated with workers through ZooKeeper which
became a bottleneck.

Storm also did not implement backpressure; when bolts became overloaded;
packets were just dropped.

**Design Alternatives.**
Twitter considered extending and modifying Storm to fix its problems, but its
flaws were deeply entrenched in its design, so a rewrite would be difficult.
They considered using other existing stream processing systems, but didn't want
to break the Storm API and have to rewrite a bunch of applications. In the end,
they felt like a rewrite was the best bet.

**Data Model and API.**
Heron follows the exact same API as Storm. Computation is expressed as a
directed graph where vertexes are spouts (sources of tuples) or bolts (tuple
processors) and edges transfer tuples between vertexes. Users provide logical
plans which are expanded to physical plans in order to exploit data
parallelism. Heron provides at least once and at most once semantics.

**Architecture Overview.**
Users submit Heron topologies to Aurora, though Heron is able to run on top of
Mesos, YARN, ECS, etc. Each topology is run as a set of containers. One
container runs the *Topology Master* (TM).  The rest run a *Stream Manager*
(SM), a *Metrics Manager* (MM), and multiple *Heron Instances* (HI).  Topology
state is kept in ZooKeeper, and the TM can have a standby. All communication is
done via protobufs.

**Topology Master.**
The TM is responsible for overseeing the execution of a topology and reporting
its status. The TM holds an ephemeral node in ZooKeeper to ensure there is only
ever one TM and so that other things can discover it.

**Stream Manager.**
Stream Managers are responsible for routing tuples. There are `k` Stream
Managers that form a clique. Though, `O(k^2)` connections is a lot, the number
of Heron Instances can scale independently of `k`. Stream Managers communicate
via TCP, short-circuiting if delivering within a container.

Heron, unlike Storm, implements backpressure. Here are three kinds of
backpressure implementations:

- *TCP Backpressure.*
  Heron Instances communicate with Stream Managers via TCP. This provides a
  natural form of backpressure. If a TCP consumer is too slow, the TCP producer
  will slow down. This form of backpressure is easy to implement. However,
  multiple logical edges are mapped over a single SM to SM TCP connection which
  means that sometimes nodes are inadvertently slowed down when they shouldn't
  be.
- *Spout Backpressure.*
  When a bolt is overloaded, the SM in charge of the bolt can tell the spout
  that is feeding into it to slow down. This is somewhat inefficient in that
  slowing an intermediate node may be sufficient.
- *Stage-by-Stage Backpressure.*
  Backpressure can be propagated backwards from vertex to vertex.

Heron implements TCP and Spout Backpressure. Each socket is associated with a
queue whose size has a high and low watermark. If the size exceeds the high
watermark, backpressure is applied until it drops below the low watermark.

**Heron Instances.**
Each Heron instance runs a single JVM which runs a single task which makes
debugging significantly easier. Heron instances cannot be single-threaded
because slow user code could prevent things like metrics from being reported in
a timely manner. So, Heron implements Heron Instances with two threads: a
*Gateway Thread* and a *Task Execution Thread*. The Gateway Thread communicates
with the Task Execution Thread and also communicates with the SM and MM. The
Task Execution Thread runs user code and gathers metrics. The Gateway Thread
communicates with the Task Execution Thread using a set of one-directional
queues. The sizes of these queues can be adjusted to avoid bad GC.

**Metrics Manager.**
The metrics manager, well, manages metrics. It reports metrics to the TM and to
a Twitter monitoring system.

## [Borg, Omega, and Kubernetes (2016)](#borg-omega-and-kubernetes-2016)
**Summary.**
Google has spent the last decade developing three container management systems.
*Borg* is Google's main cluster management system that manages long running
production services and non-production batch jobs on the same set of machines
to maximize cluster utilization. *Omega* is a clean-slate rewrite of Borg using
more principled architecture. In Omega, all system state lives in a consistent
Paxos-based storage system that is accessed by a multitude of components which
act as peers. *Kubernetes* is the latest open source container manager that
draws on lessons from both previous systems.

All three systems use containers for security and performance isolation.
Container technology has evolved greatly since the inception of Borg from
chroot to jails to cgroups. Of course containers cannot prevent all forms of
performance isolation. Today, containers also contain program images.

Containers allow the cloud to shift from a machine-oriented design to an
application oriented-design and tout a number of advantages.

- The gap between where an application is developed and where it is deployed is
  shrunk.
- Application writes don't have to worry about the details of the operating
  system on which the application will run.
- Infrastructure operators can upgrade hardware without worrying about breaking
  a lot of applications.
- Telemetry is tied to applications rather than machines which improves
  introspection and debugging.

Container management systems typically also provide a host of other niceties
including:

- naming services,
- autoscaling,
- load balancing,
- rollout tools, and
- monitoring tools.

In borg, these features were integrated over time in ad-hoc ways. Kubernetes
organizes these features under a unified and flexible API.

Google's experience has led a number of things to avoid:

- Container management systems shouldn't manage ports. Kubernetes gives each
  job a unique IP address allowing it to use any port it wants.
- Containers should have labels, not just numbers. Borg gave each task an index
  within its job. Kubernetes allows jobs to be labeled with key-value pairs and
  be grouped based on these labels.
- In Borg, every task belongs to a single job. Kubernetes makes task management
  more flexible by allowing a task to belong to multiple groups.
- Omega exposed the raw state of the system to its components. Kubernetes
  avoids this by arbitrating access to state through an API.

Despite the decade of experience, there are still open problems yet to be
solved:

- Configuration. Configuration languages begin simple but slowly evolve into
  complicated and poorly designed Turing complete programming languages. It's
  ideal to have configuration files be simple data files and let real
  programming languages manipulate them.
- Dependency management. Programs have lots of dependencies but don't manually
  state them. This makes automated dependency management very tough.

## ['Cause I'm Strong Enough: Reasoning about Consistency Choices in Distributed Systems (2016)](https://scholar.google.com/scholar?cluster=16043456868654348168&hl=en&as_sdt=0,5)
Strong consistency increases latency; weak consistency is hard to reason about.
Compromising between the two, a number of databases allow each operation to run
with either strong or weak consistency. Ideally, users choose the minimal set
of strongly consistent operations needed to enforce some application specific
invariant. However, deciding which operations to run with strong consistency
and which to run with weak consistency can be very challenging. This paper

1. introduces a formal hybrid consistency model which subsumes many existing
   consistency models,
2. introduces a modular proof rule which can determine whether a given
   consistency model enforces an invariant, and
3. implements a prototype using a standard SMT solver.

**Consistency Model, Informally.**
Consider

- a set of states `s, s_init \in State`,
- a set of operations `Op = {o, ...}`,
- a set of values `\bot in Val`, and
- a set of replicas `r_1, r_2, ...`.

The denotation of an operation is denoted `F_o` where

- `F_o: State -> (Val x (State -> State))`,
- `F_o^val(s) = F_o(s)[0]` (the value returned by the operation), and
- `F_o^eff(s) = F_o(s)[1]` (the effect of the operation).

For example, a banking operation may let states range over natural numbers
where

- `F_deposit_a(s) = (\bot, fun s' -> s' + a)`
- `F_interest(s) = (\bot, fun s' -> s' + 0.05 * s)`
- `F_query(s) = (s, fun s' -> s')`

If all operations commute (i.e. `forall o1, o2, s1, s2. F_o1(s1) o F_o2(s2) =
F_o2(s2) o F_o1(s1)`), then all replicas are guaranteed to converge. However,
convergence does not guarantee all application invariants are maintained. For
example, an invariant `I = {s | s >= 0}` could be violated by merging
concurrent withdrawals. To enforce invariants, we introduce a token system
which can be used to order certain operations.

A token system `TS = (Token, #)` is a set of tokens `Token` and a symmetric
relation `#` over `Token`. We say two sets of tokens `T1 # T2` if `exists t1 in
T1, t2 in T2. t1 # t2`. We also update our definition of operations to acquire
tokens:

- `F_o: State -> (Val x (State -> State) x P(Token))`,
- `F_o^val(s) = F_o(s)[0]` (the value returned by the operation),
- `F_o^eff(s) = F_o(s)[1]` (the effect of the operation), and
- `F_o^tok(s) = F_o(s)[2]` (the tokens acquired by the operation).

Our consistency model will ensure that two operations that acquire conflicting
operations will be ordered.

**Formal Semantics.**
Recall a *strict partial order* is a partial order that is transitive and
irreflexive (e.g. sets ordered by strict subset). Given a partial order `R`, we
say `(a, b) \in R` or `a -R-> b`. Consider a countably infinite set `Event` of
events ranged over by `e, f, g`. If operations are like transactions, an event
is like applying a transaction at a replica.

- *Definition 1.* Given token system `TS = (Token, #)`, an *execution* is a
  tuple `X = (E, oper, rval, tok, hb)` where
    - `E \subset Event` is a finite subset of events,
    - `oper: E -> Op` designates the operation of each event,
    - `rval: E -> Val` designates the return value of each event,
    - `tok: E -> P(Token)` designates the tokens acquired by each event, and
    - `hb \subset Event x Event` is a happens before strict partial order where
      `forall e, f. tok(e) # tok(f) => (e-hb->f or f-hb->e)`.

An execution formalizes operations executing at various replicas concurrently,
and the happens before relation captures how these operations are propagated
between replicas. The transitivity of the happens before relation ensures at
least causal consistency.

Let

- `Exec(TS)` be the set of all executions over token system `TS`, and
- `ctxt(e, X) = (E, X.oper|E, X.rval|E, X.tok|E, X.hb|E)` be the context of `e`
  where `E = X.hb^-1(e)`. Intuitively, `ctxt(e, X)` is the subexection of `X`
  that only includes operations causally preceding `e`.

Executions are directed graphs of operations, but without a semantics, they are
rather meaningless. Here, we define a relation `evald_F \subset Exec(TS) x
P(State)` where `evald_F(Y)` is the set of all final states `Y` can be in after
all operations are propagated to all replicas. We'll see shortly that if all
non-token-conflicting operations commute, then `evald_F` is a function.

- `evald_F(Y) = {}` if `Y.E = {}`, and
- `evald_F(Y) = {F_e^eff(s)(s') | e \in max(Y), s \in evald_F(ctxt(e, Y)), s'
  \in evalfd_F(Y|Y.E - {e})}` otherwise.

Now,

- *Definition 2.* An execution `X \in Exec(TS)` is *consistent* with `TS` and
  `F` denoted `X |= TS, F` if `forall e \in X.e. exists s \in evald_F(ctxt(e,
  x)). X.val(e) = F_X.oper(e)^val(s) and X.tok(e = F_X.oper(e)^tok(s))`.

We let `Exec(TS, F) = {X | X |= TS, F}`. Consistent operations are closed under
context. Furthermore, `evald_F` is a function when restricted to consistent
executions where non-token-conflicting operations commute. We call this
function `eval_F`.

This model can model a number of consistency models:

- *Causal consistency.* Let `Token = {}`.
- *Sequential consistency.* Let `Token = {t}`, `t # t`, and `F_o^tok(s) = {t}`
  for all `o`.
- *RedBlue Consistency.* Let `Token = {t}`, `t # t`, and `F_o^tok(s) = {t}` for
  all red `o` and `F_o^tok(s) = {}` for all blue `o`.

**State Based Proof Rule.**
We want to construct a proof rule to establish the fact that `Exec(TS, F)
\subset eval_F^-1(I)`. That is, every execution results in a state that
satisfies the invariant. Since executions are closed under context, this also
means that all operations execute on a state that satisfies the invariant.

Our proof rule involves a *guarantee relation* `G(t)` over states which
describes all possible state changes that can occur while holding token `t`.
Similarly, `G_0` describes the state transitions that can occur without holding
any tokens.

Here is the proof rule.

- *S1*: `s_init \in I`.
- *S2*: `G_0(I) \subset I and forall t. G(t)(I) \subset I`.
- *S3*: `forall o, s, s'.`
    - `s \in I and`
    - `(s, s') \in (G_0 \cup G(F_o^tok(s)^\bot))* =>`
    - `(s', F_o^eff(s)(s')) \in G_0 \cup G(F_o^tok(s))`.

In English,

- *S1*: `s_init` satisfies the invariant.
- *S2*: `G` and `G_0` preserve the invariant.
- *S3*: If we start in any state `s` that satisfies the invariant and can
  transition in any finite number of steps to any state `s'` without acquiring
  any tokens conflicting with `o`, then we can transition from `s'` to
  `F_o^eff(s)(s')` in a single step using the tokens acquired by `o`.

**Event Based Proof Rule and Soundness.**
Instead of looking at states, we can instead look at executions. That is, if we
let invariants `I \subset Exec(TS)`, then we want to write a proof rule to
ensure `Exec(TS, F) \subset I`. That is, all consistent executions satisfy the
invariant. Again, we use a guarantee `G \subset Exec(TS) x Exec(TS)`.

- *E1*: `X_init \in I`.
- *E2*: `G(I) \in I.`
- *E3*: `forall X, X', X''. forall e in X''.E.`
    - `X'' |= TS, F and`
    - `X' = X''|X''.E - {e} and`
    - `e \in max(X'') and`
    - `X = ctxt(e, X'') and`
    - `X \in I and `
    - `(X, X') \in G* =>`
    - `(X', X'') \in G`.

This proof rule is proven sound. The event based rule and its soundness is
derived from this.

**Examples and Automation.**
The authors have built a banking, auction, and courseware application in this
style. They have also built a prototype that you give `TS`, `F`, and `I` and it
determines if `Exec(T, f) \subset eval_F^-1(I)`. Their prototype modifies the
state-based proof rule eliminating the transitive closure and introducing
intermediate predicates.

## [Decibel: The Relational Dataset Branching System (2016)](TODO) ##
**Summary.**
*Decibel* is like git for relational data.

Often, teams need to simultaneously query, analyze, clean, or curate a
collection of data without clobbering each other's work. Currently, the best
solution available involves each team member making copies of the entire
database. This is undesirable for a number of reasons:

- Data is stored very redundantly, wasting a huge amount of storage.
- It is difficult to share or merge changes made to local snapshots of the
  data.
- There is no systematic way to say which version of the data was used for a
  given experiment.

Version control solves these problems, but existing version control systems
(e.g. git) have a couple of shortcomings when applied naively to large
datasets:

- Using distributed version control systems like git, clients clone an entire
  copy of a repository's contents. This is infeasible when working with large
  datasets.
- Version control systems like git operate on arbitrary data and do not
  understand the structure of relational data. This makes certain operations
  like generic diffing a bad fit.
- Systems like git do not support high-level data management or query APIs.

Decibel manages *datasets* which are collections of relations, and each
relation's schema includes an immutable primary key which is used to track the
version of each row. Beginning with an initial snapshot of a dataset, users can
check out, branch, and commit changes to the data set in a style very similar
to git. When data is merged, non-conflicting changes to separate columns of the
same row are both applied. If conflicting changes to the same column of the
same row occur, one branch's changes take priority. Diffing two tables across
two branches results in a table of insertions and deletions. Finally, data can
be queried across versions using VQuel: a query language which is notably not
SQL.

This paper describes three physical realizations of Decibel's logical data
model.

1. In a *tuple-first* representation, tables contain every version of every
   row, and each row is annotated with a bitmap indicating the set of versions
   in which the row is live. In a *tuple-oriented* approach, each of `N` tuples
   comes with a `B`-bit bitmap for `B` branches. In a *branch-oriented*
   approach, there are `B` `N`-bit bitmaps.
2. In a *version-first* representation, all the changes made to a table in a
   branch are stored together in the same file, and these branched files
   contain pointers to their ancestors forming a directed acyclic graph.
3. In a *hybrid* representation, data is stored similarly to the version-first
   approach, but each of the branched files (called *segments*) includes a
   *segment index*: a bitmap, like in the tuple-first representation, that
   tracks the liveness of each row for all descendent branches. Moreover, there
   is a single *branch-segment bitmap* which for each branch, records the set
   of segments with a tuple live in that branch.

The tuple-first representation is good for multi-version queries, the
version-first representation is good for single-version queries, and the hybrid
approach is good at both.

This paper also presents a versioned database benchmarking framework in the
form of a set of branching strategies and characteristic queries to analyze
Decibel and any future versioned databases.

**Commentary.**
A git-like versioned relational database seems like a great idea! This paper
focuses on how to *implement* and *analyze* such a database efficiently; it
focuses less on the higher-level semantics of data versioning. Notably, two
unanswered questions stuck out to me that seem like interesting areas for
future research.

1. The current merging strategy seems inconvenient, if not inconsistent.
   Imagine a table `R(key, a, b)` with the invariant that `a + b < 10`. Imagine
   branch `b1` updates a tuple `(0, 0, 0)` to `(0, 9, 0)` and branch `b2`
   updates it to `(0, 0, 9)`.  If these tuples are merged, the tuple becomes
   `(0, 9, 9)` violating the invariant. In systems like git, users can manually
   inspect and verify the correctness of the merge, but if a large dataset is
   being merged, this becomes infeasible. I think more research is needed to
   determine if this strategy is good enough and won't cause problems in
   practice. Or if it is insufficient, what merging strategies can be applied
   on large datasets.  Perhaps ideas could be borrowed from CRDT literature; if
   all columns were semilattices, the columns could be merged in a sane way.
2. How useful is diffing when dealing with large datasets? When working with
   git, even a modestly sized diff can become confusing. Would a non-trivial
   diff of a large dataset be incomprehensible?

## [Disciplined Inconsistency with Consistency Types (2016)](https://scholar.google.com/scholar?hl=en&q=disciplined+inconsistency+with+consistency+types&btnG=&as_sdt=1%2C5)
**Overview.**
In the face of latency, partitions, and failures, application sometimes turn to
weak consistency for improved performance. However, strong consistency cannot
always be avoided, so some data stores (e.g. Cassandra, Riak) allow
applications to operate on some data with weak consistency and on other data
with strong consistency. However, it is difficult to ensure that data read with
weak consistency does not leak into the strongly consistent data.

For example, imagine a ticketing application that displays the number of
remaining tickets for an upcoming movie. To improve performance, an application
may choose to read the data using weak consistency; it's okay if the displayed
number is slightly erroneous. However, now imagine that we want to increase the
price of the last 10 tickets. If we used the same weakly consistent value to
determine the price of a ticket, we may make less money that we expect. Weak
and strong consistency cannot be carelessly mixed.

The *Inconsistent, Performance-bound, Approximate* (IPA) storage system uses a
type system of *consistency types* to ensure *consistency safety*: the
guarantee that weakly consistent data does not leak into strongly consistent
data. They also introduce error-bounded consistency which allows a runtime to
dynamically choose the strongest consistency that satisfies certain constraints
(e.g. latency or accuracy).

**Programming Model.**
The IPA programming model involves three components: abstract data types,
consistency policies, and consistency types. Users annotate ADTs with
consistency policies which determines the consistency types returned by the
ADT's methods.

1. *ADTs.* IPA is bundled with a set of common ADTs (e.g. sets, maps, lists)
   that can be implemented with any number of backing stores (e.g. Cassandra,
   Riak, Redis). The collection of ADTs is also extensible; users can add their
   own ADTs.
2. *Consistency Policies.* ADT instances (or their methods) are annotated with
   consistency policies which designate the level of consistency the user
   desires. *Static policies*, like strong consistency, are fixed throughout
   the lifetime of the ADT. *Dynamic policies* allow the runtime to dynamically
   choose the consistency of the system to meet some constraint. For example, a
   user can set a policy LatencyBound(x), and the system will choose the
   strongest consistency that can be served and still satisfy the latency
   bound. Or, a user can set a ErrorTolerance(x) allowing the system to return
   approximate results with weaker consistency.
3. *Consistency Types.* Consistency types are parameterized on a type `T` and
   are partially ordered

                  Consistent[T]
                  /     |     \
        Interval[T] Rushed[T] ...
                  \     |     /
                 Inconsistent[T]

   Users can explicitly endorse, or upcast, weak types to stronger types.
   Rushed types are produced by ADTs with LatencyBound policies and are a sum
   of other consistency types. Interval types are a numeric interval which are
   guaranteed to contain the correct value.

**Enforcing Consistency Policies.**
Static consistency policies are easy to enforce. The ADT operations simply set
flags that are used by the underlying store. Dynamic policies are trickier to
implement.

- *Latency bounds.* To service a read with a latency bound of `x` milliseconds,
  for example, a read is issued at every single consistency level. The
  strongest to return in less that `x` milliseconds is used. This naive
  approach can put unwanted load on a system, so IPA can also monitor and
  predict the strongest consistency model for a given latency bound in order to
  issue only a couple of reads.
- *Error bounds.* Error bounds are enforced using a reservation scheme similar
  to the one used by bounded CRDTs. For example, imagine a counter with value
  10 should be concurrently decremented but never drop below 0. We can allocate
  10 decrement tokens and distribute them to nodes. A node can decrement so
  long as it has enough tokens. IPA involves reservation servers which are
  assigned tokens. A node can return an approximate read by knowing the number
  of outstanding tokens. For example, given a value of 100 and knowing there
  are 10 outstanding increment tokens, the true value is somewhere in the range
  [100, 110]. The more outstanding tokens there are, the weaker the error
  bounds. To avoid unnecessarily granting tokens, IPA uses an *allocation
  table* which maps reservation servers to tokens. Tokens are only allocated
  when necessary.

**Implementaton.**
IPA is implemented in Scala on top of Cassandra with some middleware for the
reservation servers. Type checking is done by leveraging Scala's type system.

## [Goods: Organizing Google's Datasets (2016)](TODO) ##
**Summary.**
In fear of fettering development and innovation, companies often allow
engineers free reign to generate and analyze datasets at will. This often leads
to unorganized data lakes: a ragtag collection of datasets from a diverse set
of sources. Google Dataset Search (Goods) is a system which uses unobstructive
post-hoc metadata extraction and inference to organize Google's unorganized
datasets and present curated dataset information, such as metadata and
provenance, to engineers.

Building a system like Goods at Google scale presents many challenges.

- *Scale.* There are 26 billion datasets. *26 billion* (with a b)!
- *Variety.* Data comes from a diverse set of sources (e.g. BigTable, Spanner,
  logs).
- *Churn.* Roughly 5% of the datasets are deleted everyday, and datasets are
  created roughly as quickly as they are deleted.
- *Uncertainty.* Some metadata inference is approximate and speculative.
- *Ranking.* To facilitate useful dataset search, datasets have to be ranked by
  importance: a difficult heuristic-driven process.
- *Semantics.* Extracting the semantic content of a dataset is useful but
  challenging. For example consider a file of protos that doesn't reference the
  type of proto being stored.

The Goods catalog is a BigTable keyed by dataset name where each row contains
metadata including

- *basic metatdata* like timestamp, owners, and access permissions;
- *provenance* showing the lineage of each dataset;
- *schema*;
- *data summaries* extracted from source code; and
- *user provided annotations*.

Moreover, similar datasets or multiple versions of the same logical dataset are
grouped together to form *clusters*. Metadata for one element of a cluster can
be used as metadata for other elements of the cluster, greatly reducing the
amount of metadata that needs to be computed. Data is clustered by timestamp,
data center, machine, version, and UID, all of which is extracted from dataset
paths (e.g. `/foo/bar/montana/August01/foo.txt`).

In addition to storing dataset metadata, each row also stores *status
metadata*: information about the completion status of various jobs which
operate on the catalog. The numerous concurrently executing batch jobs use
*status metadata* as a weak form of synchronization and dependency resolution,
potentially deferring the processing of a row until another job has processed
it.

The fault tolerance of these jobs is provided by a mix of job retries,
BigTable's idempotent update semantics, and a watchdog that terminates
divergent programs.

Finally, a two-phase garbage collector tombstones rows that satisfy a garbage
collection predicate and removes them one day later if they still match the
predicate. Batch jobs do not process tombstoned rows.

The Goods frontend includes dataset profile pages, dataset search driven by a
handful of heuristics to rank datasets by importance, and teams dashboard.

## [Realtime Data Processing at Facebook (2016)](http://dl.acm.org/citation.cfm?id=2904441)
There's an enormous number of stream (a.k.a. real-time, interactive) processing
systems in the wild: Twitter Storm, Twitter Heron, Google Millwheel, LinkedIn
Samza, Spark Streaming, Apache Flink, etc. While all similar, the stream
processing systems differ in their ease of use, performance, fault-tolerance,
scalability, correctness, etc. In this paper, Facebook discusses the design
decisions that go into developing a stream processing system and discusses the
decisions they made with three of their real-time processing systems: Puma,
Swift, and Stylus.

**Systems Overview.**
- *Scribe.* Scribe is a persistent messaging system for log data. Data is
  organized into categories, and Scribe buckets are the basic unit of stream
  processing. The data pushed into scribe is persisted in HDFS.
- *Puma.* Puma is a real-time data processing system in which applications are
  written in a SQL-like language with user defined functions written in Java.
  The system is designed for compiled, rather than ad-hoc, queries. It used to
  compute aggregates and to filter Scribe streams.
- *Swift.* Swift is used to checkpoint Scribe streams. Checkpoints are made
  every `N` strings or every `B` bytes. Swift is used for low-throughput
  applications.
- *Stylus.* Stylus is a low-level general purpose stream processing system
  written in C++ and resembles Storm, Millwheel, etc. Stream processors are
  organized into DAGs and the system provides estimated low watermarks.
- *Laser.* Laser is a high throughput, low latency key-value store built on
  RocksDB.
- *Scuba.* Scuba supports ad-hoc queries for debugging.
- *Hive.* Hive is a huge data warehouse which support SQL queries.

**Example Application.**
Imagine a stream of events, where each event belongs to a single topic.
Consider a streaming application which computes the top `k` events for each
topic over 5 minute windows composed of four stages:

1. *Filterer.* The filterer filter events and shards events based on their
   dimension id.
2. *Joiner.* The joiner looks up dimension data by dimension id, infers the
   topic of the event, and shards output by (event, topic).
3. *Scorer.* The scorer maintains a recent history of event counts per topic as
   well as some long-term counts. It assigns a score for each event and shards
   output by topic.
4. *Ranker.* The ranker computes the top `k` events per topic.

The filterer and joiner are stateless; the scorer and ranker are stateful. The
filterer and ranker can be implemented in Puma. All can be implemented in
Stylus.

**Language Paradigm.**
The choice of the language in which users write applications can greatly impact
a system's ease of use:

- *Declarative.* SQL is declarative, expressive, and everyone knows it.
  However, not all computations can be expressed in SQL.
- *Functional.* Frameworks like Dryad and Spark provide users with a set of
  built-in operators which they chain together. This is more flexible that SQL.
- *Procedural.* Systems like Storm, Heron, and Samza allow users to form DAGs
  of arbitrary processing units.

Puma uses SQL, Swift uses Python, and Stylus uses C++.

**Data Transfer.**
Data must be transferred between nodes in a DAG:

- *Direct message transfer.* Data can be transferred directly with something
  like RPCs or ZeroMQ. Millwheel, Flink, and Spark Streaming do this.
- *Broker based message transfer.* Instead of direct communication, a message
  broker can be placed between nodes. This allows an output to be multiplexed
  to multiple outputs. Moreover, brokers can implement back pressure. Heron
  does this.
- *Persistent message based transfer.* Storing messages to a persistent
  messaging layer allows data to be multiplexed, allows for different reader
  and writer speeds, allows data to be read again, and makes failures
  independent. Samza Puma, Swift, and Stylus do this.

Facebook connects its systems with Scribe for the following benefits:

- Fault Tolerance: If the producer of a stream fails, the consumer is not
  affected. This failure independence becomes increasingly useful at scale.
- Fault Tolerance: Recovery can be faster because only nodes need to be
  replaced.
- Fault Tolerance: Multiple identical downstream nodes can be run to improve
  fault-tolerance.
- Performance: Different nodes can have different read and write latencies. The
  system doesn't propagate back pressure to slow down the system.
- Ease of Use: The ability to replay messages makes debugging easier.
- Ease of Use: Storing messages in Scribe makes monitoring and alerting easier.
- Ease of Use: Having Scribe as a common substrate lets different frameworks
  communicate with one another.
- Scalability: Changing the number of Scribe buckets makes it easy to change
  the number of partitions.

**Processing Semantics.**
Stream processors:

1. Proccess inputs,
2. Generate output, and
3. checkpoint state, stream offsets, and outputs for recovery.

Each node has

- state semantics: can each input affect state at least once, at most once, or
  exactly once.
- output semantics: can each output be produced at least once, at most once, or
  exactly once.

For state semantics, we can achieve

- at least once by saving state before saving stream offsets,
- at most once by saving stream offsets before saving state, and
- exactly once by saving both state and stream offsets atomically.

For output semantics, we can achieve

- at least once by saving output before offset/state,
- at most once by saving offset/state before output,
- exactly once by saving output and offset/state atomically.

At-least-once semantics is useful when low latency is more important than
duplicate records. At most once is useful when loss is preferred over
duplication. Puma guarantees at least once state and output semantics, and
Stylus supports a whole bunch of combinations.

**State-saving Mechanisms.**
Node state can be saved in one of many ways:

- *Replication.* Running multiple copies of a node provides fault tolerance.
- *Local DB.* Nodes can save their state to a local database like LevelDB or
  RocksDB.
- *Remote DB.* Nodes can save their state to remote databases. Millwheel does
  this.
- *Upstream backup.* Output messages can be buffered upstream in case
  downstream nodes fail.
- *Global consistent snapshot.* Flink maintains globally consistent snapshots.

Stylus can save to a local RocksDB instance with data asynchronously backed up
to HDFS. Alternatively, it can store to a remote database. If a processing unit
forms a monoid (identity element with associate operator), then input data can
be processed and later merged into the remote DB.

**Backfill Processing.**
Being able to re-run old jobs or run new jobs on old data is useful for a
number of reasons:

- Running new jobs on old data is great for debugging.
- Sometimes, we need to run a new metric on old data to generate historical
  metrics.
- If a node has a bug, we'd like to re-run the node on the data.

To re-run processing on old data, we have three choices:

- *Stream only.*
- *Separate batch and streaming systems.* This can be very annoying and hard to
  manage.
- *Combined batch and streaming system.* This is what Spark streaming, Flink,
  and Facebook does.

Puma and Stylus code can be run as either streaming or batch applications.

## [TARDiS: A Branch-and-Merge Approach To Weak Consistency (2016)](https://scholar.google.com/scholar?q=TARDiS%3A+A+Branch-and-Merge+Approach+To+Weak+Consistency&btnG=&hl=en&as_sdt=0%2C5)
**Overview.**
Strong consistency is expensive. The alternative, weak consistency, is hard to
program against. The lack of distributed synchronization or consensus in a
weakly consistent system means that replica state can diverge. Existing systems
try to hide this divergence with *causal consistency* to deal with read-write
conflicts and per-object *eventual convergence* to deal with write-write
conflicts, but neither is sufficient to deal with complex multi-object
write-write conflicts.

As a motivating example, imagine a Wikipedia article for Donald Trump with
three parts: some text, an image, and some references. In one partition,
Hillary modifies the text to oppose Trump and subsequently, Tim changes the
picture to a nastier image of Trump. In another picture, Trump modifies to the
text to support Trump and subsequently, Mike changes the references to link to
pro-Trump websites. Later, the partitions need to be merged. The write-write
conflict on the text needs to be reconciled. Moreover, the modifications to the
image and references do not produce conflicts but still need to be updated to
match the text. Existing systems solve this in one of two ways:

1. *syntactic conflict resolution:* Some policy like last-writer-wins is
   chosen.
2. *lack of cross-object semantics:* Users are forced to merge individual
   objects.

TARDiS (**T**ransactional **A**synchronously **R**eplicated **Di**vergent
**S**tore) is a distributed weakly-consistent transactional key-value store
that supports branching computation as a core abstraction. When working off a
single branch, applications are shielded from diverging computations.
Conversely, applications can merge branches and reconcile conflicts.  By
allowing applications to merge entire branches rather than single objects,
users have the ability to perform semantically rich multi-object merges.

TARDiS employs three techniques:

1. *branch-on-conflict*
2. *inter-branch isolation*
3. *application-driven cross-object merge*

and has the following properties:

1. *TARDiS knows history:* TARDiS maintains a DAG of branching execution and
   uses DAG compression to minimize memory overhead.
2. *TARDiS merges branches, not objects:* TARDiS allows applications to merge
   branches rather than single-objects.
3. *TARDiS is expressive:* TARDiS supports various isolation levels.
4. *TARDiS improves performance of the local site:* Branching on conflict and
   deferring merge until later can improve performance in a local setting as
   well as in a distributed setting.

**Architecture.**
TARDiS is a distributed multi-master key value store with asynchronous
replication. There are four main layers to TARDiS:

1. *Storage layer:* this layer implements a disk-backed multiversion B-tree.
2. *Consistency layer:* this layer maintains the DAG of execution branches
   where each vertex is a logical state.
3. *Garbage collection layer:* this layer performs DAG compression and record
   pruning.
4. *Replicator service layer:* this layer propagates transactions.

**Interface.**
Applications use TARDiS in either

- *single-mode*, where transaction execute on a single branch, or
- *multi-mode*, where a transaction can read from multiple branches and create
  a new merged branch.

TARDiS provides an API to find the set of conflicting writes for a set of
branches, the find the fork points for a set of branches, and to get the
versioned values of objects. It also supports varying levels of begin and end
constraints including serializability, snapshot isolation, read committed, etc.

Here's an example TARDiS application that implements a counter.

```
func increment(counter)
    Tx t = begin(AncestorConstraint)
    int value = t.get(counter)
    t.put(counter, value + 1)
    t.commit(SerializabilityConstraint)

func decrement(counter)
    Tx t = begin(AncestorConstraint)
    int value = t.get(counter)
    t.put(counter, value - 1)
    t.commit(SerializabilityConstraint)

func merge()
    Tx t = beginMerge(AnyConstraint)
    forkPoint forkPt = t.findForkPoints(t.parents).first
    int forkVal = t.getForId(counter, forkPt)
    list<int> currentVals = t.getForId(counter, t.parents)
    int result = forkVal
    foreach c in currentVals
        result += (c - forkVal)
    t.put(counter, result)
    t.commit(SerializabilityConstraint)
```

**Design and Implementation.**
When a transaction begins, TARDiS performs a BFS from the leaves of the state
DAG to find the first state satisfying the begin constraint. When a transaction
is committed, TARDiS walks down the state DAG as far as possible until a state
is reached that doesn't satisfy the end constraint, branching if necessary.
Branches are represented as a set of (b, i) pairs to indicate the bth child of
the ith node. Keys are mapped to a topologically sorted list of versions used
during reading.

The garbage collector performs DAG compression, eliminating unneeded states in
the state DAG. It also prunes record unneeded record versions after DAG
compression.
